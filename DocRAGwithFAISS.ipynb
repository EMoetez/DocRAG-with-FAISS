{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building an RAG Query pipeline with FAISS, and Ollamaâ€™s Llama 2 Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project develops a Retrieval-Augmented Generation (RAG) query app that combines efficient retrieval with AI-driven responses. By integrating Chroma DB for data storage, FAISS for vector search, and Llama 2 via Ollama for response generation, it can deliver precise, context-aware answers to user queries. Each part of the code is structured for clarity, following best practices to ensure a seamless experience, from setup to deployment. Let's dive in and bring this RAG app to life!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0-cp310-cp310-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.9.0-cp310-cp310-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 14.9/14.9 MB 808.9 kB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss\n",
    "%pip install langchain_ollama\n",
    "%pip install faiss-cpu\n",
    "%pip install --upgrade gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.13-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.13-cp39-abi3-win_amd64.whl (16.2 MB)\n",
      "   ---------------------------------------- 16.2/16.2 MB 557.2 kB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv\n",
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\document_loaders\\arxiv.py:6: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.utilities.arxiv import ArxivAPIWrapper\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import langchain\n",
    "import langchain.vectorstores\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain.vectorstores import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader, PyPDFLoader\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter= RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Llama2 \n",
    "instruct_llm = ChatOllama(model=\"llama2\", temperature=0.6, num_predict=256)\n",
    "\n",
    "#Using Nvidia embeddings\n",
    "embedder= OllamaEmbeddings(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    #ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    #ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    #ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    #ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    content=json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chunking\n"
     ]
    }
   ],
   "source": [
    "#Chunking the documents and remove very short chunks\n",
    "print(\"Start chunking\")\n",
    "doc_chunks=[text_splitter.split_documents(doc) for doc in docs]\n",
    "doc_chunks=[[c for c in dchunks if len(c.page_content)>200] for dchunks in doc_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the big-picture details\n",
    "Doc_string=\"Available Documents: \"\n",
    "Doc_metadata=[]\n",
    "for chunk in doc_chunks:\n",
    "    metadata= getattr(chunk[0], 'metadata',{})\n",
    "    Doc_string+= \"\\n - \" + metadata.get('Title')\n",
    "    Doc_metadata+= [str(metadata)]\n",
    "    \n",
    "BP_Chunks=  [Doc_string] + Doc_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Documents: \n",
      " - Attention Is All You Need\n",
      " - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena \n",
      "\n",
      "Document 0\n",
      " - # Chunks: 35\n",
      " - Metadata: \n",
      "{'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion '\n",
      "            'Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
      " 'Published': '2023-08-02',\n",
      " 'Summary': 'The dominant sequence transduction models are based on complex '\n",
      "            'recurrent or\\n'\n",
      "            'convolutional neural networks in an encoder-decoder '\n",
      "            'configuration. The best\\n'\n",
      "            'performing models also connect the encoder and decoder through an '\n",
      "            'attention\\n'\n",
      "            'mechanism. We propose a new simple network architecture, the '\n",
      "            'Transformer, based\\n'\n",
      "            'solely on attention mechanisms, dispensing with recurrence and '\n",
      "            'convolutions\\n'\n",
      "            'entirely. Experiments on two machine translation tasks show these '\n",
      "            'models to be\\n'\n",
      "            'superior in quality while being more parallelizable and requiring '\n",
      "            'significantly\\n'\n",
      "            'less time to train. Our model achieves 28.4 BLEU on the WMT 2014\\n'\n",
      "            'English-to-German translation task, improving over the existing '\n",
      "            'best results,\\n'\n",
      "            'including ensembles by over 2 BLEU. On the WMT 2014 '\n",
      "            'English-to-French\\n'\n",
      "            'translation task, our model establishes a new single-model '\n",
      "            'state-of-the-art\\n'\n",
      "            'BLEU score of 41.8 after training for 3.5 days on eight GPUs, a '\n",
      "            'small fraction\\n'\n",
      "            'of the training costs of the best models from the literature. We '\n",
      "            'show that the\\n'\n",
      "            'Transformer generalizes well to other tasks by applying it '\n",
      "            'successfully to\\n'\n",
      "            'English constituency parsing both with large and limited training '\n",
      "            'data.',\n",
      " 'Title': 'Attention Is All You Need'}\n",
      "\n",
      "Document 1\n",
      " - # Chunks: 44\n",
      " - Metadata: \n",
      "{'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, '\n",
      "            'Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric '\n",
      "            'P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica',\n",
      " 'Published': '2023-12-24',\n",
      " 'Summary': 'Evaluating large language model (LLM) based chat assistants is '\n",
      "            'challenging\\n'\n",
      "            'due to their broad capabilities and the inadequacy of existing '\n",
      "            'benchmarks in\\n'\n",
      "            'measuring human preferences. To address this, we explore using '\n",
      "            'strong LLMs as\\n'\n",
      "            'judges to evaluate these models on more open-ended questions. We '\n",
      "            'examine the\\n'\n",
      "            'usage and limitations of LLM-as-a-judge, including position, '\n",
      "            'verbosity, and\\n'\n",
      "            'self-enhancement biases, as well as limited reasoning ability, '\n",
      "            'and propose\\n'\n",
      "            'solutions to mitigate some of them. We then verify the agreement '\n",
      "            'between LLM\\n'\n",
      "            'judges and human preferences by introducing two benchmarks: '\n",
      "            'MT-bench, a\\n'\n",
      "            'multi-turn question set; and Chatbot Arena, a crowdsourced battle '\n",
      "            'platform. Our\\n'\n",
      "            'results reveal that strong LLM judges like GPT-4 can match both '\n",
      "            'controlled and\\n'\n",
      "            'crowdsourced human preferences well, achieving over 80% '\n",
      "            'agreement, the same\\n'\n",
      "            'level of agreement between humans. Hence, LLM-as-a-judge is a '\n",
      "            'scalable and\\n'\n",
      "            'explainable way to approximate human preferences, which are '\n",
      "            'otherwise very\\n'\n",
      "            'expensive to obtain. Additionally, we show our benchmark and '\n",
      "            'traditional\\n'\n",
      "            'benchmarks complement each other by evaluating several variants '\n",
      "            'of LLaMA and\\n'\n",
      "            'Vicuna. The MT-bench questions, 3K expert votes, and 30K '\n",
      "            'conversations with\\n'\n",
      "            'human preferences are publicly available at\\n'\n",
      "            'https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.',\n",
      " 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Printing out some summary information for reference\n",
    "print(Doc_string, '\\n')\n",
    "for i, chunks in enumerate(doc_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consctructing the vector store\n",
    "vecstore=[FAISS.from_texts(BP_Chunks, embedding=embedder)]\n",
    "vecstore+=[FAISS.from_documents(doc_chunk,embedding=embedder) for  doc_chunk in doc_chunks]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity.\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 82 chunks\n"
     ]
    }
   ],
   "source": [
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstore)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {Doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "    \"Be concize and precize to answer in less than 250 words\"\n",
    "), ('user', '{input}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign({'history' : itemgetter('input')| convstore.as_retriever()| long_reorder | docs2str })\n",
    "    | RunnableAssign({'context' : itemgetter('input')|  docstore.as_retriever()| long_reorder | docs2str})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from Attention Is All You Need] .1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\\\nused beam search with a beam size of 4 and length penalty \\\\u03b1 = 0.6 [38]. These hyperparameters\\\\nwere chosen after experimentation on the development set. We set the maximum output length during\\\\ninference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6\\n[Quote from Attention Is All You Need] .\\\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V ) = softmax(QKT\\\\n\\\\u221adk\\\\n)V\\\\n(1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof\\\\n1\\\\n\\\\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\\\nmatrix multiplication code.\\\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\\\ndot product attention without scaling for larger values of dk [3]\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] . We then verify the agreement between LLM judges and human preferences\\\\nby introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\\\\nArena, a crowdsourced battle platform. Our results reveal that strong LLM judges\\\\nlike GPT-4 can match both controlled and crowdsourced human preferences well,\\\\nachieving over 80% agreement, the same level of agreement between humans.\\\\nHence, LLM-as-a-judge is a scalable and explainable way to approximate human\\\\npreferences, which are otherwise very expensive to obtain. Additionally, we show\\\\nour benchmark and traditional benchmarks complement each other by evaluating\\\\nseveral variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes,\\\\nand 30K conversations with human preferences are publicly available at https:\\\\n//github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge\\n[Quote from Document] Available Documents: \\n - Attention Is All You Need\\n - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)Answer in less than 250 words', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about RAG!', additional_kwargs={}, response_metadata={})])\n",
      "Ah, another exciting topic related to natural language processing! RAG, or the Recent Advances in Generative models, is a fascinating area of research that has seen significant developments in recent years.\n",
      "\n",
      "RAG refers to the latest advancements in generative models, such as transformer-based architectures like BERT and RoBERTa, which have shown remarkable performance in various natural language processing tasks. These models are capable of generating coherent and contextually relevant text, thanks to their ability to learn from large amounts of data and capture complex contextual relationships.\n",
      "\n",
      "One of the most significant advancements in RAG is the development of attention-based models. Attention mechanisms allow these models to focus on specific parts of the input when generating output, resulting in more accurate and relevant text. For instance, when translating a sentence from one language to another, an attention-based model can focus on specific words or phrases in the source language to ensure an accurate translation.\n",
      "\n",
      "Another exciting development in RAG is the use of multimodal input representations. These models can process and generate text, image, and even audio content, opening up new possibilities for applications like chatbots, voice"
     ]
    }
   ],
   "source": [
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  \n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://c13aa2db04d38efd90.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c13aa2db04d38efd90.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: What is the core idea behind the LLM-a-Judge concept?\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\n[Quote from Document] User previously responded with Tell me about RAG!\\n[Quote from Document] Agent previously responded with Ah, another exciting topic related to natural language processing! RAG, or the Recent Advances in Generative models, is a fascinating area of research that has seen significant developments in recent years.\\n\\nRAG refers to the latest advancements in generative models, such as transformer-based architectures like BERT and RoBERTa, which have shown remarkable performance in various natural language processing tasks. These models are capable of generating coherent and contextually relevant text, thanks to their ability to learn from large amounts of data and capture complex contextual relationships.\\n\\nOne of the most significant advancements in RAG is the development of attention-based models. Attention mechanisms allow these models to focus on specific parts of the input when generating output, resulting in more accurate and relevant text. For instance, when translating a sentence from one language to another, an attention-based model can focus on specific words or phrases in the source language to ensure an accurate translation.\\n\\nAnother exciting development in RAG is the use of multimodal input representations. These models can process and generate text, image, and even audio content, opening up new possibilities for applications like chatbots, voice\\n\\n\\n Document Retrieval:\\n[Quote from Attention Is All You Need] . There are many choices of positional encodings,\\\\nlearned and fixed [9].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0 to 10000 \\\\u00b7 2\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] .com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\\\\n1\\\\nIntroduction\\\\nThere has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised\\\\ninstruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new\\\\ninstruction following and conversational abilities [31, 2, 30, 8, 52, 48, 14]. Once aligned with\\\\nhumans, these chat models are strongly preferred by human users over the original, unaligned models\\\\non which they are built. However, the heightened user preference does not always correspond to\\\\nimproved scores on traditional LLM benchmarks \\\\u2013 benchmarks like MMLU [19] and HELM [24]\\\\ncannot effectively tell the difference between these aligned models and the base models. This\\\\nphenomenon suggests that there is a fundamental discrepancy between user perceptions of the\\\\nusefulness of chatbots and the criteria adopted by conventional benchmarks\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] \"Judging LLM-as-a-Judge\\\\nwith MT-Bench and Chatbot Arena\\\\nLianmin Zheng1\\\\u2217\\\\nWei-Lin Chiang1\\\\u2217\\\\nYing Sheng4\\\\u2217\\\\nSiyuan Zhuang1\\\\nZhanghao Wu1\\\\nYonghao Zhuang3\\\\nZi Lin2\\\\nZhuohan Li1\\\\nDacheng Li13\\\\nEric P. Xing35\\\\nHao Zhang12\\\\nJoseph E. Gonzalez1\\\\nIon Stoica1\\\\n1 UC Berkeley\\\\n2 UC San Diego\\\\n3 Carnegie Mellon University\\\\n4 Stanford\\\\n5 MBZUAI\\\\nAbstract\\\\nEvaluating large language model (LLM) based chat assistants is challenging due to\\\\ntheir broad capabilities and the inadequacy of existing benchmarks in measuring\\\\nhuman preferences. To address this, we explore using strong LLMs as judges to\\\\nevaluate these models on more open-ended questions. We examine the usage and\\\\nlimitations of LLM-as-a-judge, including position, verbosity, and self-enhancement\\\\nbiases, as well as limited reasoning ability, and propose solutions to mitigate some\\\\nof them\\n[Quote from Document] Available Documents: \\n - Attention Is All You Need\\n - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)Answer in less than 250 words', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the core idea behind the LLM-a-Judge concept?', additional_kwargs={}, response_metadata={})])\n",
      "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: Is it effecient enough to be used in different tasks \\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\n[Quote from Document] User previously responded with Tell me about RAG!\\n[Quote from Document] Agent previously responded with Ah, another exciting topic related to natural language processing! RAG, or the Recent Advances in Generative models, is a fascinating area of research that has seen significant developments in recent years.\\n\\nRAG refers to the latest advancements in generative models, such as transformer-based architectures like BERT and RoBERTa, which have shown remarkable performance in various natural language processing tasks. These models are capable of generating coherent and contextually relevant text, thanks to their ability to learn from large amounts of data and capture complex contextual relationships.\\n\\nOne of the most significant advancements in RAG is the development of attention-based models. Attention mechanisms allow these models to focus on specific parts of the input when generating output, resulting in more accurate and relevant text. For instance, when translating a sentence from one language to another, an attention-based model can focus on specific words or phrases in the source language to ensure an accurate translation.\\n\\nAnother exciting development in RAG is the use of multimodal input representations. These models can process and generate text, image, and even audio content, opening up new possibilities for applications like chatbots, voice\\n[Quote from Document] Agent previously responded with Ah, a great question! The LLM-a-Judge concept is all about using large language models (LLMs) as judges to evaluate other LLMs on more open-ended questions. The idea is that since LLMs have broad capabilities and existing benchmarks don\\'t capture human preferences, we can use these powerful models to assess their peers in a more nuanced way. By having an LLM act as a judge, we can evaluate the chatbots on more complex tasks and better reflect how humans might use them.\\n\\nThe paper \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\" delves into this concept in detail. The authors explore the potential biases and limitations of using LLMs as judges, such as position, verbosity, and self-enhancement biases, as well as limited reasoning ability. They also propose solutions to mitigate some of these issues.\\n\\nOverall, the core idea behind LLM-a-Judge is to leverage the strengths of LLMs to provide a more comprehensive evaluation of their peers in the chatbot space. By doing so\\n[Quote from Document] User previously responded with What is the core idea behind the LLM-a-Judge concept?\\n\\n\\n Document Retrieval:\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] . The training details are in Appendix E. Since we have shown that\\\\nGPT-4 single-answer grading also performs well in Section 4.2, we use GPT-4 single-answer grading\\\\nfor MT-bench in favor of its scalability and simplicity. We ask GPT-4 to give a score for each turn\\\\non a scale of 10 by using our prompt templates (Figure 6, Figure 10) and report an average score of\\\\n160 = 80\\\\u00d72 turns. Table 8 shows the results. We find that fine-tuning on high-quality dialog datasets\\\\n(i.e., ShareGPT) can consistently improve the model performance on MMLU and the improvement\\\\nscales with fine-tuning data size. On the other hand, a small high-quality conversation dataset can\\\\nquickly teach the model a style preferred by GPT-4 (or approximately human) but cannot improve\\\\nMMLU significantly, as shown by the Vicuna-7B (selected) which is trained with only 4.8M tokens\\\\nor 3K conversations. In Table 8, no single benchmark can determine model quality, meaning that a\\\\ncomprehensive evaluation is needed\\n[Quote from Attention Is All You Need] .\\\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V ) = softmax(QKT\\\\n\\\\u221adk\\\\n)V\\\\n(1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof\\\\n1\\\\n\\\\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\\\nmatrix multiplication code.\\\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\\\ndot product attention without scaling for larger values of dk [3]\\n[Quote from Attention Is All You Need] . To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\\\nthe input sequence centered around the respective output position. This would increase the maximum\\\\npath length to O(n/r). We plan to investigate this approach further in future work.\\\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2)\\n[Quote from Attention Is All You Need] . Aligning the positions to steps in computation time, they generate a sequence of hidden\\\\nstates ht, as a function of the previous hidden state ht\\\\u22121 and the input for position t. This inherently\\\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\\\nconstraint of sequential computation, however, remains.\\\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\\\nthe input or output sequences [2, 19]\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)Answer in less than 250 words', additional_kwargs={}, response_metadata={}), HumanMessage(content='Is it effecient enough to be used in different tasks ', additional_kwargs={}, response_metadata={})])\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://c13aa2db04d38efd90.gradio.live\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    \n",
    "\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    \n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"Mydocstore_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a Mydocstore_index\n",
      "a Mydocstore_index/index.faiss\n",
      "a Mydocstore_index/index.pkl\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!tar czvf Mydocstore_index.tgz Mydocstore_index\n",
    "\n",
    "!rm -rf Mydocstore_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x Mydocstore_index/\n",
      "x Mydocstore_index/index.faiss\n",
      "x Mydocstore_index/index.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Our results indicate that using LLM-as-a-judge to approximate\\nhuman preferences is highly feasible and could become a new standard in future benchmarks. We\\nare also hosting a regularly updated leaderboard with more models 2. Notably, DynaBench [21], a\\nresearch platform dedicated to dynamic data collection and benchmarking, aligns with our spirit.\\nDynaBench addresses the challenges posed by static standardized benchmarks, such as saturation and\\noverfitting, by emphasizing dynamic data with human-in-the-loop. Our LLM-as-a-judge approach\\ncan automate and scale platforms of this nature.\\n6\\nDiscussion\\nLimitations. This paper emphasizes helpfulness but largely neglects safety. Honesty and harm-\\nlessness are crucial for a chat assistant as well [2]. We anticipate similar methods can be used to\\nevaluate these metrics by modifying the default prompt\n"
     ]
    }
   ],
   "source": [
    "#Make sure the retreival from Mydocstore_index works\n",
    "!tar xzvf Mydocstore_index.tgz\n",
    "new_db = FAISS.load_local(\"Mydocstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs_test = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs_test[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now our DocQuery pipeline works fine and stores locally the files successfully. In the next steps, we will use ChromaDB for effecient vector store and retreival. Furthermore, we will try to implement the concept of LLM-as-Judge to evaluate the results generated by our model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using ChromaDB as Vector Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a chromaDB client\n",
    "chroma_client=chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection= chroma_client.create_collection(\"This_is_a_research_paper_collection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for i, doc in enumerate(doc_chunks):\n",
    "    #Generate an embedding\n",
    "    embedding=embedder.embed_documents(doc[0].page_content)\n",
    "    \n",
    "    document={\n",
    "        \"id\":f\"doc_{i}\",\n",
    "        \"embedding\": embedding,\n",
    "        \"text\":doc[0].page_content,\n",
    "        \"metadata\": doc[0].metadata      \n",
    "    }\n",
    "    documents.append(document)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'doc_0', 'embedding': [[0.009755322, -0.01926408, 0.008863539, 0.0008448122, -0.020804169, -0.005998144, 0.0072136475, -0.023670753, 0.02452533, -0.005019852, -0.0016919915, -0.008492391, 0.009254219, 0.008378187, -0.0044605196, 0.013003512, -0.018523658, 0.015078548, 0.019184208, -0.013521163, 0.012511935, -0.01633989, -0.017151237, -0.0056184684, 0.0092539145, 0.002122987, 0.009276425, 0.007788671, 0.0032514117, 0.0008590021, 0.0013625367, -0.008021739, -0.018879239, 0.008564272, -0.009087808, -0.014277264, -0.01456393, 0.0025856835, -0.014063155, -0.0023733499, -0.009481689, 0.018401893, 0.025337018, -0.0063798814, -0.024430888, 0.006744091, -0.010955901, -0.013315011, -0.0037905634, 0.012678949, -0.0034331274, 0.007996763, 0.002896165, -0.004052194, -0.017099557, 0.008977967, -0.023415381, 0.0022502155, 0.015317851, 0.0016659342, -0.0031318755, -0.013876448, -0.0029693202, -0.008296445, 0.015213418, 0.0038922185, -0.022887455, 0.019575207, 0.01629644, 0.007112028, 0.026366, -0.01185273, 0.02393401, -5.957063e-05, -0.0010575245, 0.018121308, 0.013528546, 0.016705966, -0.015326212, 0.0058876085, -0.034313515, -0.015518404, 0.017017972, 0.022619125, -0.0048540444, -0.015479578, -0.018515948, -0.00025884242, -0.017299844, 0.0031187646, -0.011680607, 0.020016074, 0.011371959, 0.004685481, -0.010313653, 0.011021763, 0.009930038, 0.016967675, -0.017057816, 0.00968467, 0.015560642, 0.0032880777, -0.01624123, 0.010191617, 0.01369546, 0.014418611, -0.0065111667, -0.016939104, -0.0047011, -0.004749221, 0.0024294576, -0.0054025976, -0.052406855, -0.013268223, -0.009003702, 0.00024947157, 0.0025947301, -0.0023105931, 0.0018398105, -0.007949149, -0.0018712054, -0.0063708047, -0.008580754, -0.015663818, -0.019750997, -0.010210372, 0.009313533, 0.005211189, 0.003691778, -0.0010461345, 0.0016800517, 0.0016816566, -0.0019583905, -0.02643991, 0.013444479, 0.013169521, 0.015579793, -0.016445898, 0.01090594, 0.007731201, -0.0061578946, 0.029813003, 0.0111708315, 0.006199699, 0.0006956444, 0.004950141, -0.012624112, 0.011337322, -0.00075098436, -0.010502563, 0.001781778, 0.013703988, -0.01076928, -0.02679272, 0.008324447, -0.016640458, -0.00043800258, 0.021306938, 0.005010025, 0.027280137, 0.000574092, 0.002761269, 0.009085861, -0.014298666, -0.011047469, -0.012717417, 0.00032795203, -0.0022609748, 0.00031099285, 0.005527186, -6.1631054e-06, -0.008998584, -0.0131394975, -0.00870413, -0.02416446, -0.00743498, 0.00022849605, 0.005469718, 0.0049886266, 0.008241046, 0.009631116, -0.002801664, 0.0002740156, -0.002200418, 0.02329786, 0.01564174, -0.02435219, -0.023796188, -0.026813893, -0.016357401, 0.009958233, 0.015838888, 0.01106955, 0.0030412916, 0.0116287, -0.022740262, 0.0016158631, -0.01491314, -0.0052856654, -0.012945271, 0.0022309069, 0.014709745, 0.018719306, 0.023501148, 0.0013304423, 0.0019570608, 0.021206897, 0.0018863949, -0.012177925, -0.0015569644, -0.030180503, -0.0026072531, 0.016110515, 0.010034848, 0.011312165, 0.012932827, -0.002538952, 0.008282549, 0.011600341, 0.0014102494, 0.008184423, -0.0032054912, 0.0066708517, 0.021289408, 0.0061335373, 0.0022581709, 0.0086019775, -0.030820314, 0.013350617, -0.014310309, 0.019618189, 0.0041567865, 0.0052559073, -0.020483717, 0.012154219, -0.00238844, -0.022314029, -0.010721077, 0.010212148, 0.007069051, -0.009735111, -0.009166354, -0.012215369, -0.007245551, -0.011259251, 0.010197716, -0.033503667, 0.028413033, -0.0063147815, -0.032519445, 0.00909841, -0.008767778, 0.00033334634, 0.015988031, 0.00014115927, 0.008182041, 0.016071541, -0.0057101226, -0.014106133, 0.0029157507, -0.011295454, 0.017922072, 0.007355516, 0.010720357, -0.008000534, -0.005520889, 0.008916508, 0.0017109336, 0.017771719, 0.032381512, 0.01458106, -0.008696151, 0.0070783123, -0.008675597, -0.005836454, -0.045656726, -0.0019775946, 0.009746541, -0.001386683, 0.0074953735, 0.00018867942, -0.0075079915, -0.0022861867, 0.00093933754, -0.003732953, -0.010099619, -0.013513787, -0.0029238637, 0.008435749, 0.0077280584, -0.0016876145, 0.009559222, -0.0066913734, -0.0053024613, 0.0077432888, -0.0004316073, -0.0007730456, -0.0004238316, 0.0041433293, 0.016498515, 0.0043238467, 0.019627644, 0.004659308, -0.008023901, 0.010486727, -0.009928314, -0.022415027, 0.00075239217, -0.004471536, -0.015227319, 0.007828812, 0.033440832, -0.013016706, -0.0003688822, -0.009693779, -0.012881157, 0.0063833795, -0.02883925, -0.004392523, -0.0030517138, 0.015486545, 0.00800649, 0.008553396, -0.006618099, 0.007154481, 0.0031685343, -0.011058456, -0.010383876, 0.0031405438, 0.00451895, 0.006417904, -0.004042963, 0.006506784, -0.0056597586, 0.00079987716, 0.011548959, 0.026886001, 0.0015045999, 0.009142213, -0.023305122, 0.0058872085, 0.009214687, 0.00029932393, 0.0128293615, 0.007964532, 0.009416409, 0.018587947, -0.015760927, -0.01164064, -0.00567582, -0.0071905814, 0.0066332486, -0.007941308, 0.0032015578, -0.018610254, -0.0245774, 0.000624654, -0.0034349444, 0.024876142, -0.00810415, -0.004432916, 0.009499486, -0.0021033664, 0.0070969546, -0.0077214767, -0.0056576426, 0.019426424, 0.023953885, 0.029263223, -0.0115804365, -0.029152919, -0.007577222, -0.0015223123, 0.00022725432, -0.009860514, -0.010828396, -0.0014399232, 0.020873642, -0.0063068764, -0.015702892, 0.0048506325, -0.042956904, 0.0024617747, -0.00741265, -0.016004382, -0.009995722, 0.0063953027, -0.0039067017, -0.0044538667, 0.009727636, -0.0073525696, 0.0048463224, 0.025327584, 0.024029242, -0.0045133065, -0.005172823, -0.011238149, -0.013675032, -0.016113762, -0.015412537, -0.0064440826, -0.004115341, -0.009576573, 0.011524928, -0.026062323, 0.012233932, 0.0013278368, 0.022917273, 0.010018946, -0.012084969, 0.0067133973, -0.016660832, 0.0035447103, 0.00042875277, 0.012620285, 0.00013022331, 0.0047532725, 0.011040953, -0.005909365, -0.0014445655, -0.0041190847, -0.014785496, 0.009226222, -0.003563398, 0.016512038, -0.010782931, -0.009089429, -0.0031115778, 0.008944318, -0.019415665, -0.011102155, -0.0057178033, -0.0023297993, 0.0024778459, -0.005823774, -0.021714034, 0.008433433, -0.0019546214, 0.011804697, 0.003990219, 0.04137453, 0.010310428, -0.0066821817, -0.009555889, 0.01881564, 0.021651072, 0.0026696345, 0.010418885, 0.042148024, 0.012048151, -0.02292128, 0.018671349, 0.009421022, -0.015259558, 0.0007653188, -0.015459234, -0.011215465, 0.003777006, -0.035365056, -0.00796816, 0.011582398, -0.00231983, -0.004746786, 0.0015287558, 0.017874438, 0.023343654, 0.0010204411, 0.018182123, 0.0029963383, 0.0029085493, -0.0022259334, 0.014496083, -0.015967317, -0.01844542, 0.005951289, 0.0032365283, 0.022801517, -0.015364591, 0.023484576, 0.0010047196, 0.012515954, 0.006923885, -0.013090099, -0.0031961922, -0.004153623, 0.0022305734, 0.008465234, 0.019907974, 0.0057585514, 0.0150518315, 0.00037239236, -0.005346148, 0.0075208275, -0.010254836, -0.008219352, -0.0019980457, -0.0042244373, 0.00907913, -0.012885684, -0.003696677, 0.0041934513, 0.005193358, 0.0007856019, 0.0140055325, 0.00026760745, -0.004789718, -0.02398499, 0.018142702, -0.016689839, 0.0156623, -0.014168927, -0.008843505, 0.0012995108, -0.0063587963, -0.11525858, 0.0032967567, 0.012010278, 0.009982275, 0.0012890009, -0.02082875, 0.006600369, -0.017375406, 0.003929406, -0.009768401, -0.002767996, 0.038047757, 0.0073154024, 0.011346775, 0.006818783, -0.014716917, 0.019717507, 6.4248474e-05, -0.00032992446, 0.007078857, -0.012064709, 0.0059981374, 0.0031949314, 0.0053665186, -0.0038593276, -0.020127196, -0.012924172, 0.04071182, -0.02189632, 0.0138740195, 0.0044615725, -0.010139994, -0.0016560877, 0.018391473, 0.022704162, -0.0009433346, -0.00259272, 0.013898379, -0.0023291965, 0.0018956547, 0.0013748773, 0.021075262, 0.002764348, -0.02464036, -0.011417994, -0.03396815, 0.0036326977, 0.00021549368, 0.0017705262, -0.0026317122, 0.0063796993, 0.018346626, 0.0119371675, 0.0010842622, -0.0034138563, -0.017323896, -0.01786964, -0.016464844, 0.010594641, -0.0049328236, -0.005360011, -0.013622016, -0.0099157635, -0.0157632, -0.008948306, 0.010605145, -0.006816422, -0.017170811, -0.009825058, -0.0018794371, 0.01450569, -0.005505433, -0.0019009766, 0.0099913785, -0.0050489223, 0.024478381, -0.030787889, 0.027571665, 0.005427265, -0.023266045, -0.0052637425, -0.00092591933, -0.0053360295, -0.019760193, -0.0041233143, 0.015224748, -0.03163764, -0.010077522, 0.021168701, 0.026522031, 0.03020191, -0.00481766, 0.021955656, -0.016764577, -0.006209492, 0.0048387945, 0.0025690729, 0.0066257874, -0.013038902, -0.02355118, 0.025645493, -0.015183742, -0.0033728727, 0.011560589, -0.006325636, 0.006849326, -0.0104733305, 0.021297527, 0.009934438, -0.013072441, 0.0009967308, -0.019719874, 0.0021089332, -0.005694675, 0.0034853753, -0.012604067, -0.009938324, -0.010087062, 0.013739882, -0.011643299, 0.021046298, 0.02364878, 0.005822637, 0.0020163893, 0.007648833, 0.016711501, -0.015757157, 0.021153407, -0.018130168, 0.0135057615, 0.016540078, -0.00055948284, -0.0010580927, -0.01642946, -0.010082698, -0.016339825, -0.017375184, 2.7317321e-05, 0.021451052, 0.012167171, -0.011306366, 0.0075612166, -0.024099678, -0.0027067526, -0.0020873814, 0.01389638, 0.003743398, 0.0076690223, -0.008879926, -0.0049268184, -0.0012119801, 0.015114647, 0.0058306092, -0.015003608, -0.0032238415, -0.0036166804, 0.012033776, 0.007481476, 0.004134527, -0.0015838986, 0.00076491875, -0.029174557, 0.009628123, 0.0104859555, 0.033714175, 0.005556605, 0.0106161, -0.006256675, 0.018022992, -0.0015833604, 0.012798572, 0.0028878897, -0.024507128, 0.018317046, 0.018569862, -0.0030486141, -0.010639995, -0.003060804, 0.0076100486, 0.013325118, -0.02426229, -0.004170325, 0.018552328, -0.00089396926, -0.013079406, 0.0022474767, 0.0015411408, -0.0033701677, 0.013250877, 0.015565255, 0.028828213, -0.005774898, -0.029574925, -0.007124438, 0.0193722, -0.024170633, -0.03637731, -0.0035072293, -0.013247754, -0.00016612977, 0.019366324, 0.008669057, -0.023515575, 0.007872941, -0.005281913, -0.0017372067, 0.030074771, 0.007101492, -0.007995913, 0.0036482841, -0.013016769, 0.01375968, -0.015119941, 0.0066582374, 0.0107322605, -0.004964564, 0.012607842, 0.0039565465, -0.029268438, 0.010424679, -0.02522075, -0.00804392, -0.02888819, -0.010142039, -0.017113337, -0.021862864, -0.00078784954, -0.011452529, -0.01379957, 0.0055160997, -0.0083410395, 0.0063220575, -0.0048816893, -0.0018016468, 0.0051408396, -0.01107923, -0.010461556, -0.016477764, 0.02539496, -0.007326286, -0.013147723, -0.029340753, -0.0047268337, 0.0066868295, 0.0043770974, -0.0033418706, -0.03347993, -0.024446884, 0.015299582, -0.0011474104, -0.0044520875, 0.0098448, 0.0011734953, 0.0057837297, -0.0038898434, 0.0014397969, 0.000584493, -0.022726297, 0.004913012, 0.012913312, -0.0008665641, -0.0026210158, -0.00015801252, -0.0025227158, -0.010039539, 0.0013944071, 0.0008719926, -0.0012417998, -0.021535045, -0.005888451, 0.009272821, -0.0043737623, 0.02354468, 0.030773927, 0.033043794, 0.009520039, -0.017320683, 0.0046644937, -0.02045805, 0.02177493, -0.0026773424, -0.0104694925, 0.019747384, -0.0007067842, -0.0033139277, -0.0026067875, -0.022940224, 0.014489671, 0.017011756, -0.012151487, 0.01541216, 0.00711633, -0.014917089, -0.01120711, 0.0042315065, 0.013153983, -0.0014563329, 0.0033431877, 0.021663705, -0.028322257, -0.023623597, -0.012990434, 0.012482752, 0.00038567337, -0.0037186698, -0.012881651, 0.014426968, 0.004288861, -0.011343337, -0.018095326, 0.002934365, -0.026440023, 0.026183784, -0.0015032804, 0.003997406, 0.0009030277, -0.0011850852, -0.0069727716, 0.0013735516, -0.0004319158, 0.010412506, -0.002388088, 0.0041477606, 0.0022724934, -0.01455213, 0.013505501, -0.023356374, -0.021864913, 0.006280461, 0.018238582, 0.005495253, -0.012324296, 0.006948542, 0.0040332433, -0.013218741, -0.0015253427, 0.008131397, -0.012882448, 0.0003735063, -0.006273967, 0.0059371935, -0.015139582, 0.015469136, 0.001761713, 0.0030035246, 0.01762365, -0.017765248, -0.01165149, 0.00650267, -0.006361361, -0.034317717, -0.012543052, 0.0027907742, 0.02119815, 0.0097672725, -0.010056064, -0.016298598, -0.00945921, 0.015397129, -0.007642464, 0.015294267, -0.0018736903, -0.0003009098, -0.012218427, -0.0017220594, -0.0043747034, -0.004204472, -0.005580549, 0.0072309203, -0.021236913, -0.0069025797, -0.024986247, 0.011546063, 0.01725187, -0.0004998517, -0.0036470806, -0.021690618, 0.00749069, 0.0072435997, -0.017008541, -0.018203208, -0.0052505806, 0.003632563, 0.000875064, 0.008651499, 0.008014893, -0.0065555195, 0.014356367, 0.0014603813, 0.020040404, -0.0007011816, 0.026599688, -0.023763712, -0.00014639177, 0.026739534, -0.021689404, -0.00092515803, -0.003717956, 0.00062956894, 0.020011475, -0.016740452, -0.0048617884, -0.0080844965, 0.013463392, 0.0017081036, -0.00786751, 0.01800959, 0.014602517, 0.0266777, 0.008288609, 0.00373949, -0.024876857, -0.032330167, 0.009421652, -0.004859781, 0.0060868375, -0.003121366, 0.009975169, -0.011348193, 0.0007806619, 0.042538065, -0.027171623, 0.006017279, 0.0061608627, 0.02683665, -0.020844214, -0.011260203, 0.0011179688, -0.0047450922, -0.015311206, 0.017067555, 0.007010609, -0.01386184, 0.0137768565, 0.0060691326, 0.0015869557, 0.018260129, -0.012480004, -0.00791444, 0.0103498325, -0.00254657, 0.033582814, 0.010172513, -0.014021107, -0.021896673, 0.009429561, -0.0050348863, -0.00462926, -0.001523475, -0.0036795512, 0.0011208149, 0.01235466, 0.007857902, -0.0077175014, 0.008964463, 0.0025842183, 0.008161033, 0.0046640323, 0.02866332, 0.015066492, 0.0032154154, -0.0018886747, 0.009839504, -0.0082335, -0.00605209, 0.015474995, -0.009524434, -0.008490764, 0.00057719706, 0.024215452, 0.011225388, 0.00939752, 0.009205796, 0.014082318, 0.01878727, -0.0145982215, 0.012691241, 0.020515168, 0.021749286, -0.0029963849, 0.0049569965, 0.00122498, 0.010322042, -0.0064844955, -0.019714981, 0.005465388, 0.009235328, 0.022516694, -0.004926, -0.022716247, -0.0031273582, -0.011251445, 0.011716195, 0.004823307, -0.014306997, -0.025071457, 0.012075968, -0.0054895896, 0.012540583, -0.004879891, -0.0063030026, -0.0033714967, 0.018876055, 0.007870498, 0.003121054, -8.0239675e-05, -0.012614377, 0.0010112558, -0.015515618, 0.018919325, -0.013833215, -0.011976591, 0.0204045, 0.001996696, 0.023935959, -0.006813567, 0.0084922565, 0.016514301, -0.010206278, 0.0042100954, 0.023798704, 0.016691111, 0.011506689, -0.00970076, 0.0032636405, -0.011386709, -0.01864032, 0.021566425, -0.017839495, -0.005274389, -0.012728986, 0.0010547509, 0.005858151, 0.016430795, 0.002381965, -0.006430961, 0.0022495736, 0.008910246, -0.017753255, 0.00082982704, 0.010339921, -0.017482292, 0.0017683523, 0.004618377, 0.002749307, 0.003944913, 0.02292076, -0.00854381, -0.0010090083, -0.009320539, 0.00283927, 0.0006479257, 0.012691937, -0.011275839, -0.0011724876, -0.012314704, -0.012881777, 0.004346623, -0.0065723727, 0.027504636, 0.01839308, -0.008047577, 0.03138856, 0.003691923, 0.0030130567, -0.04053641, -0.014343181, 0.006693739, -0.020146422, 0.025839368, 0.005354523, 0.013842877, -0.01640786, 0.0026792702, 0.0038831958, 0.027272923, -0.022691801, -0.017946823, -0.014279016, -0.015152185, -0.028816884, -0.007972721, -0.008349148, -0.016718175, -0.008221496, 0.0036284137, -0.031696156, -0.015312144, -0.0035773301, -0.021695893, 0.022175046, 0.006632566, -0.004490077, -0.014618438, 0.018848129, -0.00180282, -0.031151222, 0.0072406195, -0.008264011, 0.008145559, -0.015984168, 0.0354193, -0.012591791, -0.009459774, -0.00785975, 0.0031419608, 0.0041208533, -0.00083429297, -0.021348532, -0.012720751, -0.004893766, -0.013347917, 0.024151476, -0.011623928, 0.004164862, -0.008576847, -0.02780192, 0.001984757, 0.0044016675, 0.0082231015, -0.0078350585, 0.01032059, 0.031491727, -0.0045926576, 0.0039100144, 0.0021804702, 0.008886442, 0.022684576, -0.01565298, -0.0058091185, 0.006679958, -0.039961036, -0.0063938126, -0.02012069, -0.015576234, -0.005025618, 0.002078754, -0.016794292, 0.007479992, -0.0047067227, -0.021757381, -0.0067271986, -0.0008187943, 0.017458096, -0.0021369865, 0.0064668064, -0.0032087578, -0.007085365, -0.022612672, 0.011687338, 0.024287181, 0.022415042, -0.004920193, -0.00010603487, -0.01074654, 0.031980183, 0.019686505, -0.00016468634, 0.017633446, -0.005385181, -0.016013622, -0.0074335723, -0.070005395, -0.013038773, 0.008563766, 0.0047020754, -0.013371613, -0.00956426, 0.011176755, -0.027124323, 0.00071379245, 0.0019248948, -0.0076508387, -0.0169693, -0.0030094022, 0.038851865, 0.007806579, -0.01848066, -0.0041434616, -0.006026669, -0.001641907, -0.016158503, -0.0030596422, -0.008073045, 0.011279222, 0.016231794, -0.007921676, 0.0062156958, -0.020134069, 0.0070830546, -0.037662275, 0.009969927, 0.02300096, -0.004021302, -0.0060280603, 0.004592237, 0.00810892, -0.009474366, -0.016317507, 0.007996762, 0.022340387, 0.011640808, -0.012330316, 0.022374632, 2.5004863e-06, 0.008753495, -0.0041431105, -0.015515786, 0.008106008, 0.0023220563, 0.008665457, 0.0066864276, 0.006261177, -0.0029522958, -0.006965783, -0.008175086, 0.002297756, -0.007190453, -0.009090708, -0.0062590465, 0.009055836, -0.01365017, 0.015904589, 0.0007046549, 0.018199882, -0.0005707047, 0.012240874, -0.017148025, -0.008533697, 0.014154435, 0.00964253, 0.0030058043, 0.000727428, 0.006150999, 0.011641086, 0.0032759167, 0.008423507, -0.016983086, -0.008480399, -0.0066614076, 0.018981911, -0.015625767, 0.0032854325, 0.009359469, -0.025701331, -0.027240423, -0.011363098, 0.0025924093, -0.004291367, -0.022542572, 0.016142799, 0.009715387, -0.033630196, -0.012316928, -0.020587763, -0.0031606124, -0.016566541, -0.0002032757, 0.023293566, 0.008401492, 0.0033507561, 0.007756635, 0.007143738, -0.004655921, 0.0047353604, -0.013387565, -0.0008914112, -0.032522146, -0.020658493, -0.005565662, -0.009988934, 0.0055358983, 0.02166548, -0.003989572, 0.0063535133, 0.015472213, 0.0016316505, 0.009373805, 0.01267305, 0.0033370368, 0.0023818645, 0.0016753972, 0.018747412, -0.014154676, -0.032531362, 0.029158125, 0.010519045, -0.0118625555, 0.0042231004, -0.01585721, 0.00030193472, 0.03082657, -0.0013136719, 0.020482242, -0.011580445, -0.0043326076, 0.01127161, 0.020291336, 0.022816338, 0.0012936593, 0.011034098, 0.022687685, -0.003485315, -0.00550784, 0.006750473, -0.011217195, 0.0045845243, -0.0060501294, -0.004988929, 0.03081318, -0.0043438957, 0.015882134, 0.016585497, 0.012900395, -0.028500643, -0.012597525, -0.011793512, -0.0027830966, -0.014868765, -0.005723094, 0.011405436, 0.023511373, -0.0051018214, 0.028214045, 0.0016306204, -0.0044058315, -0.017033964, -0.022825344, -0.01436478, 0.027164461, -0.022975173, 0.0055391286, 0.0056227557, 0.0065216636, -0.010089735, -0.010937667, -0.0055297366, 0.010290266, -0.0049943556, -0.011074715, 0.0072283596, -0.018170841, 0.014331108, -0.0044040396, 0.019860066, -0.012140105, -0.11780288, 0.0017283241, 0.016572284, 0.0040925685, 0.0054354104, -0.008613027, 0.010233184, -0.00032581508, 0.00053751306, 0.00055252766, -0.015076056, 0.0008699839, 0.0033745656, 0.001058829, 0.014282422, -0.02044432, 0.006801029, -0.0048318068, 0.0010627485, 0.0006945049, 0.019042509, 0.0042755217, -0.015911968, -0.02437744, 0.005113513, 0.017913032, 0.0154112615, -0.0005031513, 0.009163097, 0.00068218284, -0.012435246, 0.0048142257, -0.012400178, 0.017838143, 0.0014291923, 0.020475168, 0.0012598203, -0.045037325, -0.0007411347, 0.0028764976, 0.004017184, -0.01183935, -0.0022139826, 0.010245534, 0.012933341, 0.008648188, -0.011427649, -0.014568559, -0.0041183154, 0.008454001, 0.014037331, -0.018215068, -0.012476768, -0.0073286216, -0.014039425, 0.1309183, -0.01441608, -0.014555514, 0.02239699, -0.002408493, -0.01867391, 0.014733308, -0.0116921095, -0.010342101, 0.013199821, 0.0024251672, 0.007402441, 0.0055177636, -0.0034868694, -0.0114741195, -0.00068478106, 0.0013865817, -0.014414518, -0.024281086, -0.0062348004, 0.00065539026, -0.0062990678, -0.0016607933, 0.013039426, 0.01027017, 0.009905307, -0.02166283, -0.0034325419, -0.0035944514, 0.005924275, -0.002300671, -0.00024200292, -0.024594603, 0.0148609355, 0.00049610634, 0.011007674, -0.003572897, 0.009584446, 0.0013611284, -0.011445971, 0.009667237, -0.010656751, 0.018547846, 0.0056198617, -0.004325907, 0.018567162, 0.022521019, 0.023040447, 0.007086939, 0.005497953, 0.0063052573, 0.007975205, -0.012144534, 0.0010673931, -0.002549953, -0.021078859, 0.0008034735, -0.0040451353, -0.002929554, 0.005259572, -0.010647275, -0.0005794318, -0.024729237, 0.010151334, 0.006177736, 0.003182991, -0.008622943, -0.009126125, 0.015118254, -6.7861605e-05, -0.017035961, 0.008214555, 0.015635, -0.0015690572, 0.005259228, 0.014015812, -0.023088774, 0.010664108, -0.0033051528, 0.011783865, -0.00828614, -0.009568344, -0.014338753, -0.0068759797, -0.002981424, 0.01219717, -0.0056028035, -0.011422843, -0.0005623373, -0.0071456092, 0.019476335, -0.0097161215, 0.012907765, 0.00019757792, 0.011674176, -0.0025473628, -0.008543476, 9.5450385e-05, -0.017260253, 0.010843499, 0.012304287, 0.01333057, 0.0089244265, -0.024917513, -0.00939867, -0.017287435, 0.008837568, 0.0075204317, 0.0132023245, -0.012778665, 0.0052742367, 0.0009275014, 0.012242375, -0.017334245, -0.010377762, 0.021815868, -0.00036289246, 0.0037379067, 0.003735534, 0.018822396, -0.013996174, 0.0105103925, 0.011328886, 0.011055677, -0.017385505, -0.017063102, 0.01079713, -0.0058394233, -0.014415894, -0.01884232, -0.017294578, 0.015546408, -0.0128893275, -0.0074292575, 0.0052757547, 0.0028658642, 0.006272682, -0.004821803, -0.0041932445, -0.003844872, -0.004766376, -0.02459679, 0.0018777212, -0.0076001463, -0.0032281624, -0.009817864, 0.037836093, -0.0061620926, -0.0024315605, -0.014790382, -0.013207953, 0.0129052345, 0.012243764, 0.0026222002, -0.011400191, -0.0046840617, 0.0063271746, -0.0046340693, -0.0912355, -0.011386346, 0.00016735465, 0.0093355505, 0.011171061, -0.00012084335, 0.019270487, 0.00020350207, 0.005445912, 0.006019873, 0.0046777357, 0.0027626527, -0.01726091, -0.0006618692, 0.0038251174, 0.016462427, -0.025390998, 0.006196066, 0.0138654895, -0.0047706445, -0.009933617, -0.0013390664, -0.030769428, -0.00523517, 0.008929503, -0.015732415, -0.013786775, 0.0028377082, -0.0021101343, -0.012068929, 0.012531034, -0.0017875767, 0.008222577, 0.006556645, -0.011085818, 0.020895436, -0.010455723, 0.0016524104, -0.0059710084, -0.008131111, -0.013420064, -0.0076078987, 0.017163496, -0.00011371087, 0.009564257, -0.010005893, 0.0275435, -0.024272446, 0.0053917128, 0.015312551, -0.0071866987, 0.004380055, -0.008902996, 0.0010047617, 0.004295196, -0.011867425, 0.012781486, -0.015239222, -0.008350939, 0.0006824682, -0.009530875, 0.01043262, -0.01589607, -0.0073529035, 0.004938813, -0.0071374797, 0.010729825, -0.009035698, 0.012993352, -0.0025058598, -0.01037765, 0.0004360643, 0.028120326, 0.003761146, -0.005264833, -0.0012463371, 0.013365146, 0.01442003, -0.028682206, -0.009829265, -0.022460265, 0.030537196, 0.0077057513, 0.016052006, 0.017504053, 0.01729422, 0.015132462, -0.00046955748, -0.026655022, -0.0006230733, -0.002367997, 0.009319694, -0.014812956, -0.023534004, -0.006570041, -0.024455888, 0.020180125, 0.016349697, 0.003448926, -0.015468013, 0.01321292, 0.00532718, -0.00041851084, -0.003420703, -0.007741858, 0.012976582, -0.03561541, 0.014426325, -0.03363514, 0.019143056, -0.0050839675, 0.011019726, -0.029236348, 0.0012504709, -0.0007264151, 0.013410286, 0.034042757, 0.004769494, 0.0043996335, 0.020058434, 0.0050851367, -0.0029439724, 0.012737594, 0.027203701, 0.0033731868, -0.0067252363, -0.0026136076, 0.002204982, 0.0013936533, -0.009327956, -0.014541803, 0.015794735, -0.02287736, 0.0005031823, -0.00021710664, 0.0068693277, 0.006700083, -0.00045812342, -0.023679467, 0.0064052986, -0.015870547, 0.008438164, -0.006432303, -0.014903695, 0.008174512, -0.004973114, 0.0077627413, 0.006256847, -0.020979485, -0.00487976, 0.0045307893, 0.001279666, -0.03367489, 0.008584642, -0.00073876994, -0.0013183659, 0.024928374, -0.0064746104, 0.005661557, -0.0031762403, 0.005452551, 0.00076985627, 0.004588317, 0.0035953815, -0.0014227502, -0.0021156105, 0.0059528486, 0.022186518, 0.0046397727, -0.0021682428, 0.01848851, -0.0007823132, -0.014614076, 0.012797177, 0.003865771, -0.050791398, -0.017433975, 0.00029368445, -0.015912578, 0.009170169, 0.014061312, -0.015772227, -0.0041735573, 0.0038482274, 0.0057253973, 0.0045150807, -0.018942649, -0.006528215, -0.014437805, 0.010057614, -0.043467112, 0.004928388, -0.009517333, -0.011231575, -0.010747318, 0.0078017204, 0.0144484965, -0.024450362, -0.022305766, 0.0070577725, 0.0151064545, -0.004719634, -0.009407669, 0.0014380388, 0.00058217597, 0.0028152403, 0.020808224, -0.0025075777, 0.009839358, 0.017113386, -0.007347523, 0.012319064, -0.026971074, 0.014421837, -0.005635533, 0.019578211, 0.0058681886, -0.01119863, -0.02330744, 0.00064176455, -9.754871e-05, -0.006730741, 0.0138120735, -0.0035170177, -0.0062565273, -0.01850755, 0.012088757, 0.006821508, 0.024360988, 0.015423815, -0.022413041, -6.37159e-05, 0.013535139, 0.015490587, -0.014970082, 0.00939977, 0.012150375, -0.003291266, -0.0035567677, 0.022495044, -0.016796134, -0.0028569123, 0.012390969, 0.023084642, 0.012448868, 0.026691161, -0.01592208, 0.0073274556, 0.02799472, 0.013891005, 0.005728397, 0.0023751287, -0.027562728, 0.0005384467, -0.0007519123, 0.031158036, -0.00089936226, -0.0053065433, -0.0076250983, -0.016412169, 0.0017236024, -0.028189205, 0.023002494, -0.011147351, 0.009251572, -0.001449592, -0.087025724, 0.0050321436, 0.0170436, -0.004633479, 0.0046918425, -0.0026234954, -0.0028777213, 0.014063973, -0.0021822038, 0.011788215, 0.0040123225, 0.0156135615, -0.01370388, 0.01034483, -0.02726329, 0.002928857, -0.00079943484, -0.009636931, -0.005985864, 0.015904514, 0.013066142, 0.013382008, 0.0027260343, -0.020800645, -0.00054437085, -0.017796444, -0.018442886, -0.0037806588, -0.008304294, 0.007317034, 0.016467711, -0.017095078, 0.014295403, 0.0038642355, -0.013447727, -0.037194397, 0.009669321, 0.017119443, -0.016640183, 0.01197529, -0.010429571, -0.0070011877, -0.011146328, -0.010464674, -0.0024674835, -0.011704936, -0.020921253, 0.010845642, 0.009092201, -0.001808243, -0.014823284, -0.010429245, 0.024195055, -0.0075545893, -0.0104497485, -0.01428255, 0.005278805, -0.0070669246, -0.0022557408, 0.006792568, 0.00301049, 0.01888783, 0.011015698, 0.005767638, -0.0026877783, 0.0051139323, -0.019670097, -0.0023562803, 0.003744882, -0.0041151936, 0.013844042, 0.022716753, 5.3297476e-06, -0.023248117, 0.007953618, 0.011446155, 0.0029791796, 0.004663679, -0.009422712, 0.00903794, 0.005548035, -0.004654319, 0.00062742975, 0.022204975, 0.0018413977, -0.024354378, 0.017099207, 0.0035735113, -0.27362308, 0.005718066, -0.0074111023, -0.021123132, -0.012881975, -0.00047324176, 0.008800272, 0.0034976401, -0.010219327, -0.0048547536, -0.0073382487, 0.011931714, -0.006088892, -0.018499156, -0.0131104225, 0.010246271, -0.0022056422, -0.000186611, -0.021199208, -0.0012048751, -0.012559957, 0.00067184854, -0.0044175345, -0.027549373, 0.019818332, -0.008528703, 0.021557933, -0.020572335, 0.01165178, 0.0027610746, 0.0010694698, 0.009026012, 0.0008745987, 0.017569143, 0.0027493634, 0.027689645, -0.0037944878, -0.005961572, 0.025253547, -0.006248397, -0.020797318, -0.025723629, -0.0023834272, 0.00851249, -0.018472016, -0.002813668, -0.0130119445, -0.003387808, -0.017640317, 0.013819888, 0.012739274, 0.013895448, -0.0014416716, 0.011931321, 0.015957495, 0.017291036, -0.012620421, 0.006480769, -0.018160446, 0.0034789103, -0.007685066, 0.016344024, -0.018493904, 0.00594425, 0.017125284, -0.0032972721, 0.013511687, 0.0042561223, 0.004920638, 0.012165115, -0.014487836, -0.00038820237, -0.011820084, -0.028115481, -0.009481867, -0.029232323, 0.006741066, 0.047509473, -0.019769529, 0.0050934134, -0.00378786, -0.013879984, 0.0031568161, 0.030216949, 0.012186195, 0.00707301, 0.0005607261, 0.011736657, -0.004799585, -0.003022325, 0.015284317, -0.009011968, -0.00010481877, 0.0027244273, -0.042252544, -0.0008090341, -0.006137453, -0.02531916, 0.005328752, 0.010998823, 0.006544506, 0.00719118, 0.0075258873, -0.018444758, 0.007253976, -0.026757017, 0.012180473, 0.0058581587, -0.017786846, -0.01778299, -0.003142788, -0.01858164, -0.015115936, 0.0032129004, -0.0032343683, -0.03715386, 0.0011431432, 0.019447664, -0.016067699, 0.000523191, 0.00040337385, -0.01072168, -0.0041347737, -0.012437901, 0.009762343, -0.015445291, -0.0029531326, -0.023124726, -0.01229674, 0.009572438, -0.043572053, -0.012516418, -0.021745158, 0.0019488506, 4.7458503e-05, 0.017006146, -0.008152139, -0.010801316, -0.00039609193, 0.0059746425, -0.0075764237, -0.0040845196, 0.015269109, 0.0024725194, -0.026872339, -0.0041481177, 0.009649783, 0.0027992304, 0.013195092, 0.008890028, 0.0037781626, -0.012480011, 0.027764842, -0.00069736975, -0.0036546735, -0.010859613, 0.009808232, -0.11300266, -0.0034831509, -0.014295134, -0.042276148, 0.012449479, 0.0061279396, 0.016909853, 8.882732e-05, -0.0028361685, -0.013680461, 0.0010564575, -0.0042231483, 0.0009291751, -0.011395596, -0.023009181, -0.012357917, -0.011461511, -0.0012703644, -0.0073290346, 0.0046705743, -0.0023209082, -0.01613555, -0.014305292, -0.008249783, -0.021210214, -0.012902445, -0.0143153565, -0.012263359, -0.0037575166, -0.0023708392, 0.016367367, -0.01142254, 0.020371974, 0.0042729257, -0.02683698, 0.014573347, -0.00646028, -0.013971034, 0.024274765, 0.018097574, 0.002771266, -0.012464422, -0.019623538, 0.0129065495, 0.013633498, 0.0011291507, -0.006733793, 0.004244512, 0.0045715636, -0.00244053, -0.0009557521, 0.017684683, 0.017729161, -0.008963452, -0.002645861, 0.0056854775, -0.011805546, 0.001374137, 0.008817186, 0.009791193, -0.02020367, -0.0003848048, 0.0041051162, -0.005677603, 0.01772657, 0.0056421137, -0.014344556, 0.018755276, 0.016056756, 0.020417998, 0.00591717, 0.0027358697, 0.012045875, -0.0072426014, 0.015877998, -0.02075608, -0.0052163997, -0.012127445, 0.0072993916, -0.0042707007, 0.020018969, -0.018092746, -0.009631232, -0.016370427, 0.011775383, 0.003991415, -0.0043266094, -0.0133502595, 0.021122681, 0.019941622, -0.0112438, 0.004903877, -0.019562064, 0.013219565, 0.00943659, -0.0058096047, 0.01126918, 0.012191579, -0.00073053734, -0.0069647897, -0.09090709, 0.007858571, -0.0012463367, -0.019909991, -0.019961694, 0.008958991, 0.021164047, -0.0029870993, -0.0015687582, 0.00037672243, -0.010985293, 0.008938506, -0.00026396013, 0.005826179, 0.00018903863, -0.011155792, -0.011219045, 0.022729846, 0.008322719, 0.0008808572, 0.008214885, -0.006795169, -0.0044496064, 0.0066930214, 0.0047239247, -0.004110257, 0.011030611, -0.005059071, -0.0061714333, 0.024472456, 0.0058274535, -0.023067491, -0.02295628, -0.015216721, -0.0031547428, -0.021358268, 0.0034040548, 0.0056269094, 0.00046869327, 0.0051005073, 0.021778194, -0.0014196876, -0.0018201871, 0.009411367, -0.0025093644, 0.01569802, -0.009659206, 0.0034704278, 0.017366754, 0.0051131905, 0.013468663, -0.0022145007, -0.0024501942, -0.0010814472, 0.024123332, -0.024703864, 0.014284544, 0.008681176, -0.020055426, -0.018147158, -0.010480348, 0.008401387, 0.0062882, 0.0022523983, 0.0023475152, 0.013676487, -0.00923786, 0.005934636, 0.007081506, -0.015999567, -0.008645171, -0.008555205, -0.01590227, -0.002915, -0.011772141, 5.1729625e-05, -0.00096096244, -0.014743277, 0.0016033781, 0.002463118, 0.017626703, -0.009095958, -0.0058261156, -0.0045263697, 0.018363558, 0.0013604455, 0.024353864, 0.0052959435, -0.013169859, 0.011692386, 0.012334964, 0.0114646135, -0.0043342817, 0.0021306484, -0.008126062, -0.007262729, 0.0052088955, -0.062255282, 0.013741405, -0.0006693954, 0.009780141, 0.012652095, -0.0052781296, -0.010500719, -0.0136937685, 0.019816637, 0.008820262, -0.00024175951, -0.0054127765, -0.0063352794, -0.0021895764, 0.010502307, 0.013185943, 0.01694112, 0.0010561389, -0.005237098, 0.0024812801, 0.0037550577, -0.008021925, 0.01453807, -0.01939656, 0.018092431, -0.03481033, -0.00087422837, 0.0076946844, 0.024314992, 0.0035433995, -0.011466344, 0.00057421817, 0.0106000705, 0.01071858, -0.0027634257, -0.013052042, 0.010255249, 0.0024237034, 0.0022015858, 0.021060837, 0.021727772, 0.016874138, 0.009052911, -0.004053861, -0.001758136, 0.0034021146, -0.0038560627, -0.026653694, 0.0019223925, 0.009503294, -0.016730491, -0.00013933313, -0.010905648, 0.00089877896, 0.024540208, 0.002089298, 0.019121347, -0.00086132996, -0.008530693, -0.024416182, 0.0026767345, -0.004889179, 0.000119171404, -0.015764577, -0.001519087, 0.021730492, -0.0045337547, 0.0018855745, 0.015767237, -0.01525353, 0.0067307847, -0.011122674, -0.0122928275, -0.00062761805, -0.00044104122, -0.0025623408, -0.070300244, -0.01088532, -0.028431544, 0.0030603262, -0.025203085, -0.00016658194, 0.007522178, 0.0054715243, 0.019136012, -0.0092255715, -0.01820085, 0.00021431295, 0.010321842, 0.030766353, -0.0026180984, -0.0021954728, -0.009353093, -0.002933149, 0.0024190717, -0.0026793687, -0.01611877, 0.018149858, -0.0030810079, 0.003647263, -0.0014882671, -0.020883359, -0.012305608, 0.0011868823, -0.027514214, 0.038911544, -0.0066515263, -8.464329e-05, -0.010287582, 0.0037964676, -0.011152183, -0.0019848621, 0.034399413, 0.003014532, -0.00014883229, 0.021754855, -0.0074730795, -0.006013968, 0.006008363, 0.0018554558, 0.008025061, 0.010192205, 0.00910884, -0.008414249, 0.019152261, 0.0084233135, 0.003089815, 0.010937938, 0.016577287, 0.007687759, -0.0011530169, -0.020618454, -0.011253999, 0.004035503, -0.0042519965, -0.0018250748, -0.0217591, -0.0016394228, 0.00803387, -0.015945768, -0.011485628, -0.015972331, -0.013161068, -0.0105131725, -0.002435888, 0.0090835625, -0.019282438, -0.0039513446, -0.00017609434, 0.008373731, 0.014668183, -0.0138946315, 0.0025047383, -5.2120646e-05, -0.015680108, 0.015669163, 0.022772344, -0.0024352702, -0.0029181447, 2.1778056e-05, -0.0052187336, 0.012388083, -0.0031973915, 0.013215412, 0.02512906, -0.01113994, -0.013627338, 0.010214975, 0.0077777687, 0.0059218374, 0.0073027643, -0.005106131, -0.0073425006, -0.0023769347, -0.010409743, -0.0050750156, 0.0150086405, 0.014472876, 0.020813655, 0.014024805, 0.0009765894, 0.0014030731, -0.00063132594, -0.012778188, 0.021287225, -0.00059625664, 0.010370677, 0.006926477, 0.028516697, -0.00045315965, -0.014672992, 0.0073160813, 0.0011377286, -0.009516895, 0.006541278, 0.022376454, -0.0008505601, 0.00737853, -0.007226251, -0.015272488, 0.017247844, 0.0009829697, 0.0070127165, -0.0024705608, 0.017919715, 0.012267806, 0.0017212349, 0.0041145273, -0.019747136, 0.020439612, 0.01601338, -0.008086013, -0.0031517204, 0.0070345546, -0.015419767, 0.016091738, -0.01723844, 0.029643267, -0.0011620171, -0.0025720957, -0.005465364, -0.025115382, 0.006569945, -0.0021708345, -0.014352435, -0.016635822, 0.007835536, -0.005397376, -0.0178761, 0.00028556032, 0.023709247, 0.0031527984, -0.025140451, 0.008244225, 0.02090558, 0.006176812, -0.0009423736, 0.01534854, 0.004644967, 0.0040706554, -0.008212506, 0.013689896, -0.0092202285, -0.029649654, -0.011599737, -0.008626063, 0.00031041235, -0.005884798, 0.008693927, 0.019488228, 0.002479695, 0.02094143, 0.03416686, 0.0077399057, -0.015159822, -0.03513586, 0.0066294633, 0.0033671772, 0.014755331, -0.0060260207, 0.020232027, 0.008547622, -0.006598107, -0.014695431, -0.019857978, -0.0015759937, -0.0010575389, -0.00089527847, 0.005573458, -0.009207348, -0.0044328575, 0.010531991, 0.010233783, -0.008885225, 0.0039402978, -0.005613468, 0.01213554, 0.013556155, -0.0031853868, -0.0017821898, -0.0056271087, -0.010688927, -0.017470337, 0.0046217134, 0.009970057, -0.0022799422, 0.017124502, -0.0058608293, 0.00455472, 0.010706677, 0.021273961, -0.0020106859, 0.012206636, -0.0008630785, 0.0071090814, -0.014992022, 0.014190853, -0.0057320613, 0.005224661, -8.319769e-05, -0.0065205973, 0.008783392, 0.006196909, 0.01069279, -0.004452386, -0.013432135, -0.0062275957, 0.0064224415, -0.011669474, -0.0042066993, 0.003962507, -0.020999199, 0.011746467, -0.0065046144, -0.020825775, -0.004853344, 0.0009382655, -0.015800664, -0.008280585, -0.018477555, 0.010752614, 0.0036035932, -0.015012727, 0.0042050574, 0.009111034, 0.00834413, 0.0011138587, -0.0098821, -0.009836445, 0.0034520135, -0.078672975, -0.019451095, 0.008671432, -0.014991439, -0.0062385574, -0.024891319, 0.0054753465, 0.012600244, 0.009427792, 0.003718845, -0.016398724, -0.026205875, -0.022758497, 0.0101222005, 0.0144538125, 0.010902495, -0.0025004244, -0.026026435, -0.020258758, -0.0042951135, -0.013283097, 0.00042581023, -0.0011513198, 0.010665536, -0.008119717, 0.0018054768, 0.010871474, 0.009251404, 0.019710345, -0.0035081713, -0.0070420243, 0.008640114, -0.016991971, -0.02350633, 0.0021144983, -0.021178475, -0.015417246, -0.0015008209, 0.0068103783, 0.0013798121, 0.003538364, -0.0032521402, 0.011691906, 0.0018487447, 0.008519403, -0.0041661975, -0.026033271, -0.02333471, -0.02232767, -0.06815692, -0.013990242, -0.01866248, 0.008275906, 0.025070492, 0.010376521, -0.015148694, -0.0067680613, 0.021313107, -0.016541358, -0.018765422, 0.014378189, 0.006362439, -0.0007710524, -0.0019566247, -0.0004527153, 0.015948629, 0.008125867, 0.0058918437, 0.00588012, -0.007895325, -0.014193072, -0.015891157, 0.007413223, -0.008764831, 0.006010518, 0.0062278397, 0.020043338, 0.0070536598, -0.0040847664, 0.025354093, 0.019268068, -0.013308167, -0.0010595107, 0.0131431995, 0.023655012, -0.014195338, -0.013381273, -0.011323329, 0.0025670456, 0.005466159, 0.024247661, -0.004019584, 0.0048961695, 0.010723663, -0.006993921, 0.023510136, 0.032079164, 0.012181492, -0.035478845, 0.018446155, -0.0036535284, -0.0034100825, 0.0058789295, 0.010436971, 0.014523702, 0.019796224, 0.01175653, 0.017400056, -0.0091359755, -0.00911605, -0.006681081, -0.0009896097, 0.00984006, -0.0018582159, 0.006217843, -0.00043860075, 0.0061455155, 0.0053827465, -0.010568765, 0.009395534, 0.006373651, -0.018896146, 0.0065927287, -0.011199855, -0.008007905, -0.0014468651, 0.0053289924, -0.010600012, 0.01977599, 0.0053443983, -0.011761516, -0.0050032176, 0.004469843, -0.004171024, -0.0006789097, 0.006603732, 0.0010865459, -0.012578915, 0.002293654, 0.016709104, -0.015862515, 0.00697243, -0.0038928268, 0.0013589603, 0.0030652198, -0.0011423077, -0.003392391, -0.0048983423, 0.0052020666, -0.012267332, -0.010584243, 0.027500086, -0.0017472819, 0.022164626, -0.024078293, -0.016164787, 0.007885681, -0.015967615, -0.010611277, 0.0052196532, -0.0060766847, 0.026990958, -0.0076990766, 0.031049559, 0.01851983, 0.004648946, 0.022756586, 0.011956721, 0.012682232, 0.004471805, 0.013082609, -0.007937494, 0.023509653, -0.016733117, -0.014146733, 0.032017987, 0.010471601, 0.006609088, 0.00695714, 0.014869909, 0.00537885, 0.009183516, -0.02261688, 0.007243346, 0.00024439985, -0.012687375, 0.0027053447, -0.023119994, 0.015748085, -0.058362786, -0.0016581466, -0.016875956, 0.017075943, -0.0007647651, -0.020084204, -0.0080179265, 0.00070803566, -0.008956857, 0.0066112108, -0.019594632, 0.011511619, -0.0008543307, -0.016221141, 0.009165967, -0.010632681, 0.008048804, -0.02370838, 0.01782383, -0.007976445, -0.0045621255, -0.020119524, 0.008756621, 0.0038071382, 0.0056741457, -0.0096007455, -0.054304756, 0.006286968, 0.0055577443, 0.0013501013, -0.010184793, -0.0182997, -0.006583991, 0.002100641, 0.018053653, 0.0011933924, -0.0043750624, -0.00497307, 0.0086342, -0.008637712, -0.012716345, -0.014708342, -0.005305812, 0.0041306866, 0.009245069, -0.004627413, 0.010265933, 0.008833723, -0.00064907083, -0.015213902, 0.007911494, 0.010802793, -0.015901726, 0.0015048557, -0.0006052826, 0.017066749, -0.00934447, 0.0022577273, -0.0074405055, -0.0019563441, 0.00082679756, 0.010767151, 0.0033388934, -0.0246404, 0.021121824, 0.015568624, -0.0054793656, -0.016579645, 0.02611205, -0.0038957382, 0.0087335035, -0.005237616, -0.012278826, -0.020728396, 0.01935662, -0.0036804294, -0.0042295638, 0.0037521196, -0.012496923, 0.01250947, 0.0006087224, -0.02022112, 0.00097072177, 0.061727643, -0.0159071, -0.009924441, 0.003304786, -0.023456164, 0.007990545, 0.00515424, -0.0035467774, 0.0034762572, 0.0071257236, 0.007532594, -0.01041272, -0.018251158, 0.011904491, 0.0073515805, 0.01337623, 0.00012479773, 0.011864811, -0.013355552, 0.014491, -0.008748745, -0.008146193, 0.0016695808, 0.009372529, -0.0005678154, 0.010602086, 0.0033622817, 0.016715204, -0.0027508913, 0.0213224, -0.0038512622, -0.0047325664, 0.00084723206, 0.005368911, -0.004449236, 0.01532844, -0.0052173138, -0.0028651014, 0.0032267147, -0.0033096683, 0.00985331, -0.020883622, -0.00042670497, 0.0078106388, 0.01184729, 0.0068090563, -0.03423516, 0.0021073895, 0.013652193, -0.028296841, 0.005814609, -0.0032632302, -0.016703213, 0.008936504, 0.029864002, -0.010383222, -0.0020993538, 0.0003571386, 0.007464621, -0.013787381, 0.012620882, -0.019156003, 0.0071304436, -0.007636707, 0.0004073311, 0.0029277792, 0.022425016, -0.006410063, 0.0011285661, 0.00034145624, -0.010433788, -0.007002207, 0.0031468847, -0.019189378, 0.0011598697, -0.00573459, 0.007461386, 0.015173539, -0.0095527405, 0.004795634, 0.0072917165, -0.0019621768, 0.009359906, -0.0046301614, 0.017982608, -0.0013845676, -0.010287571, -0.013713627, -0.003385808, 0.0032838162, -0.013515272, -0.0068406593, -0.0011282394, -0.0011066296, -0.032499917, 0.00041540322, -0.011120058, 0.00085379917, 0.0016674673, -0.017719347, -0.0009891037, 0.017475871, -0.0019966671, 0.0026050396, 0.017623326, 0.005006894, -0.01112666, 0.0022205992, -0.006142967, -0.0009085634, -0.013455545, -0.001638496, 0.01845121, 0.014861751, 0.00391978, -0.007822684, 0.00926888, 0.015805077, -0.012688351, 6.7696645e-05, 0.0059056156, -0.015152313, -0.01848256, -0.01823538, 0.005571103, 0.031511255, -0.0066735875, 0.0055778897, 0.017297195, 0.008551605, -0.010537695, -0.015145712, 0.003058449, -0.003832028, -0.010951322, 0.008047843, -0.023269076, 0.0075446684, 0.009170752, -0.018985976, 0.014873522, 0.021454362, -0.013997542, 0.0015753912, -0.0007581289, 0.013342872, -0.01093745, 0.004920679, 0.006295582, 0.008911414, -0.00199382, 0.01195048, 0.0322781, 0.004471372, 0.035754327, 0.0032580914, 0.005586129, -0.01871529, -0.0055347253, -0.018239517, -0.008695853, 0.009862299, -0.0038813879, -0.009953314, 0.011728254, 0.0033362042, -0.049447324, -0.0024813402, 0.0026797084, 0.019727275, 0.01877715, 0.0041481066, 0.0016179201, 0.017431688, 0.02382817, 0.0030151263, -0.016058434, -0.019048667, -0.039447274, 0.0070765256, -0.009679574, -0.0050444384, -0.010183015, -0.0010875982, -0.010477287, 0.002052917, 0.013865393, -0.00580153, 0.005462564, -0.0017599731, 0.015001554, -0.009439536, 0.0057061026, -0.004447163, -0.017116643, 0.030235553, -0.018891556, 0.014343145, -0.014214363, -0.011889668, 0.0006607048, -0.02083368, 0.01023519, -0.01296275, 0.00171027, -0.011428486, -0.010207992, 0.0050113825, 0.010228939, -0.001903578, 0.014963601, 0.00068909687, -0.0053836927, 0.016998876, -0.0038120346, 0.021340257, 0.0031966432, -0.00481921, 0.0069828997, 0.0019799992, 0.02241817, -0.0029584032, -0.019551931, 0.017081866, 0.0148106525, -0.0036405728, -0.01071895, -0.014384353, -0.0004274, 0.01000766, 0.014182357, -0.004083779, 0.0061138133, 0.0046494314, -0.00015895697, 0.0147309685, 0.014085606, 0.007799299, 0.007588238, -0.004111293, -0.020527216, -0.01717929, -0.0060545406, 0.00048449685, 0.0033141868, 0.017071214, 0.012975849, -0.0065203537, 0.019915998, 0.0038059894, 0.008620016, 0.0033158106, -0.00028345062, -0.013846324, -0.010744956, 0.0015111607, 0.008261886, 0.02351287, -0.0112868, 0.018062059, 0.009346983, 3.8277165e-05, -0.016133439, 0.008018385, 0.009196969, -0.018249977, 0.005990665, 0.025915205, -0.00048541013, 0.0143210115, -0.0029407125, -0.02461693, 0.014086386, 0.0064397682, -0.007842884, -0.013172211, 0.0076813297, -0.0008508673, -0.0041748006, 0.011228709, 0.0060205925, -0.01026396, 0.008067988, 0.008942269, 0.0178356, 0.0005579529, -0.01239925, 0.008402073, -0.022499673, -0.009334589, -0.0030925327, 0.01098317, -0.013881674, 0.0020579372, -0.0062951534, 0.022713533, -0.0061920835, -0.0045631044, -0.00042439028, -0.013621664, -0.012501278, 0.0039483933, -0.01668092, -0.0014195878, 0.0064832517, 0.015030912, 0.00069269625, -0.015770867, 0.013924315, 0.016216641, -0.014669746, 0.0074678436, 0.034748387, 0.009398061, -0.038832728, -0.002822027, 0.01693982, -0.012273123, -0.001466133, 0.0005055503, 0.025204254, -0.004239772, -0.0013285119, -0.0028205677, 0.005062774, 0.041868236, 0.001171528, 0.016192053, -0.0061849314, -0.0019315685, 0.0073495787, -0.0007494418, 0.027317336, -0.0045688357, 0.0052046673, 2.1244254e-05, -0.013305953, 0.030450417, 0.014593499, 0.014310033, -0.008210152, 0.015157923, -0.0071155597, -0.014358293, 0.006644414, -0.01577448, -0.007311386, 0.01726584, 0.018006116, 0.009593269, -0.01506627, 0.024440063, -0.0038942145, 0.018279405, 0.027273798, -0.009403206, -0.012399096, -0.0070784357, 0.201396, 0.011638168, -0.011854772, -0.025133263, -0.0016731957, -0.0052716103, -0.017502083, -0.010701777, 0.005953715, -0.00030851294, -0.0006740609, 0.012016396, 0.004625193, 0.00014572761, 0.012912743, -0.023716116, 0.0073405835, -0.008743106, 0.008098409, -0.0012550924, -0.03525424, -0.0059035937, -0.016785722, -0.013018573, 0.0033471356, 0.011017981, 0.010022074, -0.012716637, 0.02135307, 0.0060179997, 0.00012306859, 0.013064163, -0.016879342, -0.0018873854, -0.011790562, 0.0161775, -0.004153955, 0.00889496, 0.0045683915, 0.017977396, 0.011622115, -0.019960608, -0.015852151, -0.007663415, 0.013329468, -0.031124188, -0.0034321477, -0.004641882, 0.013102263, 0.0061352835, -0.010051149, -0.005382258, 0.014471607, 0.015152908, -0.005787217, 0.015949795, 0.014165941, -0.019047182, -0.009380779, 0.02662746, 0.0036340137, -0.009832669, -0.016880058, -0.00041887592, -0.017349802, -0.018777499, 0.003906424, -0.017120633, 0.011192653, 0.003507671, -0.005577851, -0.008750881, -0.0057455506, -0.0072872154, 0.0030240088, 0.012497378, 0.003194137, 0.012529714, 0.003410099, 0.015170375, -0.009465034, -0.0054471237, 0.018063286, -0.0017397619, -0.008553001, 0.0036036738, 0.0040363898, 0.00856706, 0.027496025, 0.027537415, -0.0019742646, -0.005677165, 0.0059856907, 0.0025646249, -0.0043420396, 0.024672855, -0.014018021, 0.007995957, -0.009488757, -0.008109653, -0.003751918, 0.006033899, 0.01563451, 0.0026184092, -0.0100376345, -0.020508284, -0.009150574, 0.005047852, -0.0054044807, -0.014087536, -0.012973054, -0.015557594, 0.014229982, -0.010957304, 0.0011139774, 0.01918933, 0.022827642, 0.0019651009, -0.020319356, -0.01580681, 0.0002496537, 0.006835883, 0.01947186, 0.005647984, -0.013346863, -0.0001492739, 0.031457823, -0.0042073135, 0.01456087, -0.018608613, -0.007333392, -0.016383184, -0.007142185, 0.004795545, 0.0049342145, 0.0011640183, -0.012014916, -0.005330479, 0.001018855, -0.014692447, 0.0150433965, -0.0048591876, -0.00085687893, 0.02537613, -0.008843469, 0.024059253, -0.020006275, 0.016027322, -0.016763315, 0.008408807, -0.002590997, -0.012550807, -0.0005073697, 0.013442724, -0.0047934204, 0.0042266166, 0.026753886, 0.007946463, -0.014386776, -0.022370832, 0.003009374, -0.00479253, -0.027153146, -0.009121244, -0.0016455158, 0.009391195, -0.01554923, -0.008398637, -0.012604411, 0.003816212, -0.004426606, -0.002899737, 0.0061917547, -0.013851268, 0.0029989008, 0.00046286418, 0.0036702678, -0.03031488, 0.0022427244, 0.005109174, 0.015617539, -0.004638461, 0.008034927, -0.008381875, 0.016782438, 0.003492844, -0.005098537, 0.022533692, 0.0018808554, -0.0035402186, -0.010880166, -0.01433853, -0.013737305, -0.0066399537, 0.020475771, -0.0019016166, 0.009009329, 0.00088230724, 0.050518014, 0.023032673, 0.039704476, 0.007062179, -0.009764002, 0.09382578, 0.0060872277, 0.013061728, 0.014402556, 0.0049219904, 0.0060954406, 0.019404624, -0.019567905, 0.005832817, 0.01223395, -0.010819426, 0.001003537, -0.01634669, -0.011190677, 0.011138246, -0.024782786, 0.0150469, -0.026435822, -0.012140381, 0.0067796796, 0.017697942, -0.0053024604, -0.011902175, 0.010140896, -0.0129299015, 0.0006138262, 0.012406431, 0.026129458, 0.0117466515, -0.00019820334, 0.0018600001, 0.018577063, -0.013866565, 0.0050868015, -2.3518924e-05, -0.01676086, 0.008562514, 0.02553044, -0.017691253, -0.018598318, -0.0050427425, -0.007795558, -0.01367651, -0.016152002, 0.0021085, -0.014480599, -0.0046652737, 0.0034714313, -0.00290963, -0.0011905516, -0.03562917, 0.0011366504, 0.016434677, 0.0046284306, 0.0033410427, 0.0048735384, -0.016152672, -0.02858034, -0.022761157, -0.00451178, -0.028838841, -0.017852465, 0.019122524, 0.009999095, -0.0021472268, 0.0010021981, 0.011093605, 0.000435319, 0.0022640827, -0.0005658961, -0.0048997067, 0.016094957, 0.018074755, 0.0021177176, -0.007985646, 0.005353287, -0.00915747, 0.0057449075, -0.0055468106, -0.021784738, 0.0014869934, -0.0015661148, -0.0034944664, 0.007601689, 0.0010349805, -0.0203116, -0.0071653677, -0.0016115052, 0.024074914, -0.0099035995, 0.015262045, 0.0054174126, 0.013213111, 0.008846557, -0.005320776, 0.0139427055, 0.030356148, 0.0015477005, 0.011098418, -0.012071677, 0.0067563, 0.0010226711, 0.015144671, 0.014040441, -0.0004578084, -0.008964287, 0.011711591, -0.0014692497, 0.008365355, -0.0056834226, 0.012722132, 0.008303392, -0.032124322, -0.010617632, 0.009204659, 0.019311182, 0.02863721, -0.014833188, 0.0030066199, 0.010825179, -0.0071618515, 0.010806407, 0.0004648647, 0.011413787, 0.00025207835, 0.0048063663, -0.026794663, 0.009786924, 0.01232761, 0.007536069, -0.0060308315, 0.016932813, 0.023862971, 0.008132825, 0.024244031, 0.006146538, 0.007587997, -0.007789256, -0.03981678, 0.0018724524, -0.0010883454, 0.003479179, 0.013988694, 0.019378655, -0.0062869005, -0.016487395, 0.0019769096, -0.03976157, 0.0033826916, 0.005278352, 0.014203318, 0.010278828, -0.016950184, -0.020529486, 0.022373848, 0.0041358797, 0.002381308, -0.008300153, 0.009668705, 0.007026887, -0.017169783, 0.012432557, 0.0013250695, 0.0053885654, -0.021576107, 0.011380615, 0.008471557, 0.0012591562, 0.009022228, 0.012092564, -0.005292296, 0.03222248, 0.0013427717, 0.0063975835, -0.008912722, -0.008588035, 0.002525023, 0.011722819, -0.010564689, -0.028964886, 0.008169996, -0.0009969905, 0.0055460497, -0.0022956382, 0.011628472, 0.01964218, -0.004651262, 0.016950581, 0.0075864126, -0.010705741, -0.028744247, -0.0021878832, -0.000839763, -0.037429478, 0.011404988, -0.006991899, -0.010725652, 0.0035088758, -0.0026981498, 0.012191672, -0.0046699727, -0.0073611676, -0.009943317, -0.0073818304, -0.007708185, 0.0064604334, -0.0043619755, -0.010957122, -0.0067004384, -0.0016395962, -0.0083542075, -0.016752107, -0.0011125341, -0.017200992, 0.00922135, -0.018447133, 0.014124374, 0.0002580618, -0.029364087, 0.0018417906, 0.023500701, -0.006943187, -0.005162481, -0.013255715, -0.010231634, -0.007742257, 0.003228214, -0.0032532166, -0.0117279105, 0.005640045, 0.009545989, -0.0046947836, -0.017404938, 0.006525216, -0.009945496, 0.002878511, -0.0279683, 0.018608743, 0.0047233514, -0.0034274745, -0.09911293, 0.0048664445, -0.004308105, -0.0046788254, 0.003950977, -0.020430768, -0.008072499, -0.0027447382, -0.0035469143, -0.0016019169, 0.0025633639, -0.030806009, -0.008472251, -0.013085067, 0.004563971, -0.0061729634, 0.026476191, -0.0006711541, 0.007238336, 0.012640409, -0.004850319, -0.021246213, 0.0044573843, -0.011292846, 0.011433866, -0.0023302808, -0.01654467, -0.01633065, -0.025527824, -0.008628924, -4.3364307e-05, 0.01735782, 0.00032800494, -0.008165322, -0.011581323, 0.005965398, -0.0002702619, 0.0008412502, -0.004715914, -0.020290537, -0.004873641, 0.002247161, 0.003252897, 0.0063808025, -0.0053529097, 0.0068207546, 0.0046700346, -0.0031967491, -0.0002954444, -0.008980904, -0.0062753977, 0.002258004, -0.014363326, 0.0062014, 0.012425702, -0.027387237, -0.0068235532, 0.0020758691, 0.0089795375, 0.012935713, 0.009891405, 0.014319992, -0.010941276, -0.017583868, 0.0032686908, -0.008450613, 0.005896807, 0.012189996, 0.0053900387, 0.002384715, 0.034745645, 0.020316474, -0.006626436, 0.018044963, 2.0059993e-05, 0.00022391789, -0.0035548795, -0.008533076, 0.0009757464, 0.029679853, 0.013992196, -0.009844401, 0.0019256567, -0.012594956, -0.00052336324, -0.021913774, 0.013382652, 0.0017619837, 0.021027317, 0.022232467, -0.01472909, 0.008949437, -0.007962824, -0.013779043, 0.0067407214, 0.0054598865, -0.029742356, -0.0071470286, 0.00036800132, -0.0072100447, -0.004453289, -0.018060593, 0.0090585, -0.010458367, -6.5629814e-05, 0.011582131, -0.0068335915, -0.0026373921, -0.0068493867, 0.0011192561, 0.015414109, -0.015067393, -0.019869009, 0.0024468573, -0.009973708, -0.0100070825, -0.011625665, 0.005462131, -0.0063823494, 0.012735641, -0.0022044866, 0.0023965624, 1.7982164e-05, -0.00022571774, 0.0070783696, 0.00881958, -0.0057628388, -0.005648648, -0.005007772, 0.009574336, 0.00064119836, 0.003356455, 0.011052757, -0.019312553, -0.011154274, 0.0061530825, 0.0095916735, 0.0053560883, -0.015713105, 0.0293387, 0.008785312, 0.01377719, 0.028176472, -0.014875793, -0.02184541, 0.0017602582, 0.07241775, -0.016197274, -0.028952563, 0.013311781, 0.0015488019, 0.0043153437, 0.012460065, 0.01895678, -0.0141759, -0.011076877, -0.0073860306, -0.0062781153, -0.014443665, -0.023949156, -0.027179165, 0.0052728015, 0.001250579, 0.024727989, -0.0035027014, -0.0012862707, -0.010960842, 0.011978686, -0.0027573165, -0.004593882, 0.0093199685, 0.0054172003, 0.018545631, -0.02455841, 0.00072725373, -0.0132844085, 0.010711315, -0.038228024, 0.0117971245, 0.0003236994, 0.009910218, 0.005429036, -0.0045008357, 0.0021798755, -0.0050947256, -0.0022793335, -0.014256404, 0.005549543, -0.0012798831, 0.011319007, 0.03334624, -0.013682741, -0.0017010608, -0.000586401, 0.023773102, -0.008977545, 0.0065017655, 0.048813224, 0.006335601, -0.0020953168, -0.002665612, -0.00047566416, 0.0073680435, -0.0029715402, 0.020743622, -0.025996735, 0.016376548, 0.006460407, 0.008130232, -0.013350997, 0.018807333, -0.020801729, 0.008056637, 0.016793149, 0.0082952585, 0.0010866898, 0.0052407724, 0.0257964, 0.0050551235, -0.0011047185, -0.03010985, -0.009786276, -0.018779859, -0.017371112, 0.009824359, 0.013888898, 0.025149144, -0.015572387, -0.008751936, 0.021099947, -0.01802492, -0.015409208, 0.0028178822, 0.015486653, -0.009482331, 0.0027449203, -0.0005657034, -0.015970059, -0.0013528543, 0.013034281, -0.012586175, 0.0046397396, 0.01687879, -0.00018179293, 0.008704908, 0.0041855252, 0.011189132, -0.010713944, 0.012974531, 0.012062599, 0.028563833, 0.018523248, 0.00046181688, -0.014210556, -0.016646842, 0.0012548931, -0.0007323708, 0.02335168, -0.009212509, 0.01346466, -0.018420387, -0.03340107, 0.009039216, 0.012923408, -0.009577999, -0.0033924342, 0.008030793, -0.015786154, 0.010394841, -0.0016545806, 0.012718305, -0.014757449, -0.0034227283, -0.01354421, -0.02324514, -0.0047550527, 0.006583294, 0.027810218, -0.020303212, -0.0048726387, -0.0036765223, 0.0033750827, -0.0009031966, -0.06655188, -0.0060826507, 0.007245348, -0.0039600553, -0.019209515, -0.021230346, -0.014860525, 0.008811557, 0.0022797382, 0.009979233, 0.010873963, 0.009328736, 0.018138433, 0.0035911053, -0.006298387, -0.012432111, 0.002726255, 0.002995754, 0.0021372286, 0.0035441557, -0.0067119654, 0.018584758, 0.008204802, -0.012095003, -0.0047028917, 0.0073206844, 0.021145884, -0.023823604, -0.009253229, -0.0052792174, -0.008085545, -0.0010764103, 0.0012931525, -0.012707469, -0.010255505, 0.01503907, -0.0058230394, -0.008712172, -0.004822208, -0.01799317, -0.005992814, 0.0031824554, 0.016439615, 0.0035602527, 0.017892202, -0.0018418232, 0.010401832, -0.049790695, -0.022374785, 0.002413226, -0.0054777805, 0.00927701, 0.023632653, 0.02378207, 0.019164313, -0.0030971149, 0.008041407, 0.021756692, -0.009633438, 0.0073090526, -0.011667234, 0.0026886875, 0.01243743, -0.0027855854, 0.0074787997, 0.008086383, 0.016435226, 0.0062525165, 0.001105615, -0.019717412, -0.019207336, -0.0064857076, -0.007361788, -0.0075223213, 0.008047559, -0.01314653, -0.029312829, 0.018609438, -0.034870908, -0.013758676, -0.0053839665, 0.0006913542, -0.0058063543, -0.0039010558, 0.007108209, 0.005524565, -0.08550109, 0.020883182, -0.0059163417, 0.028333817, 0.0044051376, 0.016145537, -0.011733263, -0.0068508247, -0.007460691, 0.0049158414, -0.025885038, 0.0041811634, 0.0071671507, -0.010804688, -0.016702661, -0.003977339, -0.02821977, -0.011596031, 0.006629403, -0.009915087, -0.003242932, -0.013956405, -0.0023929612, 0.018569542, -0.00033004757, -0.013789877, 0.011349419, -0.0012127651, -0.01114088, 0.025034543, -0.012392282, -0.009587353, -0.010839332, -0.005349563, -0.00012739729, -0.008128471, 0.020286314, -0.021772644, 0.009159715, 0.0018286804, 0.012102132, 0.008152071, -0.007002502]], 'text': '\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism', 'metadata': {'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}}, {'id': 'doc_1', 'embedding': [[0.024650585, -0.011758773, -0.009649718, 0.0015418294, -0.02813674, 0.0156663, -0.0075357286, -0.02341131, 0.006145343, -0.0008645317, -0.0028449188, -0.005306468, -0.014150595, 0.01784635, -0.0052605695, 0.022899462, 0.0038172554, 0.010378941, 0.0039757974, -0.017390735, 0.019409308, 0.0004517691, 0.0008956157, -0.0067719333, 0.0024676449, -0.011904432, 0.005525919, -0.011158227, 0.0043630064, 0.01924754, 0.022520497, -0.004620964, -0.010065206, 0.017589668, 0.020629091, -0.0058974107, -0.012119622, 0.00010023837, 0.006649644, 0.0019249993, -0.012980204, 0.005509307, 0.0045090634, -0.0024392554, -0.023438873, 0.0047590435, 0.013970165, 0.027301304, -0.013012879, 0.0027326753, -0.014780703, 0.022754917, 0.003427898, -0.007320337, -0.0070244204, 0.0018058121, 0.0038519697, -0.013286205, 0.00094224425, -0.003696957, -0.020229442, -0.01755959, 0.011664453, -0.019040039, -0.005537244, -0.0028514194, -0.010536698, 0.017636709, -0.009791057, -0.009482628, 0.004020084, 0.006953888, 0.004901557, 0.011899196, -0.00068290316, 0.009552857, 0.024932351, -0.010189577, -0.0121239815, -0.005937898, -0.028962325, -0.011692221, -0.0049095266, -0.0047600614, 0.017951233, -0.0103403, 0.0012559007, -0.0042752423, 0.005865535, -0.0048798644, -0.0036329036, 0.00202491, 0.024998946, -0.0076870285, 0.015706545, 0.0033873874, 0.001975143, 0.009915449, -0.013016656, -0.0036373513, 0.01321495, 0.00037635997, -0.009837682, 0.009527264, -0.00025099472, 0.00059214345, 0.010801517, -0.013834468, 0.00079451525, -0.0061048786, -0.010638296, -0.002001689, -0.02454526, -0.014841611, -0.023954727, 0.01785608, -0.0034570629, -0.003101971, -0.011572829, -0.009364395, -0.017046742, 0.0034324701, 0.014494298, -0.004099663, -0.020977858, -0.010248918, -0.0065102344, 0.007862293, 0.00016020752, -0.01963028, -0.003567971, -0.011589275, -0.0044776956, -0.028034152, 0.033390686, -0.0067528775, 0.010580437, -0.019163886, 0.012964443, 0.013137885, 0.014905251, 0.022747474, -0.00091801275, -0.018638866, -0.010227984, 0.009077526, -0.013679308, 0.018437687, -0.013122674, 0.024623733, -0.0011737758, 0.015575899, -0.021408448, 0.0017165802, 0.00016213564, 0.0070598875, -0.0014086035, 0.003773859, -0.0059913835, 0.016656492, 0.0098843025, 0.010545444, 0.01146507, -0.012155097, -0.001946762, -0.015450689, 0.006333586, 0.0040924647, -0.008979811, 0.009345721, 0.0039328123, 0.004312719, 0.01447583, -0.01712782, -0.0182848, -0.009187233, 0.010745943, -0.007790825, 0.021346202, 0.0072682654, -0.0012399487, 0.0046227043, 0.0150511805, 0.008729776, 0.024315948, -0.005643428, -0.013280568, -0.018721454, -0.017590081, -0.008728505, 0.006358832, -0.02095157, 0.016892456, 0.00857012, 0.012730467, -0.025335126, -0.005144227, -0.028023865, 0.00694037, 0.00316865, -0.0012697884, 0.018051412, 0.008336484, 0.012936121, -0.012404001, 0.01154072, 0.004959737, 0.005093407, -0.014878197, 0.0022620026, -0.016494447, 0.002588019, 0.0023493385, -0.015279171, 0.011784581, 0.00056751963, 0.0019302466, -0.0023044941, -0.020027371, -0.004218349, 0.02086949, -0.00094252446, 0.016869787, 0.011502775, 0.0053092367, 0.0012436829, -0.008367827, -0.01503368, 0.00061105523, -0.008126259, -0.008052051, -0.0041718073, -0.007042015, -0.015036906, 0.013197793, 0.00827717, -0.017446566, -0.025433576, -0.0016341067, 0.035854205, 0.008752815, -0.021206446, -0.013269641, -0.008756433, 0.0052160723, 0.005884686, -0.029580653, 0.01658163, -0.011231026, -0.018450199, 0.016200002, -0.0005134306, 0.008363516, 0.010160395, 0.007588953, -0.0142535465, 0.0171593, -0.008395386, 0.024879353, 0.016946634, 0.0038610445, 0.009535411, 0.005750432, 0.015554668, -0.011083047, -0.0057497527, -0.0011383768, 0.0010168497, 0.020392362, 0.020700535, 0.012421896, -0.0025887361, 0.006625084, -0.020095162, 0.022560667, -0.0071047773, -0.01526503, 0.002700812, 0.008173448, 0.0039555547, 0.00045409345, 0.00014530912, 0.01948128, -0.0005565232, -0.0030875935, 0.010826859, -0.013667157, 0.005521991, 0.0050360677, -0.023993645, -0.009703473, -0.006174823, -0.002024503, -0.015748912, -0.0060548275, 0.008706348, -0.0030442993, -0.0017222089, -0.0119974855, 0.03308187, 0.0131824715, 0.011369481, -0.00074141636, -0.0057808813, -0.006696866, 0.017919723, -0.01654071, -0.00062233047, -0.019212697, -0.010536174, -0.001128113, 0.025566652, 0.006112762, 0.008205819, -0.0060713463, -0.000567683, 0.006814875, -0.03525371, 0.0039469325, -0.0057307384, 0.010689319, -0.0103181, 0.008476038, -0.017329676, -0.007178573, -0.0062715705, -0.011048036, 0.006703323, -0.018269707, 0.00037442474, 0.017466318, -0.02270724, -0.0027304378, 0.0020274522, -0.0026191259, 0.014707714, 0.0014095352, -0.007407078, 0.009784846, -0.02043457, 0.0041970233, 0.004481295, -0.012860927, 0.01014492, -0.007979795, -0.0028388824, 0.01598404, -0.013442382, -0.006084346, -0.00876991, -0.00048672105, 0.002152637, 0.0054729064, -0.0006331224, 0.002178389, -0.01765755, 0.01021316, 0.0066268803, 0.00298444, -0.013046963, 0.0039826087, 0.0023598112, 0.022024246, -0.015402088, -0.017312, 0.009839585, 0.023445599, -0.0012315968, 0.022372162, 0.0012968928, -0.010927448, -0.003073944, -0.00939876, -0.0108240135, -0.024725698, -0.02241151, 0.0049785245, -0.0024709501, -0.011689882, -0.009455493, -0.006360774, 0.007505395, -0.016610883, -0.024323937, -0.009094687, 0.0038970574, 0.00086685264, -0.006353767, 0.0012648725, 0.0002538171, 0.0019195555, 0.0012332796, 0.034365494, -0.0041387477, -0.011508152, 0.005828182, -0.003227856, -0.015386499, -0.0037274163, -2.549902e-05, -0.0109134335, -0.020129723, -0.0095480895, -0.014221377, 0.0102603715, 0.019718386, 0.003132733, -0.015022139, 0.02771066, -0.015662758, -0.00048048244, -0.013097939, 0.019587643, -0.011849393, 0.001089553, -0.02391761, 0.00104432, 0.0073922263, -0.012699175, 0.0050293435, 0.020649707, -0.0037636042, -0.020497264, -0.009826502, -0.016541993, -0.02040705, 0.010765274, -0.014262107, -0.0060280384, -0.020262692, 0.012667848, -0.015867626, 0.009664374, -0.007121096, 0.002983064, -0.007952516, 0.013967337, -0.021440567, -0.0019176106, 0.0030582636, 0.02502124, 0.002640251, 0.0066395462, -0.00070883794, 0.017636362, 0.007825281, -0.0062229317, 0.004153885, 0.049744405, 0.010860195, -0.024766529, 0.010159565, 0.0115967775, -0.015675938, -0.004072838, 0.0062525137, -0.0030220482, 0.010677033, 0.0006317691, 0.0160577, 0.009490273, -0.004031164, -0.004005357, -0.008345215, 0.018292263, 0.001757422, 0.011462229, 0.00044177487, -3.7682756e-05, 0.0030703966, 0.02170441, 0.023380307, -0.0059837825, 0.008619769, -0.002233852, 0.020081317, -0.0085635735, -0.021226987, 0.020458158, 0.0006227896, 0.0047690547, 0.009401614, 0.0036562297, -0.01130226, -0.014667082, 0.009830415, 0.017409839, 0.017135989, 0.0004625158, 0.013033657, -0.0012571914, -0.0045130644, 0.027279573, -0.0020927012, -0.015570748, -0.005234144, -0.008341703, 0.013981452, -0.0046954714, 0.005277365, -0.011427766, 0.020160941, 0.020723265, 0.017633336, 0.013451265, 0.009468983, -0.03741316, 0.009441306, -0.006028936, 0.014455217, -0.009561845, 0.017813789, 0.010209831, -0.014948061, -0.07785587, 0.015593291, 0.014256039, -0.0070257154, 0.010175041, -0.0060185897, -0.00067679246, 0.0046755644, -0.0014999172, -0.0055025294, -0.0001603691, 0.02471165, -0.014644474, 0.0030085621, -0.021322533, -0.0020191453, 0.011021233, 0.0028889372, -0.0045328224, -0.009540496, 0.008359002, 0.014966599, -0.007535908, 0.0067117205, -0.006923878, 0.0020935398, -0.014463309, 0.01818835, -0.013235538, -0.006480777, -0.005498999, 0.0025156152, 0.014794844, -0.0014271431, 0.029499622, 0.0013937155, -0.004339551, 0.008913041, -0.0032282588, 0.0016167576, 0.012267291, 0.0036786553, 0.003835188, 0.017338796, -0.0007026576, -0.0146227125, 0.008909462, 0.008550837, 0.00881309, -0.0026994366, 0.012978294, 0.023783255, 0.0016461075, -0.008120447, 0.00053406483, -0.0035721734, -0.010499173, -0.010273688, 0.010222713, -0.00018703872, -0.008752275, 0.0013246692, -0.0028440026, -0.025922824, -0.010103965, 0.0022827128, 0.010270963, -0.007864481, -0.01065169, 0.0015262137, 0.007892777, -0.0067688664, -0.011120627, 0.021151133, 0.0029174967, -0.00036152362, 0.001818933, -0.00022189044, -0.008367586, -0.009924146, -0.014313014, 0.021244235, 0.006330309, -0.0057081925, -0.01201578, -0.0040754303, 0.0018230039, -0.014574076, 0.003959409, 0.025316868, 0.0134859355, 0.011768497, 0.008021216, -0.0058742804, 2.0092479e-05, -0.009315177, -0.0067908694, 0.011647299, -0.0024687632, -0.020569716, -0.010273274, -0.016779564, -0.0061495295, -0.0036720643, 0.006792056, -0.0041817874, 0.011000474, 0.014346498, 0.0039429134, -0.008993097, 0.0088562295, -0.011327036, -0.0060052862, 0.0010004037, -0.00037667566, -0.0028755504, -0.0009927301, 0.007821436, -0.01056944, -0.0058704717, 0.0019343004, 0.025487509, 0.00026448275, -0.007157655, -0.01531505, 0.011372536, 0.0012330443, -0.0023097405, 0.0042951144, 0.0065810303, 0.019545877, -0.0040004146, -0.01197883, -0.006086195, -0.0017889652, -0.013918839, -0.012250144, 0.0019388484, 0.0039810613, 0.0019107197, 0.004446049, 0.0072314106, -0.0099852355, 0.0027091436, -0.0060282904, -0.0012587516, 0.017342033, 0.01244294, -0.008281095, 0.0069310237, 0.017435499, 0.0038881376, -0.009944036, -0.0029917958, 0.0024724228, -0.023193056, 0.0075922804, 0.027980149, 0.007297351, -0.011053363, 0.00012616903, -0.024430629, 0.01014485, 0.016402975, 0.01880061, 0.0004193141, -0.0024736123, 0.008427056, 0.082614675, 0.0064695096, 0.007298016, -0.026741233, -0.0100516295, 0.00545864, 0.010089988, -0.012535589, -0.012950089, 0.0010516446, -0.012582672, -0.0029230306, -0.01648529, -0.00791886, 0.0075559784, 0.014856711, -0.018659893, 0.004967327, -0.018909503, 0.019059278, 0.0025855296, 0.0022248244, 0.013331189, 0.0029600384, -0.02054491, -0.0022051395, 0.007998279, -0.005780424, -0.0058378004, 0.006412447, -0.018601429, 0.01628103, 0.009604365, -0.015076713, -0.022869652, 0.011165945, 0.016422141, -0.0083927475, 0.02442604, 0.003128538, 0.005627776, 0.01071104, -0.019702313, 0.02776501, -0.013048834, -0.0045457734, -0.0005491915, 0.0011118904, 0.010829523, -0.0074417572, -0.03393221, 0.007080594, -0.025723843, -0.009833164, -0.004216646, 0.0025294372, 0.015471775, -0.0039567035, -0.010025736, -0.019371051, 0.00919239, -0.00019951192, -0.020064356, 0.003373905, 0.022766126, 0.0038710334, 0.0032288758, -0.00905804, -0.008163624, 0.007580776, 0.013039769, -0.013465963, -0.029877454, -0.01380743, -0.021450855, -0.005743437, -0.0019506548, -0.022188775, -0.024561942, -0.0075047067, 0.015915433, -0.002496089, -0.0022194036, 0.0093420455, 0.0058450913, 0.0023905043, -0.0057407473, 0.011103538, 0.005715616, -0.0006189606, -0.010656696, -0.012907918, -0.0012367164, -0.004031152, -0.0115720425, -0.013077332, 0.009685794, -0.007941672, -0.0013083025, 0.0029533615, 0.010576626, 0.01159852, -0.007935293, -0.0063940403, 0.0068601705, 0.07997103, -0.007936003, 0.025927367, -0.012648362, -0.014407748, -0.0075384323, -0.01853972, -0.00029140202, -0.015987832, 0.0027298837, 7.980207e-05, -0.016506594, -0.0088306, -0.011392756, 0.014746822, 0.016808981, -0.003036813, 0.012960287, -0.002076958, -0.000970008, -0.016637366, -0.0017620502, 0.024657166, -0.012687044, -0.008581511, 0.0010024573, -0.010586699, 0.006073346, 0.014763735, -0.010669922, -0.00061407767, -0.005246104, 0.006204302, -0.0092515, 0.000111891255, -0.021683251, -0.01213788, 0.008852322, -0.023751102, 0.004807358, 0.008155269, 0.010476755, -0.0058238413, 0.009504299, 0.0018685577, -0.018992124, -0.001930712, 0.020752383, 0.018422054, -0.0044053346, -0.0010213387, 0.0024709054, -0.0044244393, 0.0062267845, -0.006934462, 0.015416098, -0.0039031836, 0.01155919, -0.0016009188, 0.0031197998, -0.0066270563, -0.02547435, 0.00320787, -0.015181477, -0.0143125905, -0.0051056053, 0.0045753475, 0.012019126, -0.0048986725, 0.01555531, 0.005931204, 0.00046987206, 0.01272409, -0.023971261, -0.025414709, 0.014255101, -0.0052304873, -0.023350107, 0.009808804, 0.021268891, 0.010236161, -0.0074523403, -0.0037132404, 0.0038538605, 0.0014685647, 0.0027499855, 0.0069956225, 0.008943667, 0.016914444, 0.009220031, 0.009517684, -0.001638054, 0.0031863214, 0.011158181, -0.009506941, -0.002784644, -0.028485566, -0.017019002, -0.026662793, 0.002157817, 0.0070591355, -0.0041335635, 0.0023628417, -0.006767201, 0.0014747875, -0.008849151, 0.0027595283, -0.0033821415, -0.022233525, -0.026553206, 0.006021302, 0.0001407085, -0.0076303766, -0.0042976206, -0.008076311, 0.003741882, -0.0077166986, -0.0023540955, 0.017618598, -0.009690156, -0.010631051, 0.030766902, -0.0062621734, 0.009521541, -0.0036630013, 0.005092097, -0.01279989, 0.0037851946, -0.0021644104, 0.0011834305, 0.0109350635, 0.010652772, -0.008830857, 0.0011873479, -0.010360247, 0.008423408, 0.012784887, 0.0051584267, -0.029644702, -0.01277287, 0.0013931186, -0.0073415814, 0.008525537, -0.015756503, 0.013265121, -0.005790862, 0.004489706, 0.0015489502, -0.00095915765, -0.0034306247, 0.006562607, 0.002504453, -0.00016021324, -0.0065464666, -0.0030356974, -0.0116336355, 0.011279087, 0.015088932, 0.013692816, -0.01828461, 0.024494098, 0.01319294, 0.0052140485, 0.011508062, -0.015675386, -0.027106956, -0.0026270098, 0.0073576104, 0.0148248905, 0.005498665, -0.016375428, -0.006041251, 0.009107841, 0.0014870851, -0.013142163, -0.010946793, -0.0025669783, -0.025863536, 0.002838557, 0.0034019363, -0.0013164288, -0.001085557, 0.003960213, -0.013933428, 0.0010908286, 0.0048498013, 0.0076358486, 0.00071938004, 0.019783366, 0.019384488, -0.0073214904, -0.008692196, 0.0005359242, 0.012201952, -0.002565522, -0.0035872434, -0.027174486, 0.00541959, -0.0024949193, 0.022631774, 0.0059177526, 0.0191132, 0.0118204085, 0.009138389, 0.0029013657, 0.0057561104, 0.019384446, 0.003919593, 0.0024897652, 0.0019986446, 0.004439524, -0.0028934448, -0.019234426, 0.009995856, 0.0028509246, -0.0049479255, 0.0010572328, -0.010022012, -0.004184874, 0.002158031, -0.009540895, 0.00253917, 0.0049460293, 0.01792492, 0.0013295767, 0.024893438, -0.0021472587, 0.008613063, 0.002363898, 0.015530087, 0.010899021, -0.0032495887, -0.007249691, -0.010329186, -0.005404834, 0.0040520425, -0.013759408, 0.006236866, -0.002565106, 0.023044586, 0.013848641, -0.005722358, 0.0012068693, 0.010711833, 0.0064940373, -0.02102338, 0.02341631, -0.013771409, 0.016234698, 0.016394652, 0.0056567937, 0.0048685675, -0.022610243, 0.011601274, 0.0170366, 0.0050340593, -0.0013391657, -0.0025305501, -0.013548005, -0.018005127, -0.0005503067, 0.0025662126, -0.01247175, 0.010492653, -0.0054549486, -0.0018824667, 0.008561896, -0.0009705533, 0.0027311859, 0.004343764, 0.0015765087, 0.0069582853, -0.0067531397, -0.0007049479, 0.009138782, -0.0076809656, -0.0053550303, -0.006315569, -0.0076227156, -0.015402777, -0.008566188, -0.0026203548, -0.0069357, -0.001770249, -0.026118545, -0.013852104, -0.008274376, 0.023357522, -0.010311748, 0.016333777, 0.0010905043, -0.008670886, -0.014790505, -0.0029851673, -0.007098005, -0.0174863, 0.0014321124, -0.02474879, 0.016177183, -0.012837197, 0.02326787, 0.010213999, -0.0026216686, -0.014724807, -0.012656549, -0.012769238, 0.003875394, -0.026919683, -0.013806821, 0.0008129987, -0.009038173, -0.01923996, -0.015058658, -0.005056697, -0.004266997, -0.0009580137, -0.0047901617, 0.011376118, 0.0016295078, -0.017305724, -0.01107698, -0.010812007, 0.0029889573, -0.013261186, 0.00043001527, -0.014437622, -0.0061797746, 0.00159526, 0.023116032, -0.003957015, -0.00236525, -0.011524034, 0.017997148, 0.0073419367, 0.011671522, -0.012928389, -0.009751955, -0.0072513744, -0.029261412, 0.010929927, 0.008132799, -0.00058981456, -0.0015857483, -0.0103423, 0.011829488, 0.008653559, -0.0042598043, 0.0070860856, 0.006635982, 0.0008989326, -0.014678642, 0.00032727967, -0.0014711941, 0.0036444087, 0.01588643, 0.008333218, -0.017010462, 0.0021554255, -0.020286826, -0.019523418, 0.008095112, 0.016032645, -0.012280983, 0.012369979, -0.006136923, 0.020175142, -0.0042177825, -0.0025549186, 0.014361642, 0.0044855694, 0.003245199, 0.0057586087, -0.0057915263, -0.012815401, 0.0009951695, -0.0060925675, 0.012847128, 0.015223827, 0.0036385413, -0.012706599, 0.0047368365, 0.0036516578, 0.016270515, 0.015621841, 0.004902306, 0.014490465, -0.0047589503, -0.017332254, -0.0077056545, -0.108758666, 0.0046908474, 0.0060897875, 0.025992189, -0.016251463, 0.029845895, 0.0013788127, -0.026042314, 0.003035594, -0.010008338, 0.00041973963, -0.015608313, -0.012924126, 0.028296798, 0.017263742, 0.0031778116, 0.0014553664, 0.011999222, -0.022911705, -0.0009842282, 0.015403688, -0.0023322315, -0.007049181, 0.0015615479, 0.012497626, 0.011744489, 0.0021446778, -0.005713675, -0.019322107, -0.012440789, 0.016602231, -0.00246582, 0.00862789, 0.00158214, 0.009720992, -0.0041258186, -0.006940577, -0.003456212, 0.009625829, 0.0012221072, -0.005230548, 0.021296084, 0.0034855502, -0.0067337593, 0.0020207032, -0.020912016, 0.020870449, 0.007346268, 0.0071568615, 0.03418926, -0.0074416064, 0.010682762, 0.017955381, -0.013368177, 0.015053427, -0.008524843, -0.002385615, -0.0018442929, 0.008950723, -0.02241298, -0.008061675, 0.009201593, 0.01356917, 0.0031933456, 0.009906601, -0.002769053, -0.011611973, 0.014081587, -0.0071280035, -0.0050839116, 0.0070057786, 0.026668185, 0.015463211, 0.016826073, 0.025118897, -0.0011008609, -0.011456164, 0.008819167, 0.019342393, -0.0044971867, 0.0029068717, -0.00039169774, -0.0037952804, -0.022911722, 0.0006385956, 0.006763237, -0.0037179848, -0.019500956, -0.0003626329, -0.008880975, -0.021791276, -0.010281197, 0.0033795177, 0.012078918, -0.012088669, 0.006969176, -0.0010892756, -0.009160306, 0.000963615, 0.006914509, -0.014275609, -0.02918105, -0.007883266, -0.014573817, 0.0048698895, 0.00029159925, 0.0032844173, -0.0040006973, -0.004303265, 0.0018084589, 0.014087408, 0.006012447, -0.016427353, -0.0036928367, -0.0011468652, -0.01096628, 0.020976944, 0.00067575905, -0.0017771693, 0.011949181, 0.019461945, -0.0063488353, 0.00042749188, 0.03050904, 0.0076896036, -0.007567042, 0.0031415203, 0.002160262, 0.0035628106, 0.012228045, -0.0049166996, 0.0318706, -0.02138061, 0.011163459, -0.013696264, 0.024943188, 0.022584848, -0.004074865, 0.04074487, 0.031233763, 0.018741747, -0.001344685, -0.0063107945, 0.00850058, -0.0019957381, 0.01172102, -0.022882093, 0.019383967, -0.0030823038, 0.0044533587, 0.003985773, -0.00040751684, -0.0093089435, 0.0043664575, -0.015606405, -0.0016101224, 0.007833393, -0.00020638008, -0.011378369, -0.040753026, -0.0067860656, 0.008123245, -0.01818933, 0.0019463135, -0.0017992284, -0.022004066, 0.005873019, 0.019898443, 0.0012879663, -0.0020929023, 0.019050434, 0.01046731, 0.00019167662, -0.016360946, -0.0042686863, 0.013103709, -0.006905516, -0.0031939945, 0.0014777546, -0.012060251, 0.013316306, 0.012093996, 0.01603753, 0.016239626, -0.23210165, 0.0002814636, -0.002031374, 0.011092923, -0.012793672, -0.006942942, 0.006205936, 0.0023526363, -0.0056149717, -0.010763055, 0.029083759, -0.011593183, 0.011375252, -0.018470852, 0.010559353, -0.0003984454, -0.0024016076, 0.025840001, 0.0009407302, -0.008921901, 0.0041371277, 0.0016446474, 0.013728229, -0.0030295416, -0.014311466, 0.014320221, 0.022551969, -0.00388812, -0.0057099643, -0.015591413, -0.00916488, -0.015400053, -0.006662589, 0.0043541654, -0.0092632845, 0.0040760795, -0.019607645, -0.02006532, 0.0005801832, -0.00030057158, 0.012093741, -0.0012891417, -0.0031620837, -0.0074850754, 0.0082084155, 0.003088439, -0.0034017437, -0.018970186, 0.005054252, 0.012811263, -0.008729889, -0.0063661914, -0.0046370593, 0.018801901, -0.023379445, 0.12360465, 0.012034374, 0.00019993559, -0.008913318, 0.017216511, -0.0015818765, 0.008345165, 0.00025952718, 0.0016809744, 0.009415709, -0.007323738, -0.002787488, 0.022615971, -0.00730171, -0.012180294, 0.014178231, 0.011493184, 0.0083789155, -0.017804453, -0.012831556, -0.0019219869, -0.01341166, 0.017242746, -0.017165873, 0.0020217563, 0.0037286163, -0.021942707, 0.009343317, 0.0074558384, 0.015567682, -0.004564769, -0.00682223, -0.00043554205, 0.010251352, -0.005507828, 0.00569256, 0.0111286305, 0.004656917, -0.0036534409, -0.005671023, 0.011193909, -0.017905917, 0.039973587, 0.007192409, -0.008347148, 0.025246615, -0.005947956, 0.0153218275, 0.002185369, -0.001592409, 0.003107042, -0.0034394604, -0.011956805, 0.010901134, 0.0041662124, -0.009140502, -0.0063201874, -0.01766204, -0.0043952987, 0.021482287, 0.0047793244, 0.013142681, -0.007285322, 0.005474566, -0.0017185335, -9.92717e-05, -0.016083365, -0.0023940087, 0.0056883125, -0.019725235, 0.00585642, 0.0061678956, 0.0018673373, -0.0029336864, -0.03127122, 0.014186452, 0.0037298466, -0.0050498866, -0.004297258, 0.0026895166, -0.03356405, -0.00823327, -0.010902843, -0.004626175, 0.0031379883, 0.018768778, -0.017487207, -0.0024653384, 0.0027907328, 0.015174872, 0.0040932246, -0.0031222461, -0.0034862012, 0.0021942633, 0.008928836, 0.018324861, 0.00509369, 0.0001926089, -0.0009404154, -0.0038415452, -0.02133662, -0.005513566, 0.0007372349, -0.012476019, -0.014885371, -0.011869727, 0.00236404, 0.01583198, 0.016119279, -0.01303282, -0.00030165366, 0.018636752, 0.0018718707, -0.0031521711, -0.025368838, 0.013738198, -0.00893939, 0.0004144759, -0.00963649, -0.0011126074, 0.0024821123, -0.0029624808, -0.005400628, 0.01152473, -0.0037020678, -0.014149466, 0.006567377, 0.01295538, -0.021973394, -0.033736102, -0.003779213, -0.013112302, -0.0065099, -0.013675722, 0.0079286, -1.985388e-05, -0.0013785277, 0.0010974227, 0.0036714692, 0.000888625, -0.009654754, 0.00031878275, -0.011439776, 0.015866427, 0.011549376, -0.00064347853, 0.023518905, 0.0045854817, -0.013710956, -0.0076012374, -0.008629639, -0.0049159154, 0.013957074, 0.01615449, 0.008458509, -0.0025395504, 0.0014756086, 0.0032019084, -0.14378347, 0.00700052, 0.0025114657, 0.014050606, 0.011229, -0.008811551, -0.008833295, -0.008302637, 0.005974377, 0.002148529, 0.046008967, 0.008689865, -0.0038894862, -0.010496531, -0.0048281285, 0.0023199557, -0.019735565, -0.006714137, -0.008604606, -0.005368659, 0.011178662, -0.009634517, -0.00024396059, -0.019250643, -0.014191909, -0.002356803, -0.0149870915, -0.006174204, 0.008256472, 0.016699584, 0.015969686, -0.008858216, 0.020440748, 0.008379012, 0.020329678, -0.0006362013, 0.004622362, 0.018992327, -0.0107505275, -3.1543794e-05, -0.0064135464, 0.0042552347, -0.006065215, -0.01768766, 0.022481168, -0.021015791, 0.015773488, -0.0139092775, 0.012057764, 0.0021642873, -0.014833129, 0.0088979965, 0.0011674721, 4.3664288e-05, 0.0019050087, -0.012355632, 0.0059577352, -0.013442794, -0.00026324627, 0.0049975063, -0.018408034, -0.01478176, -0.019838711, -0.0132400235, -0.006126709, -0.005096033, -0.0069395835, -0.06185488, -0.004515279, 0.023387779, -0.0061818096, 0.0038000536, 0.0015074464, 0.0064817625, -0.025321009, -0.018045424, 0.00023862641, 0.011770115, -0.015154402, 0.008816707, -0.023581276, 0.015455951, 0.016144529, 0.010810279, 0.00040893306, -0.007229132, 0.0006981589, 0.009273895, -0.023101764, -0.005171726, 0.010819756, 0.01015814, -0.026957827, -0.0021611885, 0.014260841, 0.0014397484, 0.009085898, -0.013460407, 0.006707247, -0.009282793, 0.011596011, 0.017077519, 0.01028059, -0.018347668, 0.0070813214, 0.02084561, -0.0057082586, 0.030121798, -0.10769065, 0.014848005, -0.007667219, 0.021614164, -0.01005896, 0.00462019, -0.002787934, 0.021976324, -0.01657049, 0.013765207, 0.0055775195, -0.01953699, 0.0047241123, -0.005972639, -0.019819684, 0.0012170336, 0.018148642, 0.0039469134, 0.0008121389, -0.006617257, 0.009295344, 0.0023298797, 0.0029148888, 0.016805256, -0.017017182, -0.017211793, -0.012711649, -0.012750469, -0.0017082647, -0.015453383, -0.023152571, 0.0007445218, 0.011947579, 0.021681504, -0.012029922, -0.0073996685, 0.00974427, 0.012130988, -0.004025212, 0.0031329815, -0.004434202, -0.008492133, 0.022668416, -0.0017957855, -0.0144906575, 0.0033407346, 0.004553136, -0.0016972166, 0.011849032, -0.00047219225, 0.005419629, -0.0068214675, 0.002182829, -0.0023660343, 0.018916337, -0.005906955, 0.008072285, 0.006794319, -0.018326383, 0.0037000685, -0.0073815472, 0.0006459803, 0.024358602, 0.022481583, 0.0074072178, -0.0070546176, -0.023010844, -0.0316105, -0.0001112712, -0.002547572, 0.00017019734, -0.00020075204, 0.014975323, -0.011147011, 0.014185942, 0.014718849, 0.003093941, 0.02726789, -0.010309945, 0.004586933, -0.03460759, -0.006331821, -0.018459069, -0.011993542, 0.004127527, -0.014045736, -0.010697255, 0.0037176614, 0.0032814478, -0.015579722, -0.007286033, -0.01296595, 0.006126617, -0.002532289, -0.020730615, -0.001795491, -0.0037070129, -0.0011009437, 0.007632842, -0.0016610315, 0.0014050942, 0.020027464, 0.008027118, 0.014107425, -0.01839631, 0.02028035, -0.0058701546, 0.0035064246, 0.008372455, -0.0012249989, -0.019440195, 0.002852624, 0.007136402, 0.015121273, 0.025874991, -0.014737301, 0.0056619123, -0.013769693, 0.00067066826, 0.013493601, 0.024227878, 0.005948941, -0.0016807328, -0.015746683, 0.0011111605, -0.008391981, 0.0015755551, 0.0070881033, 0.0035662807, 0.0018866989, 0.010109085, 0.003233672, -0.013267653, -0.01763044, 0.008183259, 0.03215091, 0.019154735, 0.007269247, 0.0029746979, 0.012744845, -0.0027520594, 0.0119661335, -0.010946748, -0.0017124652, 0.022871044, 0.009901044, -0.011172262, 0.014869677, 0.007421724, -0.015569191, 0.002269329, -0.020727731, 0.007579875, -0.009207734, 0.016646996, -0.022226179, 0.0031838971, 0.011131323, -0.11123093, 0.011379117, 0.030291384, -0.00249343, 0.023920693, -0.0019912424, 0.014973213, -0.005087872, 0.0043866495, -0.0032591755, 0.0015232229, 0.02868083, -0.004379501, 0.0058114333, -0.002855376, -0.004002764, -0.0042261537, 0.012759117, -0.0033092666, 0.011500442, 0.020671485, 0.01966356, -0.0040639634, -0.0028387813, 0.0122204255, -0.0055389386, -0.009247467, 0.00984452, 0.0015712372, -0.016094564, -0.0025117798, 0.00055823324, 0.0030890228, -0.0075455313, -0.0151629485, -0.017431444, 0.028846642, 0.007498003, -0.020072903, 0.013902455, -0.016444182, -0.02263704, 0.008251899, 0.00080556876, 0.007589371, -0.009922306, -0.0033057989, 0.008837153, 0.020975176, -0.025976183, -0.018399615, 0.009240332, 0.015322863, -0.01755018, -0.01359308, -0.012721597, -0.007763944, 0.0030271865, -0.026849126, 0.0022407467, -0.009645875, 0.013937686, 0.015202226, -0.010069208, 0.011638026, -0.008553677, -0.0067684934, -0.0015424349, 0.015637828, 0.007231976, 0.024710367, 0.01558496, 0.0041352776, -0.005670972, 0.0061198915, -4.432785e-05, 0.0065268613, -0.0033713104, -0.0005287884, 0.00031479413, 0.011696489, -0.018548295, 0.01205128, 0.011445809, 0.018289538, -0.030538471, 0.0009920427, -0.0055019474, -0.16278584, 0.01802407, -0.02934813, -0.002251653, -0.0085655125, -0.01145617, 0.017298779, 0.0032682882, 0.009288911, 0.0016235546, 0.0033888335, -0.004276181, -0.010130818, -0.0048534325, 0.00018903069, -0.0017072666, -0.0103366235, -0.0143569, 0.0011559444, -0.0039298763, -0.014721668, 0.0015494229, 0.0147214625, -0.017725768, -0.0026955386, 0.0130809, 0.018756723, -0.02938419, 0.01480262, -0.019042185, 0.010150673, -0.0029220341, 0.0049653878, 0.0047702743, 0.005346089, 0.0033648552, -0.017763995, 0.004666192, 0.012462698, -0.010620876, -0.0184986, -0.026070783, -0.016410718, 0.004802399, -0.0010052692, -0.0027787867, 0.005995467, 0.007863659, -0.0030216551, 0.001442242, 0.0026913655, -0.0031074178, -0.015946867, 0.016913077, -0.0033883366, -0.0019372809, 0.0018698772, -0.015904803, 0.00077892764, 0.0048960946, -0.004870974, -0.008480346, 0.0034331048, 0.00029538153, 0.012423152, 0.013474561, 0.012883684, 0.007098449, 0.0055412753, -0.013681568, 0.0006950326, -0.0019086064, -0.0057562706, 0.0035247256, 0.0105213, -0.08692545, -0.0065650083, 0.00452463, 0.0038720034, 0.0052955057, 0.0033857787, -0.018793847, -0.0035011142, 0.009338948, 0.01419486, -0.0013063253, -0.005180474, 0.010639802, -0.010220105, -0.010463628, 0.02166017, -0.00013676797, 0.0064485236, -0.008759022, -0.08216888, -0.0069218124, 0.012860096, -0.0066265217, 0.01468561, -0.0052790157, 0.03735012, 0.005608294, -0.0019436983, -0.026901728, -0.018180927, 0.010175928, 0.0031178968, 0.022259044, 0.0031962027, -0.03222548, 0.010523185, -0.016070958, 0.001377197, 0.00073713576, -0.010088358, -0.035216708, -0.01339996, 0.019811908, 0.0025970463, 0.0014813261, 0.024463981, -0.017585171, 0.00040627763, 0.0033063681, 0.016759045, -0.00427087, -0.009764608, -0.0013122022, -0.0057912376, 0.019784227, -0.0074767554, -0.013609264, -0.0023065798, -0.0038511087, -0.019003393, 0.013681583, 0.0128368195, 0.0048637935, -0.017119493, 0.0077821836, 0.0028205742, -0.00096436375, 0.019471452, 0.0109354295, -0.008751875, 0.013295696, -0.006648906, 0.024183521, 0.0051173186, 0.019630847, -0.002118264, -0.0179201, 0.019334974, 0.0008796419, -0.021116974, 0.010731982, 0.010706683, -0.1513139, -0.0021537645, -0.029681085, -0.017248249, 0.004392796, 0.010152796, 0.014999525, 0.0012052407, 0.007654118, -0.008302388, -0.005390937, 0.017018545, 0.008896297, -0.004533281, -0.009967321, -0.014854036, -0.0008897507, -0.0056024115, 0.017322503, -0.012547143, -0.0098491665, -0.012785001, 0.0074306326, 0.010653038, -0.00054165034, 0.007244701, 0.002822548, -0.014203737, 0.003538647, -0.0029082028, 0.0072445045, -0.018902386, 0.008500374, -0.0075839483, 0.0031538818, 0.010102817, -0.0038960706, -0.00861004, 0.03151292, -0.0059666154, 0.0149801, -0.010593073, -0.013363123, 0.0021759912, 0.0055262987, 0.0042432984, -0.0047703413, -0.0036616172, 0.021775471, -0.006132377, 0.014431887, 0.016230175, 0.015398428, 0.0010249208, -0.0114391865, -0.0036398612, -0.036338054, -0.0048167175, -0.034462363, -0.00739901, -0.030067937, 0.0010783751, 0.005949635, 0.017079461, -0.0068186545, 0.002185313, -0.00146108, 0.004248305, -0.00025716244, 0.014310577, -0.007955184, 0.0023386942, -0.017855385, 0.008982694, 0.015941478, 0.0011315112, -0.0020345363, -0.007877904, -0.009930607, 0.0043885224, 0.00032929808, 0.0072487146, 0.0011676102, 0.0015498754, 0.011910876, 0.010737142, -0.015857952, -0.010423306, -0.0044065667, 0.024183799, 0.027248783, 0.0069485125, -0.012867703, 0.008291284, 0.02839215, 0.0034306117, -0.0010524028, 0.0022778006, 0.004057772, -0.012719462, -0.05428528, -0.01099855, 0.022827988, -0.02440128, -0.013541406, -0.008112857, 0.014674865, -0.03263599, 0.001284387, -0.0049275663, 0.012968324, 0.009834153, -0.0010110488, 0.030981876, 0.0043735025, -0.018352356, -0.012437529, 0.014130707, 0.006272945, -0.0030131806, -0.012800612, -0.005834587, -0.010008907, -0.008708517, -0.013977557, -0.04983802, 0.00081408635, -0.009377444, -0.024846772, 0.011351303, -0.0037396564, 0.025293045, -0.014955936, 0.0055544483, -0.002749298, -0.020497406, 0.00042035207, -0.002263398, 0.0030263648, -0.03141711, 0.006408606, 0.0014873742, -0.0010388326, 0.011155488, -0.0047041066, -0.007916288, 0.0020744698, 0.009550167, 0.021623781, 0.019480098, 0.035073906, -0.0025552155, 0.0030678462, -0.0058933543, -0.0026808719, -0.016368544, 0.010664061, 0.0063560745, 0.0014764831, 0.0042697606, -0.007178779, 0.010084845, -0.005098255, -0.019688947, 0.003759031, 0.015231986, -0.00901732, -0.011549939, 0.008998739, -0.006240046, -0.011815717, 0.0027796638, -0.014701335, -0.019436194, -0.020861646, 0.009048664, 0.0058111707, -0.0020666388, 0.011149287, -0.0032752005, 0.010168782, 0.0018205068, 0.009285473, 0.016190745, -0.0038914795, 0.023122106, 0.015708093, 0.012553006, -0.030521192, 0.02305456, 0.0069936886, -0.0028182147, 0.0045364955, 0.004368539, 0.009265824, 0.007231369, 0.0060478123, -0.007498548, -0.013773733, 0.0030167205, -0.007870682, 0.017607477, -0.006661978, -0.013063992, 0.025063114, 0.0018708733, -0.0028595796, -0.0031171, 0.0047954503, -0.0054658772, -0.0075080693, 0.021783423, 0.014580368, 0.00434255, -0.00068566855, -0.008348751, -0.0032373462, 0.008999361, -0.010130434, 0.0019447206, 0.0046563502, 0.00035228356, -0.027235327, -0.020663206, 0.020395832, -0.010420928, -0.00024579692, -0.007148536, 0.009270718, 0.026296278, -0.0029046591, 0.0023778782, -0.014139384, 0.003841791, -0.0044132043, 0.009018242, 0.025695128, 0.020616738, -0.016443232, -0.0008056395, -0.0133156255, 0.00561022, -0.0029546805, -0.01831461, -0.021733522, -0.00083041866, 0.01766166, 0.01574723, -0.002095904, -0.009212792, 0.00584547, -0.00452298, -0.0010547229, 0.015808204, 5.479182e-05, -0.006151155, -0.013530177, 0.012975508, -0.0073436243, 0.015376771, -0.006692192, -0.009011986, 0.012411285, -0.02752481, -0.004474406, 0.00886122, -0.008488071, 0.002578422, -0.010511326, -0.020606775, -0.0057056723, -0.009016358, 0.0086485185, -0.073331065, 0.017554777, -0.001739576, -0.003321006, -0.018755037, -0.0050988374, 0.009038653, 0.005999451, 0.008888213, -0.01956668, 0.0005484984, -0.017685136, -0.008248238, 0.005165375, -0.010654031, 0.0002787259, 0.002468112, -0.008324624, 0.0071966397, -0.006799183, 0.006730541, 0.0115004545, 0.005606791, 0.0047439625, 0.0049323216, -0.0052882456, -0.012666165, 0.0062269457, -0.013711337, 0.013771975, -0.0052625355, 0.0049885223, 0.0013132308, 0.012814527, -0.0025802902, 0.0055707744, 0.02138596, 0.010832763, 0.001752989, -0.01612571, -0.00651403, 0.008712747, 0.025248965, -0.022651076, 0.008781795, 0.018502215, 0.009859757, -0.019119574, -0.003938775, 0.0022005131, -0.01499845, 0.01078877, -0.01818432, 0.0010840214, -0.007564258, -0.006420231, -0.013218086, 0.0025804378, 0.0064842817, -0.006436143, 0.0042692255, -0.0022938491, -0.0018862903, 0.00028770367, -0.013013321, -0.020386165, -0.017590398, -0.00894947, -0.005806323, 0.010247797, -0.01571277, 0.005211033, -0.019321712, 0.020331668, 0.006606829, -0.017417414, 0.009218145, 0.008128041, -0.007701268, 0.02939948, 0.0043250956, 0.00721288, 0.023985604, -0.029501067, -0.012671834, 0.0024042148, 0.005729807, -0.0027180114, -0.0022870544, 0.005516562, -0.010340836, -0.005571746, 0.012699933, 0.01653855, 0.0065062973, -0.0061746496, -0.0009936376, 0.0057886713, -0.0061642034, 0.01638994, 0.020077957, -0.014719785, -0.004588829, 0.012911957, 0.005127844, -0.0032084119, -0.0137935905, -0.009117836, 0.009200338, -0.008851296, -0.013783016, -0.0034131573, -0.0010356228, 0.01708159, -0.012542442, 0.008395256, -0.00451775, -0.008054347, -0.0023162975, -0.0067912797, 0.011750168, 0.013443066, -0.01628982, -0.0070658326, -0.0038698833, 0.012363404, 0.004079124, -0.0030709389, -0.0026766206, 0.026585963, 0.02349103, 0.014328287, -0.010886698, 0.009269899, 0.018806232, -0.036686547, 0.000847303, 0.006705511, -0.01681564, 0.03884837, -0.005881401, 0.022063637, 0.01923853, 0.0041558864, -0.02721594, -0.011027233, 0.0199021, -0.0142664295, -0.018611604, -0.008896239, 0.012393969, -0.02116985, -0.016242322, 0.004647193, 0.016543197, 0.0017240073, -0.018109407, 0.013720447, 0.011245169, 0.0011666057, -0.017042195, 0.017822064, -0.0020600248, -0.01956797, 0.003324722, 0.011196764, 0.019274032, -0.020156467, -0.008179557, -0.005812769, 0.0029308326, 0.0073990333, 0.017103607, 0.011115812, -0.0036035655, 0.0003212074, 0.055027314, -0.00090982515, -0.009692582, -0.014778336, -0.009154704, -0.011993155, 0.0002199801, -0.0016745474, -0.0044815475, 0.00021859477, -0.012965768, 0.008410588, -0.018804455, 0.0035807977, 0.005082899, -0.01413326, 0.003803132, 0.004368882, -0.009738623, 0.03042383, -0.012635316, -0.010873923, 0.01031827, -0.013415658, 0.011865938, 0.015489344, -0.017165279, -0.0111452835, -0.0012169536, 0.015535799, -0.019505816, 0.013487345, 0.0018025743, -0.008159177, 0.013237394, 0.0031407373, -0.0037211394, -0.00035198315, -0.00030488233, 0.020615611, 0.010003903, -0.0050911265, 0.027830552, -0.009689356, -0.00046813398, 0.004771463, 0.011648281, 0.0030079854, 0.002604129, 0.0062964642, -0.006875981, -0.0008981041, 0.0036828485, -0.022532588, -0.0072951806, 0.020340452, -0.009352191, -0.004733296, 0.008160445, 0.017668327, -0.0069079283, 0.005927915, -0.021925768, -0.00045019065, -0.011800252, -0.008333763, 0.00049178343, -0.014974391, 0.0077579203, 0.00790012, -0.026435267, -0.016021246, 0.025299542, 0.0015938337, 0.005348909, -0.00821029, 0.00025062112, 0.010509617, -0.08970601, -0.0072105755, 0.0021032775, 0.003441297, 0.00196213, -0.00360752, -0.0008955262, 0.0073669595, 0.0115998015, -0.013555582, -0.017828709, -0.05794153, -0.012412194, 0.00086576695, 0.01036691, -0.011200936, 0.006217387, 0.00045704187, -0.010012044, -0.025222274, 0.0045405687, -0.0033268821, -0.021307765, 0.01663513, 0.008335325, -0.015390659, -0.0038760994, 0.00410142, 0.0028953082, -0.0073897005, -0.012088313, 0.013617101, -0.012937822, -0.01605944, -0.013487068, -0.009597507, -0.0004546718, -0.0045205, 0.0011112117, 0.017232884, 0.0045541497, -0.0056962525, 0.0020189378, -0.0133079905, 0.013503779, -0.0004362515, -0.025955414, -0.021962108, -0.0100824125, -0.116927214, -0.002435128, -0.025532749, -0.010554046, 0.012913952, 5.1568244e-05, 0.014482685, 0.0018142889, 0.0022545373, -0.004235896, -0.01239882, 0.017979367, 0.0005073264, 0.018219648, 0.008192673, -0.013152573, 0.012807545, -0.012528315, 0.007629365, -0.009054187, -0.0060603484, 0.007272475, -0.017419836, -0.007509726, -0.03250382, 0.007765537, 0.015673507, 0.014915443, 0.022248318, 0.006223548, 0.014565025, 0.0031017098, 0.007331378, 0.0076735, 0.014764242, 0.022704868, 0.0032389904, -0.004760033, -0.013135675, -0.01905204, -0.007443423, -0.0066934284, -0.0010392094, 0.03172691, 0.021239242, 0.0028016933, 0.0102102095, 0.024086494, 0.016622474, -0.009993141, 0.022940008, -0.005086944, -0.010572042, -0.0105143, 0.0015068143, -0.0025573247, 0.012694958, 0.01769295, 0.011539462, -0.0035411387, 0.009785746, 0.0074557085, 0.011797493, 0.0038767965, 0.009890411, 0.021102434, 0.0030296461, -0.014999492, 0.03263431, -0.001374194, -0.0057030735, -0.016304074, -0.017825512, -0.009613478, 0.006449117, -0.0025377367, 0.014012178, -0.009455447, -0.02287029, 0.004002979, 0.010883724, -0.005937478, -0.008006499, -0.009048687, -0.0018759959, -0.0122735165, 0.008972477, -0.002829249, -0.01264649, -0.019507647, -0.005130387, -0.013651526, -0.0061828243, -0.010727431, -0.00043377274, -0.0016427281, 0.0073782075, 0.0016297066, 0.016476747, -0.013700485, -0.011553254, 0.009287234, 0.020074211, 0.0035845607, -0.0077003515, 0.006464475, -0.010916307, 0.0008896169, -0.018203197, -0.013459871, -0.015363078, 0.0002919672, 0.019268837, -0.014072545, 0.01074034, 0.02250165, 0.010467285, -0.00061077176, 0.0020405657, -0.0018329138, -0.015811399, 0.0084512215, 0.003521016, 0.017287899, -0.016342837, 0.011689048, 0.0118261315, 0.011418718, 0.0008161889, 0.022454102, 0.0048710927, 0.0009094768, 0.0015565126, -0.020790609, 0.009339004, 0.010453527, -0.033961, -0.009604033, -0.005423088, -0.007834003, -0.036799107, -0.008740979, -0.013260247, -0.0032811211, -0.0011398231, -0.0017864477, 0.014638638, 0.0028807374, -0.020195836, 0.009098558, 0.0054324483, -0.008355803, -0.013422267, -0.009706862, -0.0046763807, 0.006299755, 0.003287041, 0.0037734173, -0.0044995137, 0.006228377, -0.009152244, 0.011850459, -0.010919588, 0.010331411, -0.002510081, -0.020463709, -0.044321846, -0.0048674596, -0.00062570703, 0.0041340794, 0.0031426195, -0.011329986, -0.004921814, 0.011568623, 0.008985267, 0.0048668124, -0.017714519, -0.0053056967, -0.005569188, 4.689868e-05, -0.009070766, -0.004962028, -0.0076865377, -0.006318725, 0.02172651, -0.0061149443, 0.02541699, -0.01908179, 0.014308549, 0.011240459, 0.017392872, 0.009581692, -0.019792635, 0.015162397, 0.010276879, 0.022208055, -0.007708727, 0.013086792, -0.001787636, -0.004892741, 0.0022913357, -0.008160757, 0.015814144, -0.0211392, -0.0010526296, 0.010896575, -0.009976255, -0.020277072, 0.022834798, -0.02523276, 0.014807096, -0.022407696, -0.006924785, -0.024583083, 0.02453942, 0.00050536136, 0.0012325195, -0.018405205, -0.019571805, 0.008143109, -0.000843075, -0.019915532, 0.0081829, 0.010559039, -0.0130229695, -0.025564464, 0.0062579326, -0.012145964, 0.026012637, 0.008883999, -0.0015994099, 0.0068310467, 0.011916881, 0.017599845, 0.02127478, -0.012775173, -0.0039250804, 0.02451595, 0.003981401, -0.018139604, 0.00057476806, 0.002546812, -0.022760227, -0.0049971268, -0.009058805, -0.0012839958, 0.0014366407, 0.01210438, 0.0034851688, -0.0041724476, 0.021023087, -0.011862487, 0.009901697, 0.002115541, 0.003243868, -0.007317657, 0.004278195, -0.0072401757, 0.01645014, 0.0018146717, 0.0032567375, 0.010348699, 0.0014775136, 0.015567046, -0.023974964, 0.021224394, 0.0039081825, 0.0026673956, -0.010350828, -0.029778454, 0.0018114384, 0.022211067, -0.018953437, -0.016195852, -0.007923244, -0.009142666, 0.014235908, -0.0012773095, 0.010857271, -0.005938615, 0.008055761, -0.013840976, -0.0005847492, 0.011950245, -0.01946747, 0.007857496, 0.015396379, 0.012845636, 0.014257666, -0.00561475, -0.011581639, 0.013339351, -0.0049730674, -0.008638386, -0.004759572, -0.005243656, -0.010540907, 0.012563859, 0.007885824, -0.020898784, 0.005017116, 0.0009500532, 0.0074113486, 0.011058377, -0.0015977079, -0.0027267288, -0.0017256348, -0.001630697, 0.0022037074, -0.00104734, -0.015605027, 0.0036194331, -0.003730035, -0.006583232, -0.012319714, 0.002106251, -0.033706356, -0.01648196, -0.010785816, 0.0024460151, -0.010519114, -0.008894623, 0.0051812255, -0.0066923946, -0.0016770452, -0.0061707096, 0.00057395635, 0.0025107234, -0.0073350174, -0.016522883, -0.0072947033, 0.022548664, -0.0147349285, -0.0038268897, -0.0070150536, 0.009559543, -0.012611973, -0.00062244426, 0.003967004, 0.002680597, 0.004324685, -0.0033312917, 0.02234303, 0.012097745, 0.029123757, -0.018299796, -0.004817151, 0.012858836, -0.0003262069, -0.0043839356, 0.0002919151, 0.0030628445, 0.017538665, 0.0050504184, -0.0039871717, 0.011493147, -0.0040227715, 0.0032495453, -0.010112182, -0.023743935, -0.0155220255, 0.007948305, -0.022090845, 0.0060977554, 0.009562732, 0.0045983507, -0.008917651, 0.0010465326, 5.6768513e-05, 0.0042200782, 0.005119491, 0.020808425, 0.00087220076, 0.0026212859, 0.040733203, 0.012196448, 0.016554141, 0.021081958, 0.007911907, -0.006745215, -0.0030955258, -0.0063985344, 0.009831003, -0.006737534, 0.009227179, 0.000424834, -0.012328126, 0.013018954, 0.015662769, -0.013321472, 0.0063210893, 0.006403176, 0.028305257, 0.009670135, 0.0009900322, 0.005948595, 0.021290282, -0.005391368, -0.0031380705, -0.009145689, -0.0031120202, 0.013287525, -0.0061155683, -0.00093222316, 0.012636712, -0.011843304, -0.009735127, 0.012450275, 0.008971063, -0.009044348, -0.005931095, -0.010592724, -0.0020713806, 0.014096074, 0.005393997, -0.019864088, 0.011218789, -0.011061715, -0.006374208, -0.007127287, 0.019123532, -0.0083469795, -0.0033058277, 0.020758348, -0.013844844, 0.01160757, 0.01049874, -0.008156063, -0.006745117, -0.006486701, -0.0024851684, 0.010498945, -0.016628554, 0.0013386814, -0.020552184, -0.006789056, 0.0200779, -0.017696392, -0.0022367567, 0.004528929, 0.009816713, 0.00036904585, -0.012394041, -0.011570147, -0.010880163, 0.007912728, 0.011365463, 0.014994153, -0.004197934, -0.0056265946, -0.0014870241, 0.014978134, -0.0064785853, 0.009886225, -0.033358928, 0.015750552, -0.011474366, -0.00083620177, -0.0018249234, 0.01154488, 0.0017876013, -0.004401876, 0.0018586566, -0.0021804885, -0.0031309954, 0.022076797, -0.0020924537, -0.009289488, 0.014672082, -0.0065858886, -0.0018248726, 0.005274223, 0.022436412, 0.0008880394, -0.010627927, -0.005378732, 0.01191817, -0.0019518554, 0.01994681, -0.013170669, 0.0115793, -0.00048698645, 0.013935975, -0.0023148227, -0.00045423146, -0.017464288, -0.0026210216, 0.0015977505, -0.0052537587, 0.014860036, 0.021468284, 0.00015870019, 0.025285607, -0.020304626, -0.0040595834, 0.0051947464, 0.009649779, -0.008344847, 0.0028145984, 0.0012765481, -0.006970716, 0.00083548174, -0.030648915, 0.015593246, -0.0011780366, -0.0010595528, 0.010668892, 0.011037497, -0.010886996, -0.011030991, -0.00041711095, -0.00592124, 0.019850666, 0.0067154244, 0.013906439, -0.01600312, 0.008907202, -0.012649839, 0.021836512, -0.011104498, -0.005891061, -0.0008193323, 0.004521396, 0.020738376, 0.012976599, 0.015318538, 0.015429596, 0.012445942, 0.02384232, -0.011360314, 0.0027145327, 0.032174043, -0.008202627, -0.010605842, -0.0050603105, 0.04009519, -0.010299249, -0.034511916, -0.009832994, -2.0551715e-05, -0.013973888, 0.009807086, -0.011098552, 0.0012082809, -0.008306508, 0.0085731475, 0.0026658457, 0.011721984, 0.003074699, 0.00072413177, 0.01446694, -0.0018519067, 0.00051892974, -0.0019671214, 0.019992918, 0.0010669224, 0.0029150161, 0.0031648143, -0.00218812, -0.004511016, 0.024734331, 0.014712928, 0.0002298807, 0.026299542, 0.018499525, -0.006916264, -0.0040486334, 0.013186585, -0.023006754, -0.01668465, 0.0045584487, 0.0113904895, -0.012408796, -0.008900346, 0.013870473, 0.0012581005, -0.001868114, -0.0023673056, -0.01747945, -0.016891524, -0.015923258, 0.23707226, 0.01785413, 0.011962616, -0.011876034, -0.000919363, -0.008201382, -0.011779734, -0.0026606335, 0.005297311, -0.013407733, 0.011178792, 0.009874377, 0.012733675, 0.00059723837, 0.027477324, -0.012150627, -0.0060049947, 0.0028452426, 0.004435045, 0.006455892, -0.006981631, -0.006688197, -0.01697664, -0.013837072, 0.00024242558, 0.013656008, 0.017684067, 0.0009515627, 0.024486512, 0.0005779203, -0.012022108, 0.018448165, 0.0051667877, -0.008907563, -0.020739939, 0.020644823, -0.0038197325, -0.009202234, -0.0054430505, 0.014691544, 0.0032656842, 0.0051010265, -0.013275313, -0.00794383, -0.005625319, -0.017918585, -0.0051669814, -0.01874876, -0.0060636187, 0.025382213, -0.008800424, 0.011099878, 0.026719188, 0.0035715494, -0.013329729, -0.002038581, 0.00061011605, -0.018331874, -0.009596858, 0.0020410072, 0.0005437411, 0.012256868, 0.009681878, -0.007822557, -0.0039063888, -1.7850653e-06, 0.023649113, -0.040590443, 0.030590162, 0.024323126, -0.006790457, -0.0012844938, -0.0009915017, 0.0091198115, -0.005922277, 0.016333781, 0.019867463, 0.030208714, -0.0042645405, -0.013931514, -0.016954852, -0.0072497055, 0.019434469, 0.005905998, -0.00057549635, 0.011656742, -0.006595808, -0.002231914, 0.0030148642, 0.0037839117, -0.0072411406, -0.009199502, 0.016168281, -0.017914718, -0.015018132, 0.024352739, 0.0038074758, 0.01757607, 0.0015586185, 0.006334507, 0.018899791, 0.020614758, 0.012142484, 0.00015317602, 0.009946539, -0.0062306295, 0.0033889483, -0.0021471696, 0.0032322868, 0.003981992, -0.0065892753, -0.02269128, 0.016207857, 0.007191927, -0.0068275533, -0.009793907, 0.0022963595, 0.0113824345, -0.0099668205, 0.0032817067, -0.004233896, -0.009546334, -0.005333068, -0.005280523, -0.001514892, 0.0055754343, 0.021452527, 0.0059977435, -0.00089319766, -0.0054022004, -0.013366445, -0.0031899556, 0.00070334726, -0.009494693, -0.006352249, 0.012683297, 0.012268753, 0.018399898, 0.0062214485, -0.00218307, 0.007976727, -0.007976068, 0.025346518, -0.016214492, 0.00028523183, 0.01982634, -0.0060887136, -0.007570619, -0.031384975, 0.0056156046, -0.025761988, -0.003046237, 0.0020427825, -0.011700609, 0.007611473, 0.020848643, 0.023275396, 0.0047228197, -0.015404372, -0.013558578, -0.0035543037, 0.028485088, -0.016673366, -0.0027072052, -0.016625574, 0.012292288, -0.01870726, 0.00551974, 0.010528243, -0.0002906275, 0.012085971, 0.00061620126, -0.0062666945, 0.014289942, -0.022852466, -0.017394206, 0.0029962044, -0.0060830424, -0.0107660135, -0.010160217, 0.020605644, -0.00038125712, 0.029864416, 0.017771985, 0.01980336, 0.006077454, 0.00039568788, -0.0030313113, -0.0012258535, -0.0014939999, 0.0036261317, -0.018697781, -0.009900464, 0.006622823, 0.0073801894, 0.0056045596, 0.0015605084, -0.0068232855, 0.007053432, 0.0091294125, 0.010616601, -0.004023773, -0.0035493313, 0.13291317, 0.004308254, 0.010024197, 0.007332356, 0.0062975134, 0.0035194329, 0.008643778, -0.01136699, 0.01190065, -0.0027466768, -0.0052428246, 0.015258209, -0.004771492, -0.0038962394, 0.0008475253, -0.0087225055, 0.00093008083, -0.0068959855, -0.0060778684, -0.004443482, 0.020596009, -0.0061363345, -0.0062493957, -0.0061759986, 0.0080099115, 0.024200136, -0.0067987083, -0.023133956, -0.010222308, -0.0018790663, 0.006897206, 0.016448062, -0.0026752253, 0.0088007515, 0.005093156, -0.020070957, -0.006845793, 0.00533517, 0.000577526, -0.007969653, 0.0024350088, 0.014792306, -0.007781129, -0.003248378, 0.005867591, -0.002371425, -0.019629268, 0.0153322965, -0.0051541952, -0.005247667, -0.04957391, 0.006660629, -0.005826581, 0.022664996, 0.005038884, 0.0038106248, -0.0012553183, 0.00026457798, 0.013318992, -0.022727447, -0.0017075001, -0.024080448, 0.009999273, 0.0026452546, 0.008504806, -0.011804018, -0.015281253, 0.0034535679, 0.009660873, 0.0148488805, 0.0030590568, 0.015254673, -0.012504167, 0.00093055185, 0.00618778, -0.010706129, -0.003149042, -0.0016822771, -0.014453762, 0.01412877, 0.0026664953, -0.0051305564, -0.0130097885, -0.012744129, -0.022452164, -0.020735407, -0.019997977, 0.007376011, 0.02192801, 0.0038956776, -0.013519596, 0.0042527397, 0.015317776, 0.006850292, -4.3598746e-05, 0.0012597769, 0.019074626, -0.01228066, 0.010708761, -0.0033695172, 0.0036794054, -0.00024028623, -0.00970593, -0.0008617909, -0.010454765, -0.016851615, 0.0014722429, -0.0033072317, -0.008265191, -0.010982126, -0.004845531, -0.005329643, -0.11846537, 0.0045036296, 0.007985734, -0.001035405, 0.010645922, 0.01722545, 0.013982706, 0.017229805, -0.03225423, 0.010617064, 0.0077262535, 0.0014954243, -0.0116283065, -0.008718758, -0.02665387, 0.011601927, -0.0019835632, 0.012522053, -0.02153921, -0.0069775367, 0.017061425, 0.011123478, 0.013253617, -0.009817406, 0.00035768255, -0.032371204, -0.049418923, 0.004876588, 0.00017057294, 0.009203697, 0.015630102, 0.01460928, -0.025074279, -0.0046231532, 0.0057207644, -0.004520541, -0.0054525584, 0.00067153934, 0.0028427115, -0.0069234287, -0.018388016, 0.0013742393, 0.037775643, 0.020748073, 0.0027657184, -0.013312092, 0.008715633, 0.01139462, -0.021612726, 0.016255492, -0.0033730941, 0.008635557, -0.014165877, 0.015302126, -0.0106057, 0.0060572727, 0.024920907, 0.016979624, 0.010705437, 0.031092744, 0.0030618652, 0.00063561316, -0.0058619636, -0.009962405, 0.0019974122, -0.0055624186, -0.0022473577, -0.027951889, 0.005539608, 0.014831237, 0.011260729, -0.009395631, 0.017234234, 0.01761937, -0.01326787, -0.0013398938, 0.0042629433, -0.009584217, -0.033099987, 0.0013863513, 0.0048772483, -0.026210012, -0.022654047, 0.0043949503, -0.010836743, -0.018047823, -0.00038739652, 0.016052539, 0.0065184506, 0.0077259475, -0.0004731728, 0.008586173, -0.007844068, -0.0063106352, -0.010248526, -0.003412324, 0.013420161, 0.0004182718, -0.0019869641, -0.028948497, -0.0051472397, -0.007168782, -0.012916455, -0.02753299, -0.006532813, -0.0030142397, -0.02299461, 0.00901387, 0.0129358, 0.0023079473, 0.0019083733, 0.008292439, -0.027692636, -0.010414907, 0.0071472954, -0.006874407, -0.010707075, -0.00069037953, -0.012470738, 0.0071791, 0.0020543532, -6.4754066e-05, -0.018872892, 0.015484863, -0.009769825, 0.006338826, 0.0145656, 0.0048754867, -0.09730752, 0.00889713, 0.008901125, -0.017322877, -0.007759772, -0.00870619, -0.002423772, -0.011794845, 2.9036826e-05, -0.0038516184, 0.022372797, 0.0012425919, 0.00037839985, 0.005017304, -0.005904142, 0.0023275844, -0.00093523384, 0.0005647978, -0.005233186, 0.00353627, -0.015987026, -0.01647326, 0.0042716535, 0.014308382, -0.0037600498, -0.007694495, -0.003336525, 0.005067631, -0.008398775, -0.021900749, -0.002139922, -0.011279007, -0.008110934, 0.00082210865, -0.018738383, -0.014577271, 0.0052118404, 0.0010352678, 0.0035372581, 0.013852471, -0.006470281, 0.020050585, -0.007654057, 0.0023571495, 0.00546788, -0.017908765, 0.000632789, -0.005782372, 0.0031317968, -0.005089466, -0.0040274803, -0.0021704745, 0.004253659, 0.0018956376, 0.0036830157, -0.02853278, -0.0002765528, -0.0049277963, 0.0035951359, 0.0066317637, 0.0037983418, -0.0054717995, -0.02034008, -0.012005936, 0.021869758, -0.0028838224, -0.010938177, 0.008735726, 0.008432174, -0.004471727, 0.040008016, -0.0006240557, -0.03063046, 0.0005587447, -0.018062929, 0.009991752, -0.021793006, 0.003646401, 0.008914355, 0.011429018, 0.0061382153, 0.0027384954, -0.01117847, -0.006541804, 0.00021816618, 0.00196666, 0.0066627585, 0.017481808, -0.0026022154, 0.014775051, -0.012185711, 0.014160498, 0.0030938426, -0.03741814, -0.000990541, 0.0033829114, -0.024996156, 0.02419568, 0.006341144, -0.005351213, 0.01120672, 0.005890113, 0.018668478, 0.0014850111, 0.0071960865, -0.0026894165, 0.0034922482, 0.001324479, -0.009018682, 0.0066755805, 0.009998607, -0.00010688466, -0.012827809, -0.0090203965, -0.011183187, 0.0036803575, -0.005665097, 0.0056986664, 0.0040972345, 0.015984608, -0.003982118, 0.007873137, -0.0032244474, 0.018744333, 0.0020464344, 0.025817294, -0.006837641, 0.006828823, 0.018291777, -0.028880674, -0.0019245833, 0.0047815586, 0.0032145055, -0.021540867, 0.0043039764, 0.009741668, 0.0045351107, -0.017260918, -0.010635865, 0.0038336553, 0.020722162, -0.0033284086, 0.011485256, -0.027588263, -0.0068537053, -0.0172686, 0.022253633, -0.005794118, 5.207994e-05, 0.007427728, -0.011864832, 0.017882578, 0.015188303, 0.008239597, 0.013625498, 0.028849069, 0.021880774, -0.016963618, -0.01379296, -0.019168133, -0.0148036415, 0.0020518256, 0.011632975, -0.013880872, -0.0102695655, 0.011297573, 0.0008744419, 0.019193418, -0.004850521, -0.011060961, -0.0041066613, -0.014597714, 0.01720353, -0.018270534, -0.00538723, -0.014631085, -0.00868745, -0.0015940578, -0.0073811994, 0.02036911, 0.008461352, 0.021474492, -0.012133308, 3.6343303e-07, -0.014239955, 0.0026550225, -0.009923726, -0.00012988178, 0.007653609, -0.0009644775, 0.0070986496, -0.0012988639, -0.00858704, -0.013349275, 0.009730248, -0.0007093577, 0.013522582, 0.014386745, -0.031033942, 0.0072965147, -0.011568983, -0.012821867, 0.0054050377, -0.020282578, 0.023218472, -0.012125079, -0.023567025, 0.0057281815, 0.009171959, -0.010233959, 0.019974632, -0.010016001, 0.01181656, -0.010040254, 0.005145964, -0.0038471387, 0.0015118953, 0.002771719, 0.001862716, -0.0054877503, -0.01913758, 0.019614771, -0.014751126, 0.0077662915, 0.0003958136, 0.019613627, 0.008248469, -0.008169937, -0.007843764, 0.0039693913, 0.0022121596, 0.0013037893, 0.014487686, -0.020639677, 0.019905288, 0.016987393, 0.0049959836, 0.00025222552, -0.011362518, 0.01811901, -0.005274828, 0.0039449497, 0.022966905, -0.010057288, 0.00048417278, 0.0001595895, 0.0149832405, -0.0069556325, 0.00669782, 0.0194652, 0.004467937, -0.014200896, 0.0078355335, -0.004513734, 0.010609441, 0.0064896937, 0.0063434755, -0.0049911644, -0.009541231, 0.0023570785, 0.0030084318, -0.0060925856, 0.012543069, -0.0077280505, -0.000722175, -0.0045013996, 0.01372461, 0.007861362, -0.011794305, 0.005989468, 0.0012146694, -0.0062840665, 0.0025132166, -0.0046475674, 0.0055932733, -0.004814871, 0.020564074, -0.004699938, -0.007677117, 0.00145618, 0.012607842, -0.0027996805, -5.709753e-05, -0.033680078, -0.004463781, 0.004939701, -0.004872655, 0.050868906, -0.0042829947, 0.009930166, 0.009307888, 0.011478362, 0.00415831, 0.014666101, -0.00036234414, -0.008280569, 0.0060752626, -0.013694725, 0.0009959842, 0.018387193, -0.012284829, 0.01654879, 0.007798064, 0.012197437, 0.029456776, 0.012320361, 0.0125968885, -0.009532671, -0.008105194, -0.0034219848, 0.0026405768, -0.018925902, -0.0028549829, -0.0034417359, 0.0056682997, -0.00927975, -0.01988787, -0.027868835, 0.005750773, -0.006825877, 0.0007483353, 0.015376366, -0.008860101, 0.017295323, -0.0033985546, 0.008771885, -0.0025275918, 0.014125042, 0.026316762, 0.01320124, -0.020105656, -0.0028548308, 0.009649683, -0.0135489255, 0.0012271154, 0.0028035245, 0.008630884, -0.0048446343, 0.001018878, 0.010034186, -0.0053771813, -0.024622822, 0.018967034, -0.015712729, 0.006622122, 0.019267192, 0.01048656, 0.004933078, 0.021836497, -0.022019668, -0.0090233125, -0.0068813777, -0.013764532, -0.0022035388, -0.00019526313, 0.00890864, 0.00047471252, 0.003057148, 0.010445122, -0.007820297, 0.028767126, -0.03515257, 0.0035488517, -0.006200255, 0.009264803, -0.021836692, -0.012474857, -0.006646833, -0.0024475518, -0.052871004, 0.029939007, 0.008981927, 0.023635052, -0.011182039, 0.012374562, -0.012759284, -0.01740047, -0.012956399, 0.011846256, -0.017125089, 0.0046994095, -0.022092976, -0.020194072, -0.014978631, -0.017204937, -0.002877074, 0.010942663, 0.008079551, -0.0080072135, 0.023452258, -0.009931232, -0.009137557, 0.0019091937, 0.013931056, 0.0054224017, 0.013538515, -0.0012538363, -0.005656718, 0.019096497, -0.024505684, -0.007862959, -0.024981974, -0.007647899, -0.007607157, 0.012733463, -0.011410694, -0.0030072294, 0.0077400343, -0.0078097414, -0.0063680625, 0.00025240862, -0.0104669165]], 'text': '\"Judging LLM-as-a-Judge\\\\nwith MT-Bench and Chatbot Arena\\\\nLianmin Zheng1\\\\u2217\\\\nWei-Lin Chiang1\\\\u2217\\\\nYing Sheng4\\\\u2217\\\\nSiyuan Zhuang1\\\\nZhanghao Wu1\\\\nYonghao Zhuang3\\\\nZi Lin2\\\\nZhuohan Li1\\\\nDacheng Li13\\\\nEric P. Xing35\\\\nHao Zhang12\\\\nJoseph E. Gonzalez1\\\\nIon Stoica1\\\\n1 UC Berkeley\\\\n2 UC San Diego\\\\n3 Carnegie Mellon University\\\\n4 Stanford\\\\n5 MBZUAI\\\\nAbstract\\\\nEvaluating large language model (LLM) based chat assistants is challenging due to\\\\ntheir broad capabilities and the inadequacy of existing benchmarks in measuring\\\\nhuman preferences. To address this, we explore using strong LLMs as judges to\\\\nevaluate these models on more open-ended questions. We examine the usage and\\\\nlimitations of LLM-as-a-judge, including position, verbosity, and self-enhancement\\\\nbiases, as well as limited reasoning ability, and propose solutions to mitigate some\\\\nof them', 'metadata': {'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}}]\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(documents[0].keys())\n",
    "(documents[0])[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add some documents to the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[doc[\"text\"] for doc in documents],\n",
    "    metadatas=[{\"source\":json.dumps(doc[\"metadata\"])}for doc in documents ],\n",
    "    ids=[doc[\"id\"] for doc in documents]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use simple query from the collection using the query() method withoutthe use of gradio for simplicity and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\n",
    "        \"What is the core idea behind the LLM-as-a-Judge concept?\"\n",
    "    ],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup persistent Storage**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store embeddings generated in a local storage, we created a local folder DB and linked it to ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "storage_path = os.getenv('STORAGE_PATH')\n",
    "print(storage_path)\n",
    "if storage_path is None:\n",
    "    raise ValueError('STORAGE_PATH environment variable is not set')\n",
    "\n",
    "client = chromadb.PersistentClient(path=storage_path)\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trying Chroma from Langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import chroma\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='\"Provided proper attribution is provided, Google hereby grants permission to\\\\nreproduce the tables and figures in this paper solely for use in journalistic or\\\\nscholarly works.\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.com\\\\nNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.com\\\\nNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.com\\\\nJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.com\\\\nAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\n\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. We show that the Transformer generalizes well to\\\\nother tasks by applying it successfully to English constituency parsing both with\\\\nlarge and limited training data.\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nefficient inference and visualizations'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\\\n1\\\\nIntroduction\\\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [38, 24, 15].\\\\nRecurrent models typically factor computation along the symbol positions of the input and output\\\\nsequences'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. Aligning the positions to steps in computation time, they generate a sequence of hidden\\\\nstates ht, as a function of the previous hidden state ht\\\\u22121 and the input for position t. This inherently\\\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\\\nconstraint of sequential computation, however, remains.\\\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\\\nthe input or output sequences [2, 19]'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. In all but a few cases [27], however, such attention mechanisms\\\\nare used in conjunction with a recurrent network.\\\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2\\\\nBackground\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\\\nblock, computing hidden representations in parallel for all input and output positions'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. In these models,\\\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\\\ndescribed in section 3.2.\\\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22]'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\\\naligned recurrence and have been shown to perform well on simple-language question answering and\\\\nlanguage modeling tasks [34].\\\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\\\nentirely on self-attention to compute representations of its input and output without using sequence-\\\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\\\n3\\\\nModel Architecture\\\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\\\nsequence (y1, ..., ym) of symbols one element at a time'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='..., ym) of symbols one element at a time. At each step the model is auto-regressive\\\\n[10], consuming the previously generated symbols as additional input when generating the next.\\\\n2\\\\nFigure 1: The Transformer - model architecture.\\\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\\\nrespectively.\\\\n3.1\\\\nEncoder and Decoder Stacks\\\\nEncoder:\\\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\\\nitself'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512.\\\\nDecoder:\\\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\\\npredictions for position i can depend only on the known outputs at positions less than i.\\\\n3'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\n3.2\\\\nAttention\\\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\\\n3\\\\nScaled Dot-Product Attention\\\\nMulti-Head Attention\\\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\\\nattention layers running in parallel.\\\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\\\nquery with the corresponding key.\\\\n3.2.1\\\\nScaled Dot-Product Attention\\\\nWe call our particular attention \\\\\"Scaled Dot-Product Attention\\\\\" (Figure 2). The input consists of\\\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\\\nquery with all keys, divide each by \\\\u221adk, and apply a softmax function to obtain the weights on the\\\\nvalues'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V ) = softmax(QKT\\\\n\\\\u221adk\\\\n)V\\\\n(1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof\\\\n1\\\\n\\\\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\\\nmatrix multiplication code.\\\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\\\ndot product attention without scaling for larger values of dk [3]'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. We suspect that for large values of\\\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\\\n1\\\\n\\\\u221adk .\\\\n3.2.2\\\\nMulti-Head Attention\\\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\\\nvariables with mean 0 and variance 1. Then their dot product, q \\\\u00b7 k = Pdk\\\\ni=1 qiki, has mean 0 and variance dk.\\\\n4\\\\noutput values'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. Then their dot product, q \\\\u00b7 k = Pdk\\\\ni=1 qiki, has mean 0 and variance dk.\\\\n4\\\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\\\ndepicted in Figure 2.\\\\nMulti-head attention allows the model to jointly attend to information from different representation\\\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\\\nwhere headi = Attention(QW Q\\\\ni , KW K\\\\ni , V W V\\\\ni )\\\\nWhere the projections are parameter matrices W Q\\\\ni\\\\n\\\\u2208Rdmodel\\\\u00d7dk, W K\\\\ni\\\\n\\\\u2208Rdmodel\\\\u00d7dk, W V\\\\ni\\\\n\\\\u2208Rdmodel\\\\u00d7dv\\\\nand W O \\\\u2208Rhdv\\\\u00d7dmodel.\\\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\\\nis similar to that of single-head attention with full dimensionality.\\\\n3.2'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\n3.2.3\\\\nApplications of Attention in our Model\\\\nThe Transformer uses multi-head attention in three different ways:\\\\n\\\\u2022 In \\\\\"encoder-decoder attention\\\\\" layers, the queries come from the previous decoder layer,\\\\nand the memory keys and values come from the output of the encoder. This allows every\\\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\\\n[38, 2, 9].\\\\n\\\\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\\\nand queries come from the same place, in this case, the output of the previous layer in the\\\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\\\nencoder.\\\\n\\\\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\\\nall positions in the decoder up to and including that position'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. We need to prevent leftward\\\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\\\ninside of scaled dot-product attention by masking out (setting to \\\\u2212\\\\u221e) all values in the input\\\\nof the softmax which correspond to illegal connections. See Figure 2.\\\\n3.3\\\\nPosition-wise Feed-Forward Networks\\\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\\\nconnected feed-forward network, which is applied to each position separately and identically. This\\\\nconsists of two linear transformations with a ReLU activation in between.\\\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\\\n(2)\\\\nWhile the linear transformations are the same across different positions, they use different parameters\\\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\\\ndff = 2048.\\\\n3'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\n3.4\\\\nEmbeddings and Softmax\\\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \\\\u221admodel.\\\\n5\\\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\\\nsize of convolutions and r the size of the neighborhood in restricted self-attention'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\nLayer Type\\\\nComplexity per Layer\\\\nSequential\\\\nMaximum Path Length\\\\nOperations\\\\nSelf-Attention\\\\nO(n2 \\\\u00b7 d)\\\\nO(1)\\\\nO(1)\\\\nRecurrent\\\\nO(n \\\\u00b7 d2)\\\\nO(n)\\\\nO(n)\\\\nConvolutional\\\\nO(k \\\\u00b7 n \\\\u00b7 d2)\\\\nO(1)\\\\nO(logk(n))\\\\nSelf-Attention (restricted)\\\\nO(r \\\\u00b7 n \\\\u00b7 d)\\\\nO(1)\\\\nO(n/r)\\\\n3.5\\\\nPositional Encoding\\\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\\\norder of the sequence, we must inject some information about the relative or absolute position of the\\\\ntokens in the sequence. To this end, we add \\\\\"positional encodings\\\\\" to the input embeddings at the\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and fixed [9]'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. There are many choices of positional encodings,\\\\nlearned and fixed [9].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0 to 10000 \\\\u00b7 2\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\n4\\\\nWhy Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\n6\\\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[38] and byte-pair [31] representations'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\\\nthe input sequence centered around the respective output position. This would increase the maximum\\\\npath length to O(n/r). We plan to investigate this approach further in future work.\\\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2)'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. Even with k = n, however, the complexity of a separable\\\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\\\nthe approach we take in our model.\\\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\\\nand semantic structure of the sentences.\\\\n5\\\\nTraining\\\\nThis section describes the training regime for our models.\\\\n5.1\\\\nTraining Data and Batching\\\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\\\ntarget vocabulary of about 37000 tokens'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='. For English-French, we used the significantly larger WMT\\\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\\\ntarget tokens.\\\\n5.2\\\\nHardware and Schedule\\\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\\\n(3.5 days).\\\\n5.3\\\\nOptimizer\\\\nWe used the Adam optimizer [20] with \\\\u03b21 = 0.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.9, \\\\u03b22 = 0.98 and \\\\u03f5 = 10\\\\u22129. We varied the learning\\\\nrate over the course of training, according to the formula:\\\\nlrate = d\\\\u22120.5\\\\nmodel \\\\u00b7 min(step_num\\\\u22120.5, step_num \\\\u00b7 warmup_steps\\\\u22121.5)\\\\n(3)\\\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\\\nwarmup_steps = 4000.\\\\n5.4\\\\nRegularization\\\\nWe employ three types of regularization during training:\\\\n7\\\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\\\nModel\\\\nBLEU\\\\nTraining Cost (FLOPs)\\\\nEN-DE\\\\nEN-FR\\\\nEN-DE\\\\nEN-FR\\\\nByteNet [18]\\\\n23.75\\\\nDeep-Att + PosUnk [39]\\\\n39.2\\\\n1.0 \\\\u00b7 1020\\\\nGNMT + RL [38]\\\\n24.6\\\\n39.92\\\\n2.3 \\\\u00b7 1019\\\\n1.4 \\\\u00b7 1020\\\\nConvS2S [9]\\\\n25.16\\\\n40.46\\\\n9.6 \\\\u00b7 1018\\\\n1.5 \\\\u00b7 1020\\\\nMoE [32]\\\\n26'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.4 \\\\u00b7 1020\\\\nConvS2S [9]\\\\n25.16\\\\n40.46\\\\n9.6 \\\\u00b7 1018\\\\n1.5 \\\\u00b7 1020\\\\nMoE [32]\\\\n26.03\\\\n40.56\\\\n2.0 \\\\u00b7 1019\\\\n1.2 \\\\u00b7 1020\\\\nDeep-Att + PosUnk Ensemble [39]\\\\n40.4\\\\n8.0 \\\\u00b7 1020\\\\nGNMT + RL Ensemble [38]\\\\n26.30\\\\n41.16\\\\n1.8 \\\\u00b7 1020\\\\n1.1 \\\\u00b7 1021\\\\nConvS2S Ensemble [9]\\\\n26.36\\\\n41.29\\\\n7.7 \\\\u00b7 1019\\\\n1.2 \\\\u00b7 1021\\\\nTransformer (base model)\\\\n27.3\\\\n38.1\\\\n3.3 \\\\u00b7 1018\\\\nTransformer (big)\\\\n28.4\\\\n41.8\\\\n2.3 \\\\u00b7 1019\\\\nResidual Dropout\\\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\\\nPdrop = 0.1.\\\\nLabel Smoothing\\\\nDuring training, we employed label smoothing of value \\\\u03f5ls = 0.1 [36]. This\\\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\\\n6\\\\nResults\\\\n6'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\n6\\\\nResults\\\\n6.1\\\\nMachine Translation\\\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\\\nthe competitive models.\\\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\\\ndropout rate Pdrop = 0.1, instead of 0.3'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\\\nused beam search with a beam size of 4 and length penalty \\\\u03b1 = 0.6 [38]. These hyperparameters\\\\nwere chosen after experimentation on the development set. We set the maximum output length during\\\\ninference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\n6.2\\\\nModel Variations\\\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\\\nin different ways, measuring the change in performance on English-to-German translation on the\\\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\\\n8\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN\\\\ndmodel\\\\ndff\\\\nh\\\\ndk\\\\ndv\\\\nPdrop\\\\n\\\\u03f5ls\\\\ntrain\\\\nPPL\\\\nBLEU\\\\nparams\\\\nsteps\\\\n(dev)\\\\n(dev)\\\\n\\\\u00d7106\\\\nbase\\\\n6\\\\n512\\\\n2048\\\\n8\\\\n64\\\\n64\\\\n0.1\\\\n0.1\\\\n100K\\\\n4.92\\\\n25.8\\\\n65\\\\n(A)\\\\n1\\\\n512\\\\n512\\\\n5.29\\\\n24.9\\\\n4\\\\n128\\\\n128\\\\n5.00\\\\n25.5\\\\n16\\\\n32\\\\n32\\\\n4.91\\\\n25.8\\\\n32\\\\n16\\\\n16\\\\n5.01\\\\n25.4\\\\n(B)\\\\n16\\\\n5.16\\\\n25.1\\\\n58\\\\n32\\\\n5.01\\\\n25.4\\\\n60\\\\n(C)\\\\n2\\\\n6.11\\\\n23.7\\\\n36\\\\n4\\\\n5.19\\\\n25'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.01\\\\n25.4\\\\n(B)\\\\n16\\\\n5.16\\\\n25.1\\\\n58\\\\n32\\\\n5.01\\\\n25.4\\\\n60\\\\n(C)\\\\n2\\\\n6.11\\\\n23.7\\\\n36\\\\n4\\\\n5.19\\\\n25.3\\\\n50\\\\n8\\\\n4.88\\\\n25.5\\\\n80\\\\n256\\\\n32\\\\n32\\\\n5.75\\\\n24.5\\\\n28\\\\n1024\\\\n128\\\\n128\\\\n4.66\\\\n26.0\\\\n168\\\\n1024\\\\n5.12\\\\n25.4\\\\n53\\\\n4096\\\\n4.75\\\\n26.2\\\\n90\\\\n(D)\\\\n0.0\\\\n5.77\\\\n24.6\\\\n0.2\\\\n4.95\\\\n25.5\\\\n0.0\\\\n4.67\\\\n25.3\\\\n0.2\\\\n5.47\\\\n25.7\\\\n(E)\\\\npositional embedding instead of sinusoids\\\\n4.92\\\\n25.7\\\\nbig\\\\n6\\\\n1024\\\\n4096\\\\n16\\\\n0.3\\\\n300K\\\\n4.33\\\\n26.4\\\\n213\\\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\\\ncheckpoint averaging. We present these results in Table 3.\\\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\\\nresults to the base model.\\\\n6.3\\\\nEnglish Constituency Parsing\\\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37]'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\\\nfor the semi-supervised setting.\\\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\\\nremained unchanged from the English-to-German base translation model. During inference, we\\\\n9\\\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\\\nof WSJ)\\\\nParser\\\\nTraining\\\\nWSJ 23 F1\\\\nVinyals & Kaiser el al. (2014) [37]\\\\nWSJ only, discriminative\\\\n88.3\\\\nPetrov et al. (2006) [29]\\\\nWSJ only, discriminative\\\\n90'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.3\\\\nPetrov et al. (2006) [29]\\\\nWSJ only, discriminative\\\\n90.4\\\\nZhu et al. (2013) [40]\\\\nWSJ only, discriminative\\\\n90.4\\\\nDyer et al. (2016) [8]\\\\nWSJ only, discriminative\\\\n91.7\\\\nTransformer (4 layers)\\\\nWSJ only, discriminative\\\\n91.3\\\\nZhu et al. (2013) [40]\\\\nsemi-supervised\\\\n91.3\\\\nHuang & Harper (2009) [14]\\\\nsemi-supervised\\\\n91.3\\\\nMcClosky et al. (2006) [26]\\\\nsemi-supervised\\\\n92.1\\\\nVinyals & Kaiser el al. (2014) [37]\\\\nsemi-supervised\\\\n92.1\\\\nTransformer (4 layers)\\\\nsemi-supervised\\\\n92.7\\\\nLuong et al. (2015) [23]\\\\nmulti-task\\\\n93.0\\\\nDyer et al. (2016) [8]\\\\ngenerative\\\\n93.3\\\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \\\\u03b1 = 0.3\\\\nfor both WSJ only and the semi-supervised setting.\\\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\\\nprisingly well, yielding better results than all previously reported models with the exception of the\\\\nRecurrent Neural Network Grammar [8]'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\\\n7\\\\nConclusion\\\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks'), Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor.\\\\nAcknowledgements\\\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n')], [Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='\"Judging LLM-as-a-Judge\\\\nwith MT-Bench and Chatbot Arena\\\\nLianmin Zheng1\\\\u2217\\\\nWei-Lin Chiang1\\\\u2217\\\\nYing Sheng4\\\\u2217\\\\nSiyuan Zhuang1\\\\nZhanghao Wu1\\\\nYonghao Zhuang3\\\\nZi Lin2\\\\nZhuohan Li1\\\\nDacheng Li13\\\\nEric P. Xing35\\\\nHao Zhang12\\\\nJoseph E. Gonzalez1\\\\nIon Stoica1\\\\n1 UC Berkeley\\\\n2 UC San Diego\\\\n3 Carnegie Mellon University\\\\n4 Stanford\\\\n5 MBZUAI\\\\nAbstract\\\\nEvaluating large language model (LLM) based chat assistants is challenging due to\\\\ntheir broad capabilities and the inadequacy of existing benchmarks in measuring\\\\nhuman preferences. To address this, we explore using strong LLMs as judges to\\\\nevaluate these models on more open-ended questions. We examine the usage and\\\\nlimitations of LLM-as-a-judge, including position, verbosity, and self-enhancement\\\\nbiases, as well as limited reasoning ability, and propose solutions to mitigate some\\\\nof them'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. We then verify the agreement between LLM judges and human preferences\\\\nby introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\\\\nArena, a crowdsourced battle platform. Our results reveal that strong LLM judges\\\\nlike GPT-4 can match both controlled and crowdsourced human preferences well,\\\\nachieving over 80% agreement, the same level of agreement between humans.\\\\nHence, LLM-as-a-judge is a scalable and explainable way to approximate human\\\\npreferences, which are otherwise very expensive to obtain. Additionally, we show\\\\nour benchmark and traditional benchmarks complement each other by evaluating\\\\nseveral variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes,\\\\nand 30K conversations with human preferences are publicly available at https:\\\\n//github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\\\\n1\\\\nIntroduction\\\\nThere has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised\\\\ninstruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new\\\\ninstruction following and conversational abilities [31, 2, 30, 8, 52, 48, 14]. Once aligned with\\\\nhumans, these chat models are strongly preferred by human users over the original, unaligned models\\\\non which they are built. However, the heightened user preference does not always correspond to\\\\nimproved scores on traditional LLM benchmarks \\\\u2013 benchmarks like MMLU [19] and HELM [24]\\\\ncannot effectively tell the difference between these aligned models and the base models. This\\\\nphenomenon suggests that there is a fundamental discrepancy between user perceptions of the\\\\nusefulness of chatbots and the criteria adopted by conventional benchmarks'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\nWe argue that this discrepancy primarily arises due to existing evaluation that only measures LLMs\\\\u2019\\\\ncore capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions),\\\\nwithout adequately assessing its alignment with human preference in open-ended tasks, such as the\\\\nability to accurately adhere to instructions in multi-turn dialogues. As a demonstration, we show\\\\n\\\\u2217Joint first authors. This paper is an extended version of our earlier blog post [8].\\\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\\\\narXiv:2306.05685v4  [cs.CL]  24 Dec 2023\\\\nQuestion: If the FED buys bonds in the secondary market\\\\n(A) the money supply will increase. (B) the money supply will decrease. (C) the money supply will not be \\\\naffected. (D) the discount rate would be affected. Output the answer only.\\\\nAssistant B:\\\\n(A) the money supply will increase'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content=\". Output the answer only.\\\\nAssistant B:\\\\n(A) the money supply will increase.\\\\nAssistant A:\\\\nThe Federal Reserve buys \\\\nbonds in the secondary \\\\nmarket to increase the \\\\nmoney supply.\\\\nFollow-up Question: How does it affect my daily life? Give 3 examples.\\\\nAssistant B:\\\\nWhen the Federal Reserve buys bonds in the secondary market, it can have \\\\nseveral effects on the economy that can impact your daily life. Here are \\\\nthree examples:\\\\n1. Interest rates: The Federal Reserve's actions in the bond market can \\\\ninfluence interest rates, which in turn can affect the cost of borrowing \\\\nmoney. For example, if the Federal Reserve buys bonds and drives up demand \\\\nfor them, interest rates on things like mortgages, car loans, and credit \\\\ncards may go up, making it more expensive to borrow money.\\\\n2. Inflation: When the money supply increases, it can lead to inflation, \\\\nwhich is the general rise in prices of goods and services\"), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content=\". This can impact \\\\nyour daily life by making it more expensive to buy things like groceries, \\\\ngas, and other necessities.\\\\n3. Employment: The Federal Reserve's actions in the bond market can also \\\\nhave an impact on employment. For example, if the Federal Reserve's \\\\nactions lead to economic growth and increased demand for goods and \\\\nservices, it can create more job opportunities and boost employment.\\\\nAssistant A:\\\\nThe Federal Reserve buys \\\\nbonds in the secondary \\\\nmarket to increase the \\\\nmoney supply. This \\\\naffects your daily life \\\\nby:\\\\n1. Increasing the money \\\\nsupply.\\\\n2. Increasing the money \\\\nsupply and lowering the \\\\ndiscount rate.\\\\n3. Increasing the money \\\\nsupply and lowering the \\\\ndiscount rate and \\\\nincreasing the money \\\\nsupply.\\\\nGPT-4 Judgment:\\\\nAssistant A provided an incorrect response to the user's question about how the Federal Reserve \\\\nbuying bonds in the secondary market affects daily life\"), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content=\". The answer given is repetitive and lacks \\\\nclear examples of how the action impacts daily life.\\\\nOn the other hand, Assistant B provided a relevant and accurate response to the user's question about \\\\nthe Federal Reserve buying bonds. The answer includes three clear examples of how the action impacts \\\\ndaily life, such as interest rates, inflation, and employment.\\\\nAssistant A should improve by providing relevant, clear, and accurate examples to address the user's \\\\nquestion. Assistant B's response is more helpful, relevant, and detailed.\\\\nFigure 1: Multi-turn dialogues between a user and two AI assistants\\\\u2014LLaMA-13B (Assistant A)\\\\nand Vicuna-13B (Assistant B)\\\\u2014initiated by a question from the MMLU benchmark and a follow-up\\\\ninstruction. GPT-4 is then presented with the context to determine which assistant answers better.\\\\nconversation histories with two models on an MMLU question in Figure 1\"), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\nconversation histories with two models on an MMLU question in Figure 1. The two models are\\\\nLLaMA-13B [39], a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned\\\\nmodel from LLaMA-13B on high-quality conversations (the training details are in Appendix E).\\\\nDespite the base LLaMA models showing competitive performance on conventional benchmarks\\\\n(Table 8), its answers to open-ended questions are often not preferred by humans. This misalignment\\\\nof conventional benchmarks underscores the core problem driving this paper: the need for a robust\\\\nand scalable automated method to evaluate LLM alignment with human preferences.\\\\nTo study this, we introduce two benchmarks with human ratings as the primary evaluation metric:\\\\nMT-bench and Chatbot Arena. MT-bench is a series of open-ended questions that evaluate a chatbot\\\\u2019s\\\\nmulti-turn conversational and instruction-following ability \\\\u2013 two critical elements for human prefer-\\\\nence'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities,\\\\nsuch as reasoning and math. In addition, we develop Chatbot Arena, a crowdsourced platform fea-\\\\nturing anonymous battles between chatbots in real-world scenarios \\\\u2013 Users engage in conversations\\\\nwith two chatbots at the same time and rate their responses based on personal preferences.\\\\nWhile human evaluation is the gold standard for assessing human preferences, it is exceptionally slow\\\\nand costly. To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4,\\\\nas a surrogate for humans. Because these models are often trained with RLHF, they already exhibit\\\\nstrong human alignment. We call this approach \\\\u201cLLM-as-a-judge\\\\u201d. This approach has been tried in\\\\nour earlier blog post [8] and other concurrent or follow-up work [5, 29, 14, 12, 52, 18, 33, 40, 7, 43].\\\\nHowever, there has not been a systematic study of this approach'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\nHowever, there has not been a systematic study of this approach.\\\\nIn this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of\\\\nhuman evaluation. We examine several potential limitations of the LLM-as-a-judge approach\\\\nincluding position bias, verbosity bias, self-enhancement bias, and limited reasoning ability. We\\\\nshow that some of the biases are minor or can be mitigated. Once addressed, our results from 3K\\\\ncontrolled expert votes and 3K crowdsourced human votes in the wild verify that GPT-4 judge match\\\\n2\\\\nhuman evaluations at an agreement rate exceeding 80%, achieving the same level of human-human\\\\nagreement (\\\\u00a74.2, Table 4). Consequently, this suggests LLM-as-a-judge is a scalable method to\\\\nswiftly evaluate human preference, serving as a promising alternative to traditional human evaluations'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\nThis paper makes two contributions: (1) a systematic study of LLM-as-a-judge; and (2) human\\\\npreference datasets with high-quality questions and diverse user interactions from MT-bench and\\\\nChatbot Arena. In addition, we argue for the adoption of a hybrid evaluation framework for future\\\\nLLM benchmarks: by combining the existing capability-based benchmarks and the new preference-\\\\nbased benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core\\\\ncapabilities and human alignment of models. We publicly release 80 MT-bench questions, 3K expert\\\\nvotes, and 30K conversations with human preferences for future study.\\\\nTable 1: Sample multi-turn questions in MT-bench.\\\\nCategory\\\\nSample Questions\\\\nWriting\\\\n1st Turn\\\\nCompose an engaging travel blog post about a recent trip to Hawaii, highlighting\\\\ncultural experiences and must-see attractions.\\\\n2nd Turn\\\\nRewrite your previous response. Start every sentence with the letter A'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\n2nd Turn\\\\nRewrite your previous response. Start every sentence with the letter A.\\\\nMath\\\\n1st Turn\\\\nGiven that f(x) = 4x3 \\\\u22129x \\\\u221214, find the value of f(2).\\\\n2nd Turn\\\\nFind x such that f(x) = 0.\\\\nKnowledge\\\\n1st Turn\\\\nProvide insights into the correlation between economic indicators such as GDP,\\\\ninflation, and unemployment rates. Explain how fiscal and monetary policies ...\\\\n2nd Turn\\\\nNow, explain them again like I\\\\u2019m five.\\\\n2\\\\nMT-Bench and Chatbot Arena\\\\n2.1\\\\nMotivation\\\\nWith the recent advances of LLMs, LLM-based assistants start to exhibit artificial general intelligence\\\\nacross diverse tasks, from writing and chatting to coding [5, 30, 1, 37]. However, evaluating their\\\\nbroad capabilities also becomes more challenging. Despite the availability of numerous benchmarks\\\\nfor language models, they primarily focus on evaluating models on closed-ended questions with short\\\\nresponses'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. Given that these chat assistants can now precisely follow user instructions in multi-turn\\\\ndialogues and answer open-ended questions in a zero-shot manner, current benchmarks are inadequate\\\\nfor assessing such capabilities. Existing benchmarks mostly fall into the following three categories.\\\\n\\\\u2022 Core-knowledge benchmarks, including MMLU [19], HellaSwag [50], ARC [9], Wino-\\\\nGrande [36], HumanEval [6], GSM-8K [10], and AGIEval [51], evaluate the core capabilities of\\\\npre-trained LLMs using zero-shot and few-shot benchmark sets. They typically require LLMs to\\\\ngenerate a short, specific answer to benchmark questions that can be automatically validated.\\\\n\\\\u2022 Instruction-following benchmarks, such as Flan [27, 46], Self-instruct [44], NaturalInstruc-\\\\ntions [28], Super-NaturalInstructions [45], expand to slightly more open-ended questions and\\\\nmore diverse tasks and are used to evaluate LLMs after instruction fine-tuning'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\n\\\\u2022 Conversational benchmarks, like CoQA [35], MMDialog [15] and OpenAssistant [23], are\\\\nclosest to our intended use cases. However, the diversity and complexity of their questions often\\\\nfall short in challenging the capabilities of the latest chatbots.\\\\nWhile largely overlooked by existing LLM benchmarks, human preferences serve as a direct measure\\\\nof a chatbot\\\\u2019s utility in open-ended, multi-turn human-AI interactions. To bridge this gap, we\\\\nintroduce two novel benchmarks expressly tailored to assess human preferences. Simultaneously,\\\\nthese benchmarks are designed to distinguish the core capabilities of state-of-the-art models.\\\\n2.2\\\\nMT-Bench\\\\nWe create MT-bench, a benchmark consisting of 80 high-quality multi-turn questions. MT-bench is\\\\ndesigned to test multi-turn conversation and instruction-following ability, covering common use cases\\\\nand focusing on challenging questions to differentiate models'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. We identify 8 common categories\\\\nof user prompts to guide its construction: writing, roleplay, extraction, reasoning, math, coding,\\\\n3\\\\nknowledge I (STEM), and knowledge II (humanities/social science). For each category, we then\\\\nmanually designed 10 multi-turn questions. Table 1 lists several sample questions.\\\\n2.3\\\\nChatbot Arena\\\\nOur second approach is Chatbot Arena, a crowdsourcing benchmark platform featuring anonymous\\\\nbattles. On this platform, users can interact with two anonymous models simultaneously, posing\\\\nthe same question to both. They vote for which model provides the preferred response, with the\\\\nidentities of the models disclosed post-voting. After running Chatbot Arena for one month, we have\\\\ncollected around 30K votes. Since the platform does not use pre-defined questions, it allows gathering\\\\na wide range of unrestricted use cases and votes in the wild, based on the diverse interests of users. A\\\\nscreenshot of the platform can be found at Appendix C.2'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. A\\\\nscreenshot of the platform can be found at Appendix C.2.\\\\n3\\\\nLLM as a Judge\\\\nWhile our initial evaluations using MT-bench and Chatbot Arena rely on human ratings, collecting\\\\nhuman preferences can be costly and laborious [44, 38, 31, 2, 13]. To overcome this, we aim to\\\\ndevelop a more scalable and automated approach. Given that most questions in MT-bench and Chatbot\\\\nArena are open-ended without reference answers, devising a rule-based program to assess the outputs\\\\nis extremely challenging. Traditional evaluation metrics based on the similarity between outputs and\\\\nreference answers (e.g., ROUGE [25], BLEU [32]) are also ineffective for these questions.\\\\nAs LLMs continue to improve, they show potential in replacing human annotators in many tasks [17,\\\\n20]. Specifically, we are interested in whether LLMs can effectively evaluate the responses of chat\\\\nassistants and match human preferences. Next, we discuss the use and limitations of LLM-as-a-judge.\\\\n3'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. Next, we discuss the use and limitations of LLM-as-a-judge.\\\\n3.1\\\\nTypes of LLM-as-a-Judge\\\\nWe propose 3 LLM-as-a-judge variations. They can be implemented independently or in combination:\\\\n\\\\u2022 Pairwise comparison. An LLM judge is presented with a question and two answers, and tasked\\\\nto determine which one is better or declare a tie. The prompt used is given in Figure 5 (Appendix).\\\\n\\\\u2022 Single answer grading. Alternatively, an LLM judge is asked to directly assign a score to a\\\\nsingle answer. The prompt used for this scenario is in Figure 6 (Appendix).\\\\n\\\\u2022 Reference-guided grading. In certain cases, it may be beneficial to provide a reference solution\\\\nif applicable. An example prompt we use for grading math problems is in Figure 8 (Appendix).\\\\nThese methods have different pros and cons'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\nThese methods have different pros and cons. For example, the pairwise comparison may lack\\\\nscalability when the number of players increases, given that the number of possible pairs grows\\\\nquadratically; single answer grading may be unable to discern subtle differences between specific\\\\npairs, and its results may become unstable, as absolute scores are likely to fluctuate more than relative\\\\npairwise results if the judge model changes.\\\\n3.2\\\\nAdvantages of LLM-as-a-Judge\\\\nLLM-as-a-judge offers two key benefits: scalability and explainability. It reduces the need for human\\\\ninvolvement, enabling scalable benchmarks and fast iterations. Additionally, LLM judges provide\\\\nnot only scores but also explanations, making their outputs interpretable, as shown in Figure 1.\\\\n3.3\\\\nLimitations of LLM-as-a-Judge\\\\nWe identify certain biases and limitations of LLM judges'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\n3.3\\\\nLimitations of LLM-as-a-Judge\\\\nWe identify certain biases and limitations of LLM judges. However, we will also present solutions\\\\nlater and show the agreement between LLM judges and humans is high despite these limitations.\\\\nPosition bias is when an LLM exhibits a propensity to favor certain positions over others. This\\\\nbias is not unique to our context and has been seen in human decision-making [3, 34] and other ML\\\\ndomains [22, 41].\\\\nFigure 11 (Appendix) shows an example of position bias. GPT-4 is tasked to evaluate two responses\\\\nfrom GPT-3.5 and Vicuna-13B to an open-ended question. When GPT-3.5\\\\u2019s answer is positioned\\\\n4\\\\nTable 2: Position bias of different LLM judges. Consistency is the percentage of cases where a\\\\njudge gives consistent results when swapping the order of two assistants. \\\\u201cBiased toward first\\\\u201d is the\\\\npercentage of cases when a judge favors the first answer. \\\\u201cError\\\\u201d indicates wrong output formats'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. \\\\u201cError\\\\u201d indicates wrong output formats.\\\\nThe two largest numbers in each column are in bold.\\\\nJudge\\\\nPrompt\\\\nConsistency\\\\nBiased toward first\\\\nBiased toward second\\\\nError\\\\nClaude-v1\\\\ndefault\\\\n23.8%\\\\n75.0%\\\\n0.0%\\\\n1.2%\\\\nrename\\\\n56.2%\\\\n11.2%\\\\n28.7%\\\\n3.8%\\\\nGPT-3.5\\\\ndefault\\\\n46.2%\\\\n50.0%\\\\n1.2%\\\\n2.5%\\\\nrename\\\\n51.2%\\\\n38.8%\\\\n6.2%\\\\n3.8%\\\\nGPT-4\\\\ndefault\\\\n65.0%\\\\n30.0%\\\\n5.0%\\\\n0.0%\\\\nrename\\\\n66.2%\\\\n28.7%\\\\n5.0%\\\\n0.0%\\\\nTable 3: Failure rate under \\\\u201crepetitive list\\\\u201d at-\\\\ntack for different LLM judges on 23 answers.\\\\nJudge\\\\nClaude-v1\\\\nGPT-3.5\\\\nGPT-4\\\\nFailure rate\\\\n91.3%\\\\n91.3%\\\\n8.7%\\\\nTable 4: Judge failure rate on 10 math questions\\\\nwith different prompts. We test LLaMA-13B vs.\\\\nVicuna-13B and swap positions. A failure means\\\\nwhen GPT-4 says an incorrect answer is correct.\\\\nDefault\\\\nCoT\\\\nReference\\\\nFailure rate\\\\n14/20\\\\n6/20\\\\n3/20\\\\nfirst, GPT-4 considers GPT-3.5\\\\u2019s answer more detailed and superior'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.5\\\\u2019s answer more detailed and superior. However, upon switching the\\\\npositions of the two responses, GPT-4\\\\u2019s judgement flips, favoring Vicuna\\\\u2019s answer.\\\\nTo analyze the position bias, we construct two similar answers to each first-turn question in MT-bench\\\\nby calling GPT-3.5 twice with a temperature of 0.7. We then try three LLMs with two different\\\\nprompts: \\\\u201cdefault\\\\u201d is our default prompt in Figure 5 (Appendix). \\\\u201crename\\\\u201d renames the assistants in\\\\nour default prompt to see whether the bias is on positions or names. As in Table 2, we found all of\\\\nthem exhibit strong position bias. Most LLM judges favor the first position. Claude-v1 also shows a\\\\nname bias which makes it favors \\\\\"Assistant A\\\\\", as illustrated by the \\\\\"rename\\\\\" prompt. The position\\\\nbias can be very significant. Only GPT-4 outputs consistent results in more than 60% of cases'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. Only GPT-4 outputs consistent results in more than 60% of cases.\\\\nNote that this test is challenging because the answers are very similar and occasionally indistinguish-\\\\nable even to humans. We will show that position bias is less prominent in some cases in Appendix D.1.\\\\nAs for the origin of this bias, we suspect that it could be rooted in the training data or inherent to the\\\\nleft-to-right architecture of causal transformers, but leave a deeper study as future work.\\\\nVerbosity bias is when an LLM judge favors longer, verbose responses, even if they are not as clear,\\\\nhigh-quality, or accurate as shorter alternatives.\\\\nTo examine this bias, we design a \\\\u201crepetitive list\\\\u201d attack with model answers from MT-bench. We\\\\nfirst select 23 model answers from MT-bench that contain a numbered list. We then make them\\\\nunnecessarily verbose by asking GPT-4 to rephrase the list without adding any new information and\\\\ninsert the rephrased new list to the beginning of the original list'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. For example, if the original response\\\\ncontains 5 items, then the new response will contain 10 items but the first 5 items are rephrased from\\\\nthe original 5 items. An example is shown in Figure 12 (Appendix). We define the attack is successful\\\\nif an LLM judge thinks the new response is better than the old response. Table 3 shows the failure\\\\nrate of LLM judges under this attack, demonstrating that all LLMs may be prone to verbosity bias\\\\nthough GPT-4 defends significantly better than others. As a calibration, we find LLM judges are\\\\nable to correctly judge identical answers (i.e., they always return a tie for two identical answers) but\\\\ncannot pass the more advanced \\\\u201crepetitive list\\\\u201d attack.\\\\nSelf-enhancement bias. We adopt the term \\\\u201cself-enhancement bias\\\\u201d from social cognition litera-\\\\nture [4] to describe the effect that LLM judges may favor the answers generated by themselves.\\\\nWe examine this effect statistically'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\nWe examine this effect statistically. Figure 3(b) shows the win rate (w/o tie) of six models under\\\\ndifferent LLM judges and humans. Compared to humans, we do observe that some judges favor\\\\ncertain models. For example, GPT-4 favors itself with a 10% higher win rate; Claude-v1 favors itself\\\\nwith a 25% higher win rate. However, they also favor other models and GPT-3.5 does not favor itself.\\\\nDue to limited data and small differences, our study cannot determine whether the models exhibit\\\\na self-enhancement bias. Conducting a controlled study is challenging because we cannot easily\\\\nrephrase a response to fit the style of another model without changing the quality.\\\\n5\\\\nLimited capability in grading math and reasoning questions. LLMs are known to have limited\\\\nmath and reasoning capability [10], which results in its failure of grading such questions because they\\\\ndo not know the correct answers'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. However, what is more intriguing is that it also shows limitations in\\\\ngrading basic math problems which it is capable of solving. For instance, in Figure 13 (Appendix),\\\\nwe present an example of an elementary math question in which GPT-4 makes an incorrect judgment.\\\\nIt\\\\u2019s worth noting that although GPT-4 can solve the problem (when asked separately), it was misled\\\\nby the provided answers, ultimately resulting in incorrect judgment. This pattern can also be seen in\\\\na reasoning question example in Figure 14 (Appendix). Both GPT-3.5 and Claude-v1 show a similar\\\\nweakness. In Section 3.4, we will introduce a reference-guided method to mitigate such issues.\\\\n3.4\\\\nAddressing limitations\\\\nWe present a few methods to address position bias and the limited grading ability for math questions.\\\\nSwapping positions. The position bias can be addressed by simple solutions'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\nSwapping positions. The position bias can be addressed by simple solutions. A conservative\\\\napproach is to call a judge twice by swapping the order of two answers and only declare a win when\\\\nan answer is preferred in both orders. If the results are inconsistent after swapping, we can call it a\\\\ntie. Another more aggressive approach is to assign positions randomly, which can be effective at a\\\\nlarge scale with the correct expectations. In the following experiments, we use the conservative one.\\\\nFew-shot judge. We assess whether few-shot examples can improve consistency in the position bias\\\\nbenchmark. We select three good judgment examples using MT-bench-like questions, GPT-3.5 and\\\\nVicuna for generating answers, and GPT-4 for generating judgments. The examples cover three cases:\\\\nA is better, B is better, and tie. As shown in Table 12 (Appendix), the few-shot judge can significantly\\\\nincrease the consistency of GPT-4 from 65.0% to 77.5%'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.0% to 77.5%. However, high consistency may not imply\\\\nhigh accuracy and we are not sure whether the few-shot examples will introduce new biases. Besides,\\\\nthe longer prompts make API calls 4\\\\u00d7 more expensive. We use the zero-shot prompt by default in\\\\nour following experiments but leave an additional study in Appendix D.2.\\\\nChain-of-thought and reference-guided judge. In Section 3.3, we have shown LLM\\\\u2019s limited\\\\ncapability in grading math and reasoning questions. We propose two simple methods to mitigate\\\\nthis issue: chain-of-thought judge and reference-guided judge. Chain-of-thought is a widely used\\\\ntechnique to improve LLM\\\\u2019s reasoning capability [47]. We propose a similar technique to prompt\\\\nan LLM judge to begin with answering the question independently and then start grading. Detailed\\\\nprompt in Figure 7 (Appendix)'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. Detailed\\\\nprompt in Figure 7 (Appendix). However, even with the CoT prompt, we find that in many cases\\\\nLLM makes exactly the same mistake as the given answers in its problem-solving process (See\\\\nexample in Figure 15 (Appendix), suggesting that LLM judge may still be misled by the context.\\\\nHence, we propose a reference-guided method, in which we first generate LLM judge\\\\u2019s answer\\\\nindependently, and then display it as a reference answer in the judge prompt. In Table 4, we see a\\\\nsignificant improvement in failure rate (from 70% to 15%) over the default prompt.\\\\nFine-tuning a judge model. We try fine-tuning a Vicuna-13B on arena data to act as a judge and\\\\nshow some promising preliminary results in Appendix F.\\\\n3.5\\\\nMulti-turn judge\\\\nIn MT-bench, every question involves two turns to evaluate conversational abilities. Therefore, when\\\\ncomparing two assistants, it becomes necessary to present a total of two questions and four responses,\\\\ncomplicating the prompt design'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. We explore two possible designs, (1) breaking the two turns into two\\\\nprompts or (2) displaying complete conversations in a single prompt. Our finding is the former one\\\\ncan cause the LLM judge struggling to locate the assistant\\\\u2019s previous response precisely. We illustrate\\\\na case in Figure 16 (Appendix) where GPT-4 makes an inaccurate judgment due to a faulty reference.\\\\nThis suggests the necessity of displaying a complete conversation to enable the LLM judge to better\\\\ngrasp the context. We then consider the alternative design that presents two full conversations in a\\\\nsingle prompt in which we ask the LLM judge to focus on the second question (Figure 9 (Appendix)).\\\\nThis approach has been found to significantly alleviate the aforementioned referencing issue.\\\\n6\\\\n4\\\\nAgreement Evaluation\\\\nWe study the agreement between different LLM judges and humans on MT-bench and Chatbot\\\\nArena datasets. On MT-bench, we also study the agreement among humans'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. On MT-bench, we also study the agreement among humans. MT-bench represents a\\\\nsmall-scale study with controlled human evaluation, while Chatbot Arena represents a larger-scale\\\\nstudy with crowdsourced human evaluation in the wild.\\\\n4.1\\\\nSetup\\\\nMT-bench. We generate answers for all 80 questions with 6 models: GPT-4, GPT-3.5, Claude-V1,\\\\nVicuna-13B, Alpaca-13B [38], and LLaMA-13B [39]. We then use 2 kinds of judges: LLM judges\\\\nand 58 expert-level human labelers. The labelers are mostly graduate students so they are considered\\\\nexperts and more skilled than average crowd workers. We let LLM judges evaluate all pairs and let\\\\neach human evaluate at least 20 random multi-turn questions. This resulted in around 3K votes for all\\\\nquestions. The detailed data collection process is in Appendix C.\\\\nChatbot Arena. We randomly sample 3K single-turn votes from 30K arena data, which covers\\\\nmodels including GPT-4, GPT-3'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.5, Claude, Vicuna-7B/13B, Koala-13B [16], Alpaca-13B, LLaMA-\\\\n13B, and Dolly-12B. We use two kinds of judges: LLM judges and collected crowd judges (2114\\\\nunique IPs).\\\\nMetrics. We define the agreement between two types of judges as the probability of randomly\\\\nselected individuals (but not identical) of each type agreeing on a randomly selected question. See\\\\nmore explanation in Appendix D.3. Average win rate is the average of win rates against all other\\\\nplayers. These metrics can be computed with or without including tie votes.\\\\n4.2\\\\nHigh agreement between GPT-4 and humans\\\\nWe compute agreement on MT-bench data. In Table 5, GPT-4 with both pairwise comparison and\\\\nsingle answer grading show very high agreements with human experts. The agreement under setup S2\\\\n(w/o tie) between GPT-4 and humans reaches 85%, which is even higher than the agreement among\\\\nhumans (81%). This means GPT-4\\\\u2019s judgments closely align with the majority of humans'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. This means GPT-4\\\\u2019s judgments closely align with the majority of humans. We also\\\\nshow that GPT-4\\\\u2019s judgments may help humans make better judgments. During our data collection,\\\\nwhen a human\\\\u2019s choice deviated from GPT-4, we presented GPT-4\\\\u2019s judgments to humans and ask\\\\nif they are reasonable (details in Appendix C.1). Despite different views, humans deemed GPT-4\\\\u2019s\\\\njudgments reasonable in 75% of cases and are even willing to change their choices in 34% of cases.\\\\nThe data from Arena shows a similar trend, as illustrated by Table 6. Comparing GPT-4 and other\\\\nLLM judges, we find they reach a similar non-tie agreement ratio between humans but the number\\\\nof non-tied votes from GPT-4 is much larger. This means that GPT-4 is more affirmative and less\\\\nsuffered from position bias but other models also perform well when they give an affirmative answer.\\\\nIn both tables, GPT-4 with single-answer grading matches both pairwise GPT-4 and human prefer-\\\\nences very well'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. This means GPT-4 has a relatively stable internal rubric. Although it may sometimes\\\\nperform slightly worse than pairwise comparison and give more tie votes, it is a more scalable method.\\\\nWe then perform a breakdown analysis by computing agreement on different model pairs and\\\\ncategories. We only include non-tied votes. In Figure 2, we observe the agreement between GPT-4\\\\nand human progressively increases in line with the performance disparity of the model pairs (i.e.,\\\\nlarger win rate difference), from 70% to nearly 100%. This suggests that GPT-4 aligns with humans\\\\nbetter when significant performance differences exist between the models.\\\\nGPT-4\\\\nClaude\\\\nGPT-3.5\\\\nVicuna-13B\\\\nAlpaca-13B\\\\nLLaMA-13B\\\\n(a) All votes, first turn\\\\n0.0\\\\n0.2\\\\n0.4\\\\n0.6\\\\n0.8\\\\n1.0\\\\nGPT-4\\\\nGPT-3.5\\\\nClaude\\\\nVicuna-13B\\\\nAlpaca-13B\\\\nLLaMA-13B\\\\n(b) Non-tied votes, first turn\\\\n0.0\\\\n0.2\\\\n0.4\\\\n0.6\\\\n0.8\\\\n1.0\\\\nGPT-4\\\\nClaude\\\\nGPT-3.5\\\\nVicuna-13B\\\\nAlpaca-13B\\\\nLLaMA-13B\\\\n(c) All votes, second turn\\\\n0.0\\\\n0.2\\\\n0.4\\\\n0.6\\\\n0'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.5\\\\nVicuna-13B\\\\nAlpaca-13B\\\\nLLaMA-13B\\\\n(c) All votes, second turn\\\\n0.0\\\\n0.2\\\\n0.4\\\\n0.6\\\\n0.8\\\\n1.0\\\\nGPT-4\\\\nClaude\\\\nGPT-3.5\\\\nVicuna-13B\\\\nAlpaca-13B\\\\nLLaMA-13B\\\\n(d) Non-tied votes, second turn\\\\n0.0\\\\n0.2\\\\n0.4\\\\n0.6\\\\n0.8\\\\n1.0\\\\nWin rate\\\\nGPT-4 Judge\\\\nGPT-3.5 Judge\\\\nClaude Judge\\\\nHuman\\\\nHuman (first turn)\\\\nFigure 3: Average win rate of six models under different judges on MT-bench.\\\\n7\\\\nTable 5: Agreement between two types of judges on MT-bench. \\\\u201cG4-Pair\\\\u201d and \\\\u201cG4-Single\\\\u201d denote\\\\nGPT-4 with pairwise comparison and single-answer grading respectively. The single-answer grading\\\\ncan be converted into pairwise comparison results for calculating the agreement. We report two\\\\nsetups: \\\\u201cS1\\\\u201d includes non-tie, tie, and inconsistent (due to position bias) votes and counts inconsistent\\\\nas tie; \\\\u201cS2\\\\u201d only includes non-tie votes. The agreement between two random judges under each setup\\\\nis denoted as \\\\u201cR=\\\\u201d'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. The agreement between two random judges under each setup\\\\nis denoted as \\\\u201cR=\\\\u201d. The top value in each cell is the agreement, and the bottom gray value is #votes.\\\\nSetup\\\\nS1 (R = 33%)\\\\nS2 (R = 50%)\\\\nJudge\\\\nG4-Single\\\\nHuman\\\\nG4-Single\\\\nHuman\\\\nG4-Pair\\\\n70%\\\\n1138\\\\n66%\\\\n1343\\\\n97%\\\\n662\\\\n85%\\\\n859\\\\nG4-Single\\\\n-\\\\n60%\\\\n1280\\\\n-\\\\n85%\\\\n739\\\\nHuman\\\\n-\\\\n63%\\\\n721\\\\n-\\\\n81%\\\\n479\\\\n(a) First Turn\\\\nSetup\\\\nS1 (R = 33%)\\\\nS2 (R = 50%)\\\\nJudge\\\\nG4-Single\\\\nHuman\\\\nG4-Single\\\\nHuman\\\\nG4-Pair\\\\n70%\\\\n1161\\\\n66%\\\\n1325\\\\n95%\\\\n727\\\\n85%\\\\n864\\\\nG4-Single\\\\n-\\\\n59%\\\\n1285\\\\n-\\\\n84%\\\\n776\\\\nHuman\\\\n-\\\\n67%\\\\n707\\\\n-\\\\n82%\\\\n474\\\\n(b) Second Turn\\\\nTable 6: Agreement between two types of judges on Chatbot\\\\nArena. \\\\u201cG4-S\\\\u201d denotes GPT-4 with single-answer grading.\\\\n\\\\u201cG4\\\\u201d, \\\\u201cG3.5\\\\u201d and \\\\u201cC\\\\u201d denote GPT-4, GPT-3.5, and Claude\\\\nwith pairwise comparison, respectively. \\\\u201cH\\\\u201d denotes human.\\\\nThe remaining of table follows the same format as Table 5'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. \\\\u201cH\\\\u201d denotes human.\\\\nThe remaining of table follows the same format as Table 5.\\\\nSetup\\\\nS1 (Random = 33%)\\\\nS2 (Random = 50%)\\\\nJudge\\\\nG4-S\\\\nG3.5\\\\nC\\\\nH\\\\nG4-S\\\\nG3.5\\\\nC\\\\nH\\\\nG4\\\\n72%\\\\n2968\\\\n66%\\\\n3061\\\\n66%\\\\n3062\\\\n64%\\\\n3066\\\\n95%\\\\n1967\\\\n94%\\\\n1788\\\\n95%\\\\n1712\\\\n87%\\\\n1944\\\\nG4-S\\\\n-\\\\n60%\\\\n2964\\\\n62%\\\\n2964\\\\n60%\\\\n2968\\\\n-\\\\n89%\\\\n1593\\\\n91%\\\\n1538\\\\n85%\\\\n1761\\\\nG3.5\\\\n-\\\\n-\\\\n68%\\\\n3057\\\\n54%\\\\n3061\\\\n-\\\\n-\\\\n96%\\\\n1497\\\\n83%\\\\n1567\\\\nC\\\\n-\\\\n-\\\\n-\\\\n53%\\\\n3062\\\\n-\\\\n-\\\\n-\\\\n84%\\\\n1475\\\\n0.0\\\\n0.5\\\\n1.0\\\\nWin rate difference\\\\n0.7\\\\n0.8\\\\n0.9\\\\n1.0\\\\nAgreement\\\\nFigure 2: Agreement and win rate dif-\\\\nference. Each point corresponds to a\\\\nmodel pair and counts only the non-tie\\\\nvotes between the two models. The x-\\\\naxis value is the win rate difference be-\\\\ntween the two models. The y-axis value\\\\nis the GPT-4 and human agreement.\\\\nGPT-4\\\\nClaude\\\\nGPT-3.5\\\\nVicuna-13B\\\\nVicuna-7B\\\\nKoala-13B\\\\nAlpaca-13B\\\\nDolly-12B\\\\nLLaMA-13B\\\\n(a) All votes\\\\n0.0\\\\n0.2\\\\n0.4\\\\n0.6\\\\n0.8\\\\n1.0\\\\nGPT-4\\\\nClaude\\\\nGPT-3'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.0\\\\n0.2\\\\n0.4\\\\n0.6\\\\n0.8\\\\n1.0\\\\nGPT-4\\\\nClaude\\\\nGPT-3.5\\\\nVicuna-13B\\\\nVicuna-7B\\\\nKoala-13B\\\\nAlpaca-13B\\\\nDolly-12B\\\\nLLaMA-13B\\\\n(b) Non-tied votes\\\\n0.0\\\\n0.2\\\\n0.4\\\\n0.6\\\\n0.8\\\\n1.0\\\\nWin rate\\\\nGPT-4 Judge\\\\nGPT-3.5 Judge\\\\nHuman\\\\nGPT-4-Single Judge\\\\nFigure 4: Average win rate of nine models under different judges on Chatbot Arena.\\\\nTable 7: Category-wise win rate of models.\\\\nModel\\\\nWriting\\\\nRoleplay\\\\nReasoning\\\\nMath\\\\nCoding\\\\nExtraction\\\\nSTEM\\\\nHumanities\\\\nGPT-4\\\\n61.2%\\\\n67.9%\\\\n49.3%\\\\n66.1%\\\\n56.3%\\\\n66.2%\\\\n76.6%\\\\n72.2%\\\\nGPT-3.5\\\\n50.9%\\\\n60.6%\\\\n32.6%\\\\n63.8%\\\\n55.0%\\\\n48.8%\\\\n52.8%\\\\n53.8%\\\\nVicuna-13B\\\\n39.7%\\\\n39.2%\\\\n20.1%\\\\n18.0%\\\\n36.9%\\\\n29.2%\\\\n47.0%\\\\n47.5%\\\\nLLaMA-13B\\\\n15.1%\\\\n15.1%\\\\n7.8%\\\\n7.5%\\\\n2.1%\\\\n9.3%\\\\n6.8%\\\\n10.1%\\\\n4.3\\\\nWin rates under different judges\\\\nWe plot the average win rate of models under different judges on MT-bench and Chatbot Arena\\\\nin Figure 3 and Figure 4, respectively. The win rate curves from LLM judges closely match the\\\\ncurves from humans'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. The win rate curves from LLM judges closely match the\\\\ncurves from humans. On MT-bench second turn, proprietary models like Claude and GPT-3.5 are\\\\nmore preferred by the humans compared to the first turn, meaning that a multi-turn benchmark can\\\\nbetter differentiate some advanced abilities of models. We also list the per-category win rate of\\\\n8\\\\nTable 8: Evaluation results of several model variants.\\\\nModel\\\\n#Training Token\\\\nMMLU (5-shot)\\\\nTruthfulQA (0-shot)\\\\nMT-Bench Score (GPT-4)\\\\nLLaMA-7B\\\\n1T\\\\n35.2\\\\n0.22\\\\n2.74\\\\nLLaMA-13B\\\\n1T\\\\n47.0\\\\n0.26\\\\n2.61\\\\nAlpaca-7B\\\\n4.4M\\\\n40.1\\\\n0.26\\\\n4.54\\\\nAlpaca-13B\\\\n4.4M\\\\n48.1\\\\n0.30\\\\n4.53\\\\nVicuna-7B (selected)\\\\n4.8M\\\\n37.3\\\\n0.32\\\\n5.95\\\\nVicuna-7B (single)\\\\n184M\\\\n44.1\\\\n0.30\\\\n6.04\\\\nVicuna-7B (all)\\\\n370M\\\\n47.1\\\\n0.32\\\\n6.00\\\\nVicuna-13B (all)\\\\n370M\\\\n52.1\\\\n0.35\\\\n6.39\\\\nGPT-3.5\\\\n-\\\\n70.0\\\\n-\\\\n7.94\\\\nGPT-4\\\\n-\\\\n86.4\\\\n-\\\\n8.99\\\\nrepresentative models in Table 7 to show how MT-bench differentiates models, in which we see GPT-4\\\\nis significantly better than others'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. Vicuna-13B is noticeably worse than GPT-3.5/4 in reasoning, math,\\\\nand coding categories. Note that in math/coding category, GPT-3.5 and GPT-4 have similar overall\\\\nwin-rate because they both failed to answer some hard questions, but GPT-4 is still significantly better\\\\nthan GPT-3 in the direct pairwise comparison or single-answer grading. Please see a performance\\\\nbreakdown of MT-bench score for each category in Appendix D.4.\\\\n5\\\\nHuman Preference Benchmark and Standardized Benchmark\\\\nHuman preference benchmarks such as MT-bench and Chatbot Arena serve as valuable additions\\\\nto the current standardized LLM benchmarks. They focus on different aspects of a model and the\\\\nrecommended way is to comprehensively evaluate models with both kinds of benchmarks.\\\\nWe evaluate several model variants derived from LLaMA on MMLU [19], Truthful QA [26] (MC1),\\\\nand MT-bench (GPT-4 judge). The training details are in Appendix E'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. The training details are in Appendix E. Since we have shown that\\\\nGPT-4 single-answer grading also performs well in Section 4.2, we use GPT-4 single-answer grading\\\\nfor MT-bench in favor of its scalability and simplicity. We ask GPT-4 to give a score for each turn\\\\non a scale of 10 by using our prompt templates (Figure 6, Figure 10) and report an average score of\\\\n160 = 80\\\\u00d72 turns. Table 8 shows the results. We find that fine-tuning on high-quality dialog datasets\\\\n(i.e., ShareGPT) can consistently improve the model performance on MMLU and the improvement\\\\nscales with fine-tuning data size. On the other hand, a small high-quality conversation dataset can\\\\nquickly teach the model a style preferred by GPT-4 (or approximately human) but cannot improve\\\\nMMLU significantly, as shown by the Vicuna-7B (selected) which is trained with only 4.8M tokens\\\\nor 3K conversations. In Table 8, no single benchmark can determine model quality, meaning that a\\\\ncomprehensive evaluation is needed'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. Our results indicate that using LLM-as-a-judge to approximate\\\\nhuman preferences is highly feasible and could become a new standard in future benchmarks. We\\\\nare also hosting a regularly updated leaderboard with more models 2. Notably, DynaBench [21], a\\\\nresearch platform dedicated to dynamic data collection and benchmarking, aligns with our spirit.\\\\nDynaBench addresses the challenges posed by static standardized benchmarks, such as saturation and\\\\noverfitting, by emphasizing dynamic data with human-in-the-loop. Our LLM-as-a-judge approach\\\\ncan automate and scale platforms of this nature.\\\\n6\\\\nDiscussion\\\\nLimitations. This paper emphasizes helpfulness but largely neglects safety. Honesty and harm-\\\\nlessness are crucial for a chat assistant as well [2]. We anticipate similar methods can be used to\\\\nevaluate these metrics by modifying the default prompt'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. Additionally, within helpfulness, there\\\\nare multiple dimensions like accuracy, relevance, and creativity, but they are all combined into a\\\\nsingle metric in this study. A more comprehensive evaluation can be developed by analyzing and\\\\nseparating these dimensions. We propose preliminary solutions to address the limitations and biases\\\\nof LLM-as-a-judge in Section 3.4, but we anticipate more advanced methods can be developed.\\\\nData collection and release. Appendix C describes the detailed data collection and release processes,\\\\nwhich include the instructions we give to users, the screenshots of the data collection interface, the\\\\ninformation about participated users, and the content of the released data.\\\\n2https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\\\\n9\\\\nSocietal impacts. The societal impact of this study is multi-faceted. Our evaluation methods can\\\\nhelp enhance chatbot quality and user experiences. However, addressing biases in these methods is\\\\ncrucial'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='. However, addressing biases in these methods is\\\\ncrucial. Our dataset enables better studies of human preferences and model behavior. Advanced chat\\\\nassistants may replace certain human tasks, resulting in job displacements and new opportunities.\\\\nFuture directions. 1) Benchmarking chatbots at scale with a broader set of categories 2) Open-source\\\\nLLM judge aligned with human preference 3) Enhancing open models\\\\u2019 math/reasoning capability.\\\\n7\\\\nConclusion\\\\nIn this paper, we propose LLM-as-a-judge for chatbot evaluation and systematically examine its\\\\nefficacy using human preference data from 58 experts on MT-bench, as well as thousands of crowd-\\\\nusers on Chatbot Arena. Our results reveal that strong LLMs can achieve an agreement rate of over\\\\n80%, on par with the level of agreement among human experts, establishing a foundation for an\\\\nLLM-based evaluation framework'), Document(metadata={'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}, page_content='.\\\\nAcknowledgement\\\\nThis project is partly supported by gifts from Anyscale, Astronomer, Google, IBM, Intel, Lacework,\\\\nMicrosoft, MBZUAI, Samsung SDS, Uber, and VMware. Lianmin Zheng is supported by a Meta\\\\nPh.D. Fellowship. We extend our thanks to Xinyang Geng, Hao Liu, Eric Wallace, Xuecheng Li,\\\\nTianyi Zhang, Qirong Ho, and Kevin Lin for their insightful discussions.\\\\n')]]\n"
     ]
    }
   ],
   "source": [
    "print(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'Ch_db'\n",
    "\n",
    "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
    "\n",
    "doc=[d[0] for d in doc_chunks]\n",
    "vectordb = Chroma.from_documents(documents=doc, \n",
    "                                 embedding=embedder,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # persiste the db to disk\n",
    "# vectordb.persist()\n",
    "# vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_26948\\2337759437.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"what the innovative idea behind the transformers architecture?\")\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"what the innovative idea behind the transformers architecture?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using ChromaDB either using Chroma or from LangChain vectorstore is working fine and retreiving relevant files with persistent storage. In the next and final step, we will be implementing LLM-as-a-Judge concept and evaluate the generated content based.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
