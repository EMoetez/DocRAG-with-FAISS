{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building an RAG Query pipeline with FAISS, and Ollamaâ€™s Llama 2 Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project develops a Retrieval-Augmented Generation (RAG) query app that combines efficient retrieval with AI-driven responses. By integrating Chroma DB for data storage, FAISS for vector search, and Llama 2 via Ollama for response generation, it can deliver precise, context-aware answers to user queries. Each part of the code is structured for clarity, following best practices to ensure a seamless experience, from setup to deployment. Let's dive in and bring this RAG app to life!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0-cp310-cp310-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\msi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.9.0-cp310-cp310-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 14.9/14.9 MB 808.9 kB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss\n",
    "%pip install langchain_ollama\n",
    "%pip install faiss-cpu\n",
    "%pip install --upgrade gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.13-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.13-cp39-abi3-win_amd64.whl (16.2 MB)\n",
      "   ---------------------------------------- 16.2/16.2 MB 557.2 kB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install arxiv\n",
    "%pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import langchain\n",
    "import langchain.vectorstores\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain.vectorstores import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader, PyPDFLoader\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter= RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Llama2 \n",
    "instruct_llm = ChatOllama(model=\"llama2\", temperature=0.6, num_predict=256)\n",
    "\n",
    "#Using Nvidia embeddings\n",
    "embedder= OllamaEmbeddings(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    #ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    #ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    #ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    #ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    content=json.dumps(doc[0].page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chunking\n"
     ]
    }
   ],
   "source": [
    "#Chunking the documents and remove very short chunks\n",
    "print(\"Start chunking\")\n",
    "doc_chunks=[text_splitter.split_documents(doc) for doc in docs]\n",
    "doc_chunks=[[c for c in dchunks if len(c.page_content)>200] for dchunks in doc_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the big-picture details\n",
    "Doc_string=\"Available Documents: \"\n",
    "Doc_metadata=[]\n",
    "for chunk in doc_chunks:\n",
    "    metadata= getattr(chunk[0], 'metadata',{})\n",
    "    Doc_string+= \"\\n - \" + metadata.get('Title')\n",
    "    Doc_metadata+= [str(metadata)]\n",
    "    \n",
    "BP_Chunks=  [Doc_string] + Doc_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Documents: \n",
      " - Attention Is All You Need\n",
      " - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena \n",
      "\n",
      "Document 0\n",
      " - # Chunks: 35\n",
      " - Metadata: \n",
      "{'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion '\n",
      "            'Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',\n",
      " 'Published': '2023-08-02',\n",
      " 'Summary': 'The dominant sequence transduction models are based on complex '\n",
      "            'recurrent or\\n'\n",
      "            'convolutional neural networks in an encoder-decoder '\n",
      "            'configuration. The best\\n'\n",
      "            'performing models also connect the encoder and decoder through an '\n",
      "            'attention\\n'\n",
      "            'mechanism. We propose a new simple network architecture, the '\n",
      "            'Transformer, based\\n'\n",
      "            'solely on attention mechanisms, dispensing with recurrence and '\n",
      "            'convolutions\\n'\n",
      "            'entirely. Experiments on two machine translation tasks show these '\n",
      "            'models to be\\n'\n",
      "            'superior in quality while being more parallelizable and requiring '\n",
      "            'significantly\\n'\n",
      "            'less time to train. Our model achieves 28.4 BLEU on the WMT 2014\\n'\n",
      "            'English-to-German translation task, improving over the existing '\n",
      "            'best results,\\n'\n",
      "            'including ensembles by over 2 BLEU. On the WMT 2014 '\n",
      "            'English-to-French\\n'\n",
      "            'translation task, our model establishes a new single-model '\n",
      "            'state-of-the-art\\n'\n",
      "            'BLEU score of 41.8 after training for 3.5 days on eight GPUs, a '\n",
      "            'small fraction\\n'\n",
      "            'of the training costs of the best models from the literature. We '\n",
      "            'show that the\\n'\n",
      "            'Transformer generalizes well to other tasks by applying it '\n",
      "            'successfully to\\n'\n",
      "            'English constituency parsing both with large and limited training '\n",
      "            'data.',\n",
      " 'Title': 'Attention Is All You Need'}\n",
      "\n",
      "Document 1\n",
      " - # Chunks: 44\n",
      " - Metadata: \n",
      "{'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, '\n",
      "            'Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric '\n",
      "            'P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica',\n",
      " 'Published': '2023-12-24',\n",
      " 'Summary': 'Evaluating large language model (LLM) based chat assistants is '\n",
      "            'challenging\\n'\n",
      "            'due to their broad capabilities and the inadequacy of existing '\n",
      "            'benchmarks in\\n'\n",
      "            'measuring human preferences. To address this, we explore using '\n",
      "            'strong LLMs as\\n'\n",
      "            'judges to evaluate these models on more open-ended questions. We '\n",
      "            'examine the\\n'\n",
      "            'usage and limitations of LLM-as-a-judge, including position, '\n",
      "            'verbosity, and\\n'\n",
      "            'self-enhancement biases, as well as limited reasoning ability, '\n",
      "            'and propose\\n'\n",
      "            'solutions to mitigate some of them. We then verify the agreement '\n",
      "            'between LLM\\n'\n",
      "            'judges and human preferences by introducing two benchmarks: '\n",
      "            'MT-bench, a\\n'\n",
      "            'multi-turn question set; and Chatbot Arena, a crowdsourced battle '\n",
      "            'platform. Our\\n'\n",
      "            'results reveal that strong LLM judges like GPT-4 can match both '\n",
      "            'controlled and\\n'\n",
      "            'crowdsourced human preferences well, achieving over 80% '\n",
      "            'agreement, the same\\n'\n",
      "            'level of agreement between humans. Hence, LLM-as-a-judge is a '\n",
      "            'scalable and\\n'\n",
      "            'explainable way to approximate human preferences, which are '\n",
      "            'otherwise very\\n'\n",
      "            'expensive to obtain. Additionally, we show our benchmark and '\n",
      "            'traditional\\n'\n",
      "            'benchmarks complement each other by evaluating several variants '\n",
      "            'of LLaMA and\\n'\n",
      "            'Vicuna. The MT-bench questions, 3K expert votes, and 30K '\n",
      "            'conversations with\\n'\n",
      "            'human preferences are publicly available at\\n'\n",
      "            'https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.',\n",
      " 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Printing out some summary information for reference\n",
    "print(Doc_string, '\\n')\n",
    "for i, chunks in enumerate(doc_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consctructing the vector store\n",
    "vecstore=[FAISS.from_texts(BP_Chunks, embedding=embedder)]\n",
    "vecstore+=[FAISS.from_documents(doc_chunk,embedding=embedder) for  doc_chunk in doc_chunks]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity.\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 82 chunks\n"
     ]
    }
   ],
   "source": [
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstore)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {Doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "    \"Be concize and precize to answer in less than 250 words\"\n",
    "), ('user', '{input}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign({'history' : itemgetter('input')| convstore.as_retriever()| long_reorder | docs2str })\n",
    "    | RunnableAssign({'context' : itemgetter('input')|  docstore.as_retriever()| long_reorder | docs2str})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from Attention Is All You Need] .1, instead of 0.3.\\\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\\\nused beam search with a beam size of 4 and length penalty \\\\u03b1 = 0.6 [38]. These hyperparameters\\\\nwere chosen after experimentation on the development set. We set the maximum output length during\\\\ninference to input length + 50, but terminate early when possible [38].\\\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU 5.\\\\n6\\n[Quote from Attention Is All You Need] .\\\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V ) = softmax(QKT\\\\n\\\\u221adk\\\\n)V\\\\n(1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof\\\\n1\\\\n\\\\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\\\nmatrix multiplication code.\\\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\\\ndot product attention without scaling for larger values of dk [3]\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] . We then verify the agreement between LLM judges and human preferences\\\\nby introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\\\\nArena, a crowdsourced battle platform. Our results reveal that strong LLM judges\\\\nlike GPT-4 can match both controlled and crowdsourced human preferences well,\\\\nachieving over 80% agreement, the same level of agreement between humans.\\\\nHence, LLM-as-a-judge is a scalable and explainable way to approximate human\\\\npreferences, which are otherwise very expensive to obtain. Additionally, we show\\\\nour benchmark and traditional benchmarks complement each other by evaluating\\\\nseveral variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes,\\\\nand 30K conversations with human preferences are publicly available at https:\\\\n//github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge\\n[Quote from Document] Available Documents: \\n - Attention Is All You Need\\n - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)Answer in less than 250 words', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about RAG!', additional_kwargs={}, response_metadata={})])\n",
      "Ah, another exciting topic related to natural language processing! RAG, or the Recent Advances in Generative models, is a fascinating area of research that has seen significant developments in recent years.\n",
      "\n",
      "RAG refers to the latest advancements in generative models, such as transformer-based architectures like BERT and RoBERTa, which have shown remarkable performance in various natural language processing tasks. These models are capable of generating coherent and contextually relevant text, thanks to their ability to learn from large amounts of data and capture complex contextual relationships.\n",
      "\n",
      "One of the most significant advancements in RAG is the development of attention-based models. Attention mechanisms allow these models to focus on specific parts of the input when generating output, resulting in more accurate and relevant text. For instance, when translating a sentence from one language to another, an attention-based model can focus on specific words or phrases in the source language to ensure an accurate translation.\n",
      "\n",
      "Another exciting development in RAG is the use of multimodal input representations. These models can process and generate text, image, and even audio content, opening up new possibilities for applications like chatbots, voice"
     ]
    }
   ],
   "source": [
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  \n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\components\\chatbot.py:223: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://c13aa2db04d38efd90.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c13aa2db04d38efd90.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: What is the core idea behind the LLM-a-Judge concept?\\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\n[Quote from Document] User previously responded with Tell me about RAG!\\n[Quote from Document] Agent previously responded with Ah, another exciting topic related to natural language processing! RAG, or the Recent Advances in Generative models, is a fascinating area of research that has seen significant developments in recent years.\\n\\nRAG refers to the latest advancements in generative models, such as transformer-based architectures like BERT and RoBERTa, which have shown remarkable performance in various natural language processing tasks. These models are capable of generating coherent and contextually relevant text, thanks to their ability to learn from large amounts of data and capture complex contextual relationships.\\n\\nOne of the most significant advancements in RAG is the development of attention-based models. Attention mechanisms allow these models to focus on specific parts of the input when generating output, resulting in more accurate and relevant text. For instance, when translating a sentence from one language to another, an attention-based model can focus on specific words or phrases in the source language to ensure an accurate translation.\\n\\nAnother exciting development in RAG is the use of multimodal input representations. These models can process and generate text, image, and even audio content, opening up new possibilities for applications like chatbots, voice\\n\\n\\n Document Retrieval:\\n[Quote from Attention Is All You Need] . There are many choices of positional encodings,\\\\nlearned and fixed [9].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0 to 10000 \\\\u00b7 2\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] .com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\\\\n1\\\\nIntroduction\\\\nThere has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised\\\\ninstruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new\\\\ninstruction following and conversational abilities [31, 2, 30, 8, 52, 48, 14]. Once aligned with\\\\nhumans, these chat models are strongly preferred by human users over the original, unaligned models\\\\non which they are built. However, the heightened user preference does not always correspond to\\\\nimproved scores on traditional LLM benchmarks \\\\u2013 benchmarks like MMLU [19] and HELM [24]\\\\ncannot effectively tell the difference between these aligned models and the base models. This\\\\nphenomenon suggests that there is a fundamental discrepancy between user perceptions of the\\\\nusefulness of chatbots and the criteria adopted by conventional benchmarks\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] \"Judging LLM-as-a-Judge\\\\nwith MT-Bench and Chatbot Arena\\\\nLianmin Zheng1\\\\u2217\\\\nWei-Lin Chiang1\\\\u2217\\\\nYing Sheng4\\\\u2217\\\\nSiyuan Zhuang1\\\\nZhanghao Wu1\\\\nYonghao Zhuang3\\\\nZi Lin2\\\\nZhuohan Li1\\\\nDacheng Li13\\\\nEric P. Xing35\\\\nHao Zhang12\\\\nJoseph E. Gonzalez1\\\\nIon Stoica1\\\\n1 UC Berkeley\\\\n2 UC San Diego\\\\n3 Carnegie Mellon University\\\\n4 Stanford\\\\n5 MBZUAI\\\\nAbstract\\\\nEvaluating large language model (LLM) based chat assistants is challenging due to\\\\ntheir broad capabilities and the inadequacy of existing benchmarks in measuring\\\\nhuman preferences. To address this, we explore using strong LLMs as judges to\\\\nevaluate these models on more open-ended questions. We examine the usage and\\\\nlimitations of LLM-as-a-judge, including position, verbosity, and self-enhancement\\\\nbiases, as well as limited reasoning ability, and propose solutions to mitigate some\\\\nof them\\n[Quote from Document] Available Documents: \\n - Attention Is All You Need\\n - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)Answer in less than 250 words', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the core idea behind the LLM-a-Judge concept?', additional_kwargs={}, response_metadata={})])\n",
      "ChatPromptValue(messages=[SystemMessage(content='You are a document chatbot. Help the user as they ask questions about documents. User messaged just asked: Is it effecient enough to be used in different tasks \\n\\n From this, we have retrieved the following potentially-useful info:  Conversation History Retrieval:\\n[Quote from Document] User previously responded with Tell me about RAG!\\n[Quote from Document] Agent previously responded with Ah, another exciting topic related to natural language processing! RAG, or the Recent Advances in Generative models, is a fascinating area of research that has seen significant developments in recent years.\\n\\nRAG refers to the latest advancements in generative models, such as transformer-based architectures like BERT and RoBERTa, which have shown remarkable performance in various natural language processing tasks. These models are capable of generating coherent and contextually relevant text, thanks to their ability to learn from large amounts of data and capture complex contextual relationships.\\n\\nOne of the most significant advancements in RAG is the development of attention-based models. Attention mechanisms allow these models to focus on specific parts of the input when generating output, resulting in more accurate and relevant text. For instance, when translating a sentence from one language to another, an attention-based model can focus on specific words or phrases in the source language to ensure an accurate translation.\\n\\nAnother exciting development in RAG is the use of multimodal input representations. These models can process and generate text, image, and even audio content, opening up new possibilities for applications like chatbots, voice\\n[Quote from Document] Agent previously responded with Ah, a great question! The LLM-a-Judge concept is all about using large language models (LLMs) as judges to evaluate other LLMs on more open-ended questions. The idea is that since LLMs have broad capabilities and existing benchmarks don\\'t capture human preferences, we can use these powerful models to assess their peers in a more nuanced way. By having an LLM act as a judge, we can evaluate the chatbots on more complex tasks and better reflect how humans might use them.\\n\\nThe paper \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\" delves into this concept in detail. The authors explore the potential biases and limitations of using LLMs as judges, such as position, verbosity, and self-enhancement biases, as well as limited reasoning ability. They also propose solutions to mitigate some of these issues.\\n\\nOverall, the core idea behind LLM-a-Judge is to leverage the strengths of LLMs to provide a more comprehensive evaluation of their peers in the chatbot space. By doing so\\n[Quote from Document] User previously responded with What is the core idea behind the LLM-a-Judge concept?\\n\\n\\n Document Retrieval:\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] . The training details are in Appendix E. Since we have shown that\\\\nGPT-4 single-answer grading also performs well in Section 4.2, we use GPT-4 single-answer grading\\\\nfor MT-bench in favor of its scalability and simplicity. We ask GPT-4 to give a score for each turn\\\\non a scale of 10 by using our prompt templates (Figure 6, Figure 10) and report an average score of\\\\n160 = 80\\\\u00d72 turns. Table 8 shows the results. We find that fine-tuning on high-quality dialog datasets\\\\n(i.e., ShareGPT) can consistently improve the model performance on MMLU and the improvement\\\\nscales with fine-tuning data size. On the other hand, a small high-quality conversation dataset can\\\\nquickly teach the model a style preferred by GPT-4 (or approximately human) but cannot improve\\\\nMMLU significantly, as shown by the Vicuna-7B (selected) which is trained with only 4.8M tokens\\\\nor 3K conversations. In Table 8, no single benchmark can determine model quality, meaning that a\\\\ncomprehensive evaluation is needed\\n[Quote from Attention Is All You Need] .\\\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\\\nthe matrix of outputs as:\\\\nAttention(Q, K, V ) = softmax(QKT\\\\n\\\\u221adk\\\\n)V\\\\n(1)\\\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\\\nof\\\\n1\\\\n\\\\u221adk . Additive attention computes the compatibility function using a feed-forward network with\\\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\\\nmatrix multiplication code.\\\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\\\ndot product attention without scaling for larger values of dk [3]\\n[Quote from Attention Is All You Need] . To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\\\nthe input sequence centered around the respective output position. This would increase the maximum\\\\npath length to O(n/r). We plan to investigate this approach further in future work.\\\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\\\nconsiderably, to O(k \\\\u00b7 n \\\\u00b7 d + n \\\\u00b7 d2)\\n[Quote from Attention Is All You Need] . Aligning the positions to steps in computation time, they generate a sequence of hidden\\\\nstates ht, as a function of the previous hidden state ht\\\\u22121 and the input for position t. This inherently\\\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\\\nconstraint of sequential computation, however, remains.\\\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\\\nthe input or output sequences [2, 19]\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)Answer in less than 250 words', additional_kwargs={}, response_metadata={}), HumanMessage(content='Is it effecient enough to be used in different tasks ', additional_kwargs={}, response_metadata={})])\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://c13aa2db04d38efd90.gradio.live\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    \n",
    "\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    \n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"Mydocstore_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a Mydocstore_index\n",
      "a Mydocstore_index/index.faiss\n",
      "a Mydocstore_index/index.pkl\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!tar czvf Mydocstore_index.tgz Mydocstore_index\n",
    "\n",
    "!rm -rf Mydocstore_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x Mydocstore_index/\n",
      "x Mydocstore_index/index.faiss\n",
      "x Mydocstore_index/index.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". Our results indicate that using LLM-as-a-judge to approximate\\nhuman preferences is highly feasible and could become a new standard in future benchmarks. We\\nare also hosting a regularly updated leaderboard with more models 2. Notably, DynaBench [21], a\\nresearch platform dedicated to dynamic data collection and benchmarking, aligns with our spirit.\\nDynaBench addresses the challenges posed by static standardized benchmarks, such as saturation and\\noverfitting, by emphasizing dynamic data with human-in-the-loop. Our LLM-as-a-judge approach\\ncan automate and scale platforms of this nature.\\n6\\nDiscussion\\nLimitations. This paper emphasizes helpfulness but largely neglects safety. Honesty and harm-\\nlessness are crucial for a chat assistant as well [2]. We anticipate similar methods can be used to\\nevaluate these metrics by modifying the default prompt\n"
     ]
    }
   ],
   "source": [
    "#Make sure the retreival from Mydocstore_index works\n",
    "!tar xzvf Mydocstore_index.tgz\n",
    "new_db = FAISS.load_local(\"Mydocstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs_test = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs_test[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now our DocQuery pipeline works fine and stores locally the files successfully. In the next steps, we will use ChromaDB for effecient vector store and retreival. Furthermore, we will try to implement the concept of LLM-as-Judge to evaluate the results generated by our model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
